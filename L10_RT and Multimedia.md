# Lesson outline
- [L10a: TS-Linux](https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/L10_RT%20and%20Multimedia.md#l10a-ts-linux)
- [L10b: PTS](https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/L10_RT%20and%20Multimedia.md#l10b-pts)

# L10a: TS-Linux
<h2>1. TS-Linux Introduction</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/1.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Traditionally general purpose operating systems have catered to the needs of throughput oriented applications, such as databases and scientific applications. But now more and more applications, such as synchronous AVplayers, video games and so on, need soft real-time guarantees. That is they are latency sensitive. How do we mix the two kinds of applications in a general purpose operating system? Also, what are the pain points in developing complex, multimedia applications that need such real-time guarantees? How can they be alleviated?</li> 
  <li>In this module, we will discuss approaches to addressing these questions. In this lesson, we will discuss time-sensitive Linux, an extension of the commodity general purpose Linux operating system that addresses two questions.</li> 
  <li>The first question is how to provide guarantees for real-time applications in the presence of background throughput oriented applications, such as databases and so on.</li> 
  <li>The second question is how to bound the performance loss of throughput oriented applications in the presence of latency sensitive applications.
</li> 
</ul>

<h2>2. Sources of Latency</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/2.JPG?raw=true" alt="drawing" width="600"/>
</p>


<ul>
  <li>Time sensitive applications require quickly responding to an event. Think of playing a video game and shooting at a target. You want the instant that you shoot at the target for action to appear on the screen. Now problem is there are three sources of latency for time sensitive events in any typical general purpose operating system.</li> 
  <li>The first source of latency is what is called the timer latency. That is coming because of the inaccuracy of the timing mechanism. For instance, the timer event went off at this point, but the timer interrupt actually happens only here. That is, this is the event that should have resulted in an interrupt, but because of the inaccuracy of the timer, there's a latency between the point at which an even happened, an external event. And the point at which the timer goes off indicating that external event. And this is primarily coming up due to the granularity of the timing mechanism that's available in general purpose operating systems. For instance, periodic timers tend to have a ten millisecond granularity in Linux operating system. So that's the first source of inaccuracy or latency for time sensitive applications.</li> 
  <li>The second source of latency is what we call as the as the preemption latency. And this is because of the fact that the interupt could have happened when the kernel was in the middle of doing something from which it can not be preempted. For instance, when the kernel is modifying some critical data structure. In that case it may have turned off the interrupt and therefore even though the interrupt went off at this point the kernel cannot be preempted until this point. That may be the second source of latency. Or it could be that the kernel itself is in the middle of handling another higher priority interrupt. And in that case, this time interrupt is going to be delayed. So that the second source of latency for time sensitive applications running on commodity operating systems.</li> 
  <li>Okay, finally the timer interrupt has been delivered to the system and now the scheduler can actually schedule the process that is waiting for this timer interrupt so that that application can take the appropriate action for this external interrupt. But wait, there is another high priority task that's already in the scheduler's queue and therefore the app that was waiting for this timer event cannot yet be scheduled because of this higher priority tasks that has dibs on the processor at this point of time. So this is the third source of latency, that is the scheduler latency. That is preventing this external event from being delivered to the application that is waiting for this particular event. Finally the high priority app is done, and therefore now it is time for the app that is actually waiting for this event. So this is the point of activation of the Event app, that is, the app that is waiting for this timer event.</li> 
  <li> So you can see that this is the distance TA to Th. The difference between TA and Th is the event to activation distance. The event happened here at Th, but the app that can react to this event only gets scheduled at this point Ta, and this is the latency that is coming in for time-sensitive applications between the actual event happening. And the activation of the app that has to deal with that particular event. For time sensitive applications to do well, it is extremely important that we shrink this distance between event happening and event activation.
</li> 
</ul>


<h2>3. Timers Available</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/3.JPG?raw=true" alt="drawing" width="600"/>
</p>

<ul>
  <li>Let's review the kinds of timers that are avaialbe in an operating system and the Pros & Cons.</li> 
  <li>The timer that many of you may be familiar with is a periodic timer that is standard in most Unix operating systems. And the periodic timer, the pro is periodicity. That's the pro of a periodic timer that. The operating system is not going to be interrupted willy-nilly, but it is going to be interrupted at regular periods. But the con is the event recognition latency, because the event may have happened at a particular point in time, but because of the granularity of the periodic timer. The event is actually recognized at a much later point in real time, and that's the con for periodic timer. And the maximum time of latency is equal to the period itself, so for instance, if the event happened just after a periodic interrupt, then the event will have to be delivered by the next periodic interrupt. So the worst case latency is going to be the periodicity of the periodic timer itself.</li> 
  <li>The other extreme to periodic timer, is what we call one-shot timers. And one-shot timers are exact timers. That is, you can program these timers to go off at exactly the point at which you want the interrupt delivered to the processor, so the pro is a timeliness of the one-shot timer. And secondly, there is also fielding these one shot timers as when they occur, that's an extra overhead that the operating system has to deal with.</li> 
  <li>If you're concerned with the interrupt overhead, one extreme position to take is to get rid of timer interrupts all together, and simply use soft timers. That is, there is no timer interrupt, but the operating system is going to poll at strategic times to see if there is an external event. What would be strategic times? Typically any application is going to make system calls. And when a system call gets you into the kernel, at that point the kernel can see if there are any events that need attention at that point of time. And that is a polling method, but the downside of the polling method Is the fact that, there is latency associated with that and there is also the fact that you have to poll all the events to see if any of them have expired, and there is an overhead associated with that. These are the two cons for a soft timer but the pro is the fact that you have reduced overhead for soft timers since there are no timer interrupts per se, but the kernal is using strategic points during it's execution such as system calls or other external interrupts. For instance a network packet arrival or something like that. As a trigger for looking at the time related structures to see if any of the events may have expired at that point of time.</li> 
  <li>Firm timer is a new mechanism that is proposed in TS Linux and as we will see shortly it combines the advantages of all the three. Timers that I just mentioned, the periodic timer, the one-shot timer, and the soft timer. Essentially, what firm timer is trying to do is, take the pros of all of these types of timers in its implementation so that it can avoid the cons associated with each one of these individual choices.
</li> 
</ul>

<br> <p>OH session:  APIC</p>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/oh1.JPG?raw=true" alt="drawing" width="600"/>
</p>

<h2>4. Firm Timer Design</h2>
Firm timer paper: Goel, A., Abeni, L., Krasic, C., Snow, J., & Walpole, J. Supporting Time-Sensitive Applications on a Commodity OS.

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/4.JPG?raw=true" alt="drawing" width="600"/>
</p>

<ul>
  <li>The fundamental idea behind the firm timer design is to provide accurate timing with very low overhead. And what it does is it combines the good points of the one shot timer and the soft timers. As I mentioned, the one shot timer, its virtue is the fact that we can have the timer interrupt happen exactly at the time that we want. In other words, we get rid of the timer inaccuracy that we mentioned earlier that plagues normal timer design in general purpose operating systems. The con that we said with the oneshot timer is that there is overhead associated with processing oneshot timer events as well as reprogramming them.</li> 
  <li>So for that reason, what they do in TS Linux with the firm timer design is to have a knob, which is called the overshoot knob, and the idea is the one shot timer expired here. This is the point at which the event happened, but what we're going to do is. We're going to have a parameter associated with the one shot timer called the overshoot parameter and what that overshoot parameter is allowing you to do is, even though this is the point at which the event happened, the one-shot timer is programmed to go off at this point. So there is a distance between the actual event happening and the point at which the one-shot timer has been programmed to interrupt the CPU. This is the overshoot parameter. You may wonder, what is the advantage of this overshoot parameter? Well, within this overshoot parameter window, there could be a system call, which is a soft interrupt. We mentioned that applications typically make system calls or there could be an external interrupt. Any of those things can bring you into the kernel, and so you may be already in the kernel at this point during this overshoot window.</li> 
  <li>So what we could do now is, we'll dispatch the expired timers at this point of time and also we will reprogram the one-shot timer that expired for the next time that we need the one-shot timer to interrupt. Normally, the one shot timer would have gone off here because this is the overshoot window, but even before that, since the system call happened, at this point, we'll going to see that oh, there is this one shot timer that has already expired, its going to interrupt me not in the too distant future, might as well dispatch that expired timer right now and reprogram that one shot timer so that it is now ready for the next one-shot timer interrupt.</li> 
  <li>The upshot of doing this, dispatching the expired timer at the point of the system call that is happening within this overshoot window, is that at the point at which this one-shot timer has been programmed to interrupt, that is gone. It'll not cause an interrupt because we've reprogrammed this one-shot timer to interrupt at the next one-shot timer expiry event. Because whatever action that needed to be taken with respect to this particular one-shot timer expiration has already been taken at this point and therefore at this point we will not get another interrupt. So we have saved on the one-shot timer interrupt that would have happened at this point of time by combining the hard and soft timer in this manner. I mentioned that fielding a one-shot timer interrupt is expensive. We avioded that. Because, fielding the interrupt happened as part of handling the system call and at this point itself, we have re-programmed the one-shot timer for the next event as well.</li> 
  <li>So you can see that the firm timer design combines the good points of both the one-shot timer as well as the soft timer. It is giving us the accuracy that we want from the one-shot timer but at the same time, we are avoiding the overhead associated with one-shot timer by processing that interrupt using this over shoot parameter. Of course, within this over shoot parameter, there is no system call or external interrupt that brings us into the kernel, then we will have this one-shot timer going off at this point. But the hope is that by choosing this knob appropriately, between hard and soft timers, we can make sure that we reduce the number of times we get the one shot timer interrupts actually occurring.
</li> 
</ul>

<h2>5. Firm Timer Implementation</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/5.JPG?raw=true" alt="drawing" width="600"/>
</p>


<ul>
  <li>As you know, in Linux they use the term task to signify a schedulable entity. And so, we're using T1, T2, T3 to mean tasks, which are schedulable entities. And the timer-q data structure, what it contains is the tasks and the expiry time for that particular task. And the tasks are ordered in this Timer-q data structure maintained by the kernel sorted by the expiry time. So task T1 is the earliest task to go off because it has the earliest expiry time. T2 is next with an expiry time of 12. T3 with an expiry time of 15 and so on. So this is the way the kernel is maintaining the data structure to know when a particular task's expiry time is up for processing the event associated with that particular task.</li> 
  <li>The basis for firm timer implementation is the availability of APIC hardware. APIC is advanced programmable interrupt controller, which is implemented on chip in modern CPU starting from Intel Pentium onwards, and the firm timer implementation in TS-Linux takes advantage of the APIC hardware. The good news is with the APIC timer hardware, re-programming a one shot timer takes only a few cycles. So there is not a significant overhead to re-programming a one shot timer on modern CPUs because of the availability of the APIC hardware. So if the task that is at the head of the queue, that had it's timer event go off, if it has to be reprogrammed, all that we need to do is execute a few instructions in modern processes to reprogram that one shot timer to go off at the next one shot interval. When the APIC timer expires, the interrupt handler will go through this timer-q data structures and look for tasks whose timers have expired. Some of these tasks may be periodic tasks, some of these tasks may have been programmed to deal with the APIC timer event. So, associated with each entry in this timer queue data structure is the callback handler for dealing with that particular event. And those callback handlers are going to be called by the interrupt handler upon the expiry of the APIC timer.</li> 
  <li>The expired timers are going to be removed from those timer queue data structures. If the entry corresponds to a periodic timer, then the handler will re-enqueue that particular task in the timer-q data structure after updating its expiry field for the next periodic event for that task. If it is a one shot timer that this task was using. In that case, the interrupt handler will reprogram that task for the next one short event.</li> 
  <li>The way the APIC timer hardware works is by setting a value into register which is decremented at each memory bus cycle until it reaches zero. At which point it'll generate an interrupt. Now given a 100 megahertz memory bus, for instance, on modern CPUs, a one short timer has a theoretical accuracy of ten nanoseconds. However, in practice, the time needed to feel the timer interrupt is significantly higher. And that becomes the limiting factor in the granularity that you can get with one shot timers implemented using the APIC hardware. But the important point is that APIC hardware allows you to implement very fine grained timers in modern processes. And as I already mentioned, by choosing an appropriate overshoot parameter in reprogramming the epic timer. We can eliminate the need for fielding one shot interrupts, because of soft timers going off within that overshoot period. Another optimization that we can do, in the firm time at implementation, is looking at the distance between one shot events. For instance, in this picture I'm showing you, the long distance between two one shot events. There is a one shot event happening here. There is a one shot timer event happening here. Another one shot timer event happening here. And if you have such a long distance, it is possible that there may be several periodic timer events that may be going off within this long distance. So this suggests that if periodic events are going to go off, and if it is close enough to a one shot timer that would have gone off, why not take advantage of that? So what we want to do is dispatch a one shot event at a preceding periodic event. The key thing for time sensitivity is not missing the timer event. If you're going to process it a little bit sooner, that's okay. So that's exactly what is happening here. Just as in the case of the overshoot parameter being used in combing one shot to the soft timer. What we're doing here is, because periodic timers are going to interrupt anyhow and if the kernel notices that there is a one shot event that is coming up fairly soon, then it can simply dispatch that one shot event at the preceding periodic timer event that is any how accepting the processor. And once you do that, then you can reprogram this one shot event to go off at the next expiry point for that one shot event. So basically, what we're doing when we have a long distance between one shot events, is to use a preceding periodic timer event, so that we can both avoid the overhead of dealing with this one shot event. And also the cost of reprogramming it. Or in other words, we completely eliminate using one shot events for situations where the distance between the one shot events is so big that we can simply use the periodic event instead of the one shot event. And the reasons for doing that are twofold. One is, periodic event data structures are much more efficient than the kernel, they're order of one data structures. Whereas, the one shot event programming data structures in the kernel. Tends to be order of log n, where n is the number of active timers. So as an optimization, if the one shot events happen at fairly long distances. That there are several periodic events that are going to happen anyhow within that. We will simply use the periodic timers instead of the one shot timer. So to summarize, the firm timer implementation. The first point is that the APIC hardware allows reprogramming one shot timers in few cycles. And secondly, by choosing the appropriate overshoot distance, we can eliminate the need for fielding the one shot timer interrupts if soft timers go off within that overshoot period. And third, if the distance between one shot timers is really long, then instead of using one shot timers, we will simply use periodic timers and dispatch the one shot event at the preceding periodic timer event. Those are the ideas that are enshrined in the firm timer implementation in TS-Linux, that essentially combines the advantages of one shot timer with soft timers with periodic timers. So this should give you a feel for how TS-Linux, by a clever implementation, reduces the timer latency, the first component of the latency from the point of event happening to event activation.
</li> 
</ul>


<h2>6. Reducing Kernel Preemption Latency</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/6.JPG?raw=true" alt="drawing" width="600"/>
</p>
<ul>
  <li>The second source of latency, as I mentioned, is the kernel preemption latency. Now how do we reduce the kernel preemption latency? Timer goes off when the kernel is in the middle of something and so we have to wait for the kernel to be ready to take that interrupt. The first approach is the explicitly insert preemption points in the kernel. So that it can actually look for events that may have gone off and take action. So that is the first approach. The second approach is to allow preemption of the kernel anytime the kernel is not manipulating shared data structures. Now the reason why you don't want the kernel to be preempted is because it is manipulating some shared data structures and if you preempt that that can result in some race conditions in the kernel. Bad news. So what we want to do is, we want to make sure that the kernel is not preempted while it is manipulating shared data structures. But if it is not, then we want to allow preemption. So these are two different approaches that can be combined in order to reduce the latency associated with kernel preemption. A technique, due to Robert Love, called the lock-breaking preemptible kernel combines these two ideas. And by combining these two ideas it reduces the spin lock holding time in the kernel. The idea is the following. So there will be a long critical section in the kernel where in it is manipulating some shared data structures, but it also doing other things within this critical section. So what we're going to do is apply these two principles and break this long critical section as follows. So we're going to change this critical section code into two sections: you acquire the lock, you manipulate the shared data structures, and you release the lock. And the reason you're releasing the lock is because you're done with manipulating the shared data structures, and then we will reacquire the lock, and finally release it. So essentially we replaced this long critical section by two shorter critical sections, one manipulating shared data and the rest of the code here. What is the advantage of doing this? What we can do is, at this point, once we are done with manipulating shared data, we release the lock and at this point we can preempt the kernel safetly. And since we can preempt the kernel at this point it's a great opportunity to check for expired timers at this point. If there are expired timers, dispatch them, reprogram one-shot timers if need be. All of that stuff can be done at this point, and then you can come back, reacquire the lock, and continue with the critical section. So that's the idea of the lock breaking preemptible kernel that combines these two ideas of explicit insertion of preemption points and allowing preemption any time the kernel is not manipulating shared data structures. </li>  
</ul>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/7.JPG?raw=true" alt="drawing" width="600"/>
</p>
<ul>
  <li>The third source of time latency I mentioned is the scheduling latency, that is, the timer event goes off, and we want to schedule the app that is going to deal with the timer event as quickly as possible, but the scheduler is in the way. How do we quickly make sure that that app gets scheduled? Firm timer implementation in TS Linux uses a combination of two principles. One is called Proportional Period Scheduling and in Proportional Period Scheduling what is being done is that, when a task, so these things represent tasks T1 and T2, when a task starts up it is going to say that it needs a certain proportion of the CPU time to be allocated to it in every time quantum. So there are two parameters associated with proportional period scheduling, Q and T. T is a time window, a time quantum, and this is the time quantum that is exposed to an application. And the application can say, within the time quantum T, I need a certain proportion of the time for my task. So T1 might say that, in any time T, I need two thirds of the time to be devoted to my task and if another task, T2, says in any time period T, I need one-third of the CPU to be devoted to my task, then these two requests can be obviously satisfied by TS Linux because the two add to the periodicity of scheduling T. And this is the idea behind proportional period scheduling, in which what the scheduler does is admission control at the time that a process starts up. If it asks for a certain proportion of time it sees whether it is possible to satisfy that request. So, for example, if already the scheduler has promised T1 two-thirds of the time, and T2 comes along and says I also need two-thirds of the time in every period TS Linux is going to say uh-huh, no, cannot do that, because it doesn't have the capacity to accommodate both these requests simultaneously. So that's the idea behind Proportional Period Scheduling. So it provides temporal protection by allocating each task a fixed proportion of the CPU during each task period T. And both this Q and T parameters are adjustable parameters using a feedback control mechanism. And so this, in essence, improves the accuracy of scheduling analysis that you can do on behalf of processes that are time-sensitive. The second technique that is used in TS Linux for reducing scheduling latency is to use priority scheduling. And let me motivate that by introducing a problem that plagues real time tasks and that is what is called priority inversion. So here is a high priority task C1 and it needs some service. And it gets that service by calling a server. And this call is a blocking call to the server. And the server itself may be a low priority server. For example, this client may contact a window manager to say, I need a portion of the window and this is what I want you to do in terms of painting that portion of the window. That might be a call that a high priority task is making to a low priority server and this is a blocking call and till the server is done with its work the high priority task cannot continue execution. So if you look at the timeline, C1 is running and it makes a blocking call at this point and the server takes over. And this is the service time for the server to execute the blocking call made by C1. So at the end of this service time, C1 is ready to run again. But not so fast. It could be that during the service time of this low priority server, on behalf of C1, some of the higher priority tasks C2, which perhaps was waiting for some I/O completion, becomes runnable again. If it becomes runnable again, then this higher priority task, compared to this server, is going to preempt the server and take over the CPU. </li>  
</ul>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/8.JPG?raw=true" alt="drawing" width="600"/>
</p>
<ul>
  <li>So what happens at this point is that this medium priority task, because it is higher than the servers priority, it's going to take over, preempting the server, and that is essentially a priority inversion as far as C1 is concerned because C1 is higher in priority than C2. But unfortunately, at this point of time, the server is the one that is scheduled and that is lower priority than either of these two guys. And therefore, C2 happily preempts the server and takes over the CPU. But from the point of view of C1, that's a priority inversion. And it is a time-sensitive task that affects the sensitivity of the time-sensitive tasks. This is where the priority-based scheduling of TSL saves the day for us. Basically, the idea is that when C1 makes a request to the server, the server is going to assume the priority of C1. Even though normally, it has a static priority which is lower than the priority of C1. Because C1 is making this call, the server's priority is going to be boosted to be the priority of C1 itself. So for the duration of the service time of the server the priority of the server task is going to be the same as the priority of C1, which is higher.</li>  
</ul>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/9.JPG?raw=true" alt="drawing" width="600"/>
</p>
<ul>
  <li> Now, C2, when it becomes runnable, it cannot preempt the server because the server is now running at the priority of C1. And therefore, we avoid the priority inversion that would have normally happened because of the priority-based scheduling in TS linux, so the upside is that there will be no priority inversion with this priority-based scheduling mechanism that is there in TS linux. So to recap, TS linux avoids scheduling latency to shrink the distance between the event happening and event activation by doing this admission control through the proportion period scheduler and also it avoids priority inversion by using this priority-based scheduling. So these two mechanisms allow shrinking that distance as well. The other advantage of the Proportion Period Scheduling is that TS Linux can have a control over how much of the CPU time is devoted to time-sensitive tasks so that it can reserve a portion of the time for throughput-oriented tasks. So for instance, it could say, within any period T, I'm going to reserve a third of the time for throughput-oriented tasks, so that even if there are time-sensitive tasks running throughput-oriented tasks are going to get their dibs for running on the CPU. And that way, we can make sure that while supporting the timeliness of time sensitive tasks, TS-Linux can also ensure that throughput oriented tasks are able to make forward progress. So the three ideas that are enshrined in TS Linux, for dealing with time-sensitive tasks are, first of all, coming up with the firm timer design that increases the accuracy of the timer without exorbitant overhead in dealing with one shock timer interrupts. Second, using a preemptible kernel to reduce the kernel preemption latency and third, using priority-based scheduling to avoid priority inversion and guaranteeing a portion of the CPU time to be allowed for time-sensitive tasks. Those are the ways by which the distance between event happening and event activation can be deduced and we can get good performance for time-sensitive applications even though the operating system is a commodity operating system like Linux.</li>  
</ul>

<br> <p>OH session note</p>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/oh2.JPG?raw=true" alt="drawing" width="600"/>
</p>


<h2>7. TS-Linux Conclusion</h2>
<ul>
  <li>The upshot of attacking and fixing the three sources of latency of concern for real time task is that TS-Linux is able to provide quality of service guarantees for real time applications running on commodity operating systems such as Linux. At the same time, by admission control, using the proportional period scheduling, TS-Linux ensures that throughput-oriented tasks are not shut out from getting CPU time. The proof of the pudding is in the eating. I encourage you to read the details of performance evaluation carried out in this paper that I've assigned to you for reading, wherein the authors show that both the above objectives are achieved by their design.
</li> 
</ul>


# L10b: PTS
<h2>1. PTS Introduction</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/10.JPG?raw=true" alt="drawing" width="600"/>
</p>

<ul>
  <li>In the previous lesson, we studied an operating system scheduler adaptation for providing accurate timing for upper layers of software, especially needed for real time multi-media applications. Now we move up in the food chain. The focus in this lesson is in the middleware that sits in between commodity operating systems and novel multimedia applications that are realtime and distributed.</li> 
</ul>

<h2>2. Programming Paradigms</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/11.JPG?raw=true" alt="drawing" width="600"/>
</p>

<ul>
  <li>We have seen how PThreads serve as an API for for the development of parallel programs. For distributed programs, sockets serves as an API for application development. And conventional distributor programming such as NFS servers and so on use socket API And build RPC packets on top of that. But unfortunately socket API is too low-level and insufficient in semantic richness for emerging multimedia distributed applications. Similarly sockets serve as the API for distributed program development. So conventional distributed programs are built using socket API, for example you man have a server like an NFS server, and clients connect to an NFS server using sockets, in order to do the distributed I/O between the client and the server. But unfortunately, socket API is too low-level, and it doesn't have the semantic richness that is needed for emerging novel multimedia distributed applications.
</li> 
</ul>

<h2>3. Novel Multimedia Apps</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/12.JPG?raw=true" alt="drawing" width="600"/>
</p>

<ul>
  <li>Novel distributed multimedia applications tend to be sensor-based. And these sensors can be simple sensors, like temperature sensors and humidity sensors, or more interesting and complex sensors, like cameras and microphones and radars and so on. And these sensors are distributed, which means that you have to access them via the internet. And what we want to do with these sensor streams is live-stream analysis, and often, often such applications are also called situation awareness applications. Where what we are doing is we are gathering sensor data in order to analyze in real time what is happening in the environment and take appropriate decisions. So such situation awareness applications exhibit a control loop going from sensing, prioritizing the sense data to figure out what are important. For instance, if there are lots of cameras and there is no action in front of some cameras, some cameras are more interesting than others. That is the prioritization step, of figuring out what sensors are interesting and then devoting more computational resources to analyze the camera streams or other sensor streams and as a result of that, taking some actions. And this might involve actuating other sensors, actuating triggers, actuating other software entities, or even humans, to intervene in terms of what actions were needed to be taken. And part of this control loop may also be feedback to the sensors themselves to re-target them. For example, there may be a camera that you may want to point in a different direction. And there are cameras like pan, tilt, zoom cameras, which you may want to control depending on what you are observing in the environment. And such a control loop characterization of situation awareness applications can be applied to a lot of emerging distributed multimedia sensor-based, applications such as traffic analysis, emergency response, disaster recovery, robotics, acid tracking and so on and all such applications are computationally intensive. And because they are dealing with real live streams coming from the sensors, they have real-time properties, which means that there is a need to shrink the latency from sensing to actuation, so that we can take timely  actions based on the sensed data. Since they're computationally intensive, we need horsepower to run these large-scale novel multimedia distributed sensor-based applications. And so computational engines such as the clusters and the clouds may be deployed in order to cater to the needs of these large scale, sensor-based distributed applications.
</li> 
</ul>

<h2>4. Example- Large Scale Situation Awareness</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/13.JPG?raw=true" alt="drawing" width="600"/>
</p>

<ul>
  <li>An example of a large-scale situation awareness application would be monitoring, for instance, activities in an airport. And in that situation what we are trying to make sure is that normal activities are okay, but anything anomalous, with respect to what we see in the environment has to result in triggers being sent to the appropriate software agents or even humans, in terms of actions they may need to take. And camera networks are becoming ubiquitous. For instance, in the city of London, there is on the order of 400,000 cameras that are deployed, blanketing the city. And the intent in such camera networks is to analyze the camera streams to, to detect for anomalous situations. When you have such a large number of sensors the first thing that one has to worry about is overloading the infrastructure itself with the amount of data that is being continuously produced 24/7 from all of these sensors. That suggests that it is important to prune these sensor streams at the source in order to make sure that the infrastructure overload is avoided. And the traditional way to do surveillance is to have humans sitting in front of banks of monitors looking for any anomalous activity, for instance, in an airport. This obviously does not scale if you have on the order of 100,000 or 200,000 cameras blanketing a city, which is becoming fairly common in many large cities like New York, Chicago, London, and so on. And so, cognitive overhead is another big thing that you have to worry about in large-scale situation awareness applications. And it is extremely important to avoid false positives and false negatives. With false positives we are identifying events that are not really anomalous, but the system thinks that it is an anomalous event. That's a false positive and similarly, false negative is saying, well, there is an event that should have been caught but, but did not get caught and that's a false negative. So both of these are very, very important metrics that one has to measure in large scale situation awareness applications. From a programming model perspective, one of the important take aways in just the discussion about large-scale awareness application is that the programming infrastructure has to facilitate the domain expert who is developing a large-scale situation awareness application to be able to deal with time sensitive data. So what are the pain points in developing large scale situation awareness applications? The first thing is providing the right level of abstractions, promoting ease of use. Simplicity is the key. Interfaces that allow seamless migration of computation between the edge of the network and the computational resources in a data center, for instance. Temporal ordering of events that are taking place in the distributed system, and propagating temporal causality. By that, what I mean is if an event, lets say, was captured by a camera at a particular point of time, it make be processed at a later point of time in real time but there's got to be a temporal causality for the time at which it was captured and the digest of information that was created. And such temporal causality is extremely important in the development of large-scale situation awareness applications. Another requirement in such large-scale situation awareness application is that there are live data that we are dealing with, but at the same time we may also have to correlate that with historical data in order to make a high-level inference as to what's going on. For instance, if we see a speeding car at this point of time on the highway we may want to ask the question, was this car involved in an incident in the last n days? And that is kind of historical data that has to be correlated with live data. So a programming infrastructure that provides facility for these kinds of things would make the life of the domain expert, writing such large-scale situation awareness applications, so much simpler.
</li> 

</ul>

<h2>5. Programming Model for Situation Awareness</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/14.JPG?raw=true" alt="drawing" width="600"/>
</p>

<ul>
  <li>Let's look at the computation pipeline for a Video Analytics application. Let's say I'm interested in detecting and tracking some anomalous event. So I have cameras, which captures the images from a particular environment. And you're detecting for something specific. Maybe you're looking for Kishore's face in every camera frame, and once you detect Kishore's face then you say it what? This guy is a suspicious individual, let's track him as he moves around. So, the tracking is taking a particular object of interest for the domain and asking the question What is happening to that object as time moves on. And in the process of tracking, you are recognizing who that person is. If there are multiple people in a particular alignment, your recognizing the specific individual that you may be interested in. And if that individual is recognized, then perhaps you want to raise an alarm. So, this is just a simple pipeline of tasks to illustrate what a domain expert may be doing in a video analytics application. So such a domain expert is an expert in writing detection algorithms, tracking algorithms, and. Recognition algorithms so that they can process the video stream in real time to generate alarms. So the objective in Situation Awareness applications is to process these streams of data for high level inference. In other words, we're not watching a YouTube video, but we're using the sensor streams to derive high level inferences to what is happening in the environment. And that's what is the nature of Situation Awareness applications. As I mentioned Video Analytics is in the purview of a domain expert like a vision researcher or a developer, who knows how to write sophisticated detection, tracking, and recognition algorithms. But how do we scale that up to thousands of cameras? If an object is moving from one camera to another, these are the kinds of things that the domain expert. For Situation Awareness may have to worry about, this is where systems can come in with programming models that alleviate the pain points of a domain expert developing Situation Awareness applications. Persistent temporal streams, which is the focal point of this particular lesson. Is just one exemplar of a distributed programming system for catering to the needs of situation awareness applications. Such as, what I described to you, just now. I want to stress the fact that PTS is just an exemplar, not the last word. But it is good to look at one concrete example of a distributive programming system that can help reduce the pain points in developing Situational Awareness Applications.
</li> 
</ul>

<h2>6. PTS Programming Model</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/15.JPG?raw=true" alt="drawing" width="600"/>
</p>

<ul>
  <li>The PTS Programming Model is very simple and straightforward. It's a distributed application and in the distributed application, there are threads and channels. These are the two high level abstractions provided in the programming model. And the computation graph that you generate using the PTS programming model, in particular the extractions thread and channel provided with the PTS programming model. The computation graph looks very similar to a UNIX process socket graph. So in terms of a transition for a programmer that is familiar with the socket API. To transition to programming, using the abstractions provided by PTS, is not significant. While the computation graph looks similar to a process socket graph, the semantics of the channel abstraction is very different from the socket abstraction. In particular, what the channel holds are time sequenced data objects. So a thread is generating time sequence data objects can place those time sequence data objects into the channel. The other thing you will also notice from the structure of this computation graph that there could be multiple producers of data that go into a given channel And similarly, there could be multiple consumers of data that are getting data from a given channel. Or, in other words, a channel allows many to many connections. A thread that produces some data and wants to put it into a channel uses the primitive provided for. Operating on the channel, namely the put item primitive and the put item takes two arguments, one is the item itself, which is whatever the application decides as the content of the data that it wants to put in the channel. The time stamp that it can associate with the data that it is producing and placing in the channel. So if you look at the contents of any given channel, it's really showing a temporal evolution of data that is being produced by a particular thread. For example, this thread could be capturing data from a camera and every item that it produces is one frame of image captured from the camera associated with the wall clock time. At which that particular frame was captured. A computation that is needing to use the data will do a get on a channel and when it does a get on a channel it actually has to specify two arguments. One is a lower bound for a time stamp that it is looking for and an upper bound. Or in other words a thread that wishes to get data from this channel, for instance, will specify in the get primitive that I want to get the data that is in a particular channel starting at a particular time. Say 1:05 p.m., To an upper bound, 1:06 p.m. And what the thread is going to get back is all the items that this channel contains between this lower bound and upper bound. The programming model also allows an application to specify time variables in an abstract way. It could say, I want the oldest item from a channel, or the newest item from the channel. In addition to specifying explicit lower bound and upper bound markers if it so chooses. So in other words, a channel contains a continuous stream of data that have been snapshotted with different time stamps associated with them. So, this is the way one could write a program that uses the primitives provided by the PTS programming model. Let's say we are writing the code for a detector that is looking at video stream produced by a particular camera. In that case, the detector will associate a variable channel one with the video stream that corresponds to what that camera's producing. And continuously what it is going to do is, it is going to get a camera image from that particular channel. And it might specify a lower bound and an upper bound in terms of time that it is looking for. Process the data, produce an output, and it might place the output. That it produced as a digest of information that it processed into another channel, and it'll do that by putting the digest as a new item, and associate a time stamp. This is a place where the programming model facilitates. Temporal causality to be propagated in the distributed system. So if this thread is a detector thread that is looking at the camera images produced by a camera thread, then it can take the camera images, process it. And place it in its output channel, and when it places it in the output channel, it can use the same time stamp that it got for an input image in doing the put operation, and that way, there is temporaral  causality that is being propagated through this graph by the computation using the primitive that is available from this PTS programming system. Quite often in situation awareness application, the computation may have to use data from several different streams in order to do its high-level inferencing. So a particular thread may get information from several different streams, and it may have to make high level inference based on the composite results that it is getting from all these streams. And since all the data that is coming from the channels have time stamps associated with them, an intermediate computation like this that is taking multiple streams and generating a high level inference from it, can use the time stamps in the incoming streams to see which data items in these streams are temporally correlated with one another. And use that information to improve the kind of inferencing and hypothesis that can be derived in the situation awareness application. So in a nutshell, the PTS programming model, what it allows, is the ability to associate time stamp with data items produced by a computation. So the model allows propagation of temporal causality because the fact that a computation can associate a time stamp with an item that it produces based on the time stamp that it got. On the data that it obtained from its input channel and thirdly the fact that every stream is temporally indexed allows a computation to correlate the incoming streams. And recognize which data items in the input streams are temporally related to one another by looking at the time stamps associated with the items that it is fetching from the different channels.
</li> 
</ul>

<h2>7. Bundling Streams</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/16.JPG?raw=true" alt="drawing" width="600"/>
</p>

<ul>
  <li>Another common need in building such situation awareness applications is that a computation may need to get correspondingly timestamped items from several different sensor sources. So, for instance, in this picture I'm showing you a video source, an audio source, a text source, and gesture source. And this computation, in order to do its high level inferencing, it wants to use the multiple modalities of sensing that's available to generate high level inference that can be more robust. And this is where bundling of streams that PTS allows comes in handy. PTS allows streams to be grouped together, as a stream group. And any number of such streams can be grouped together and labelled as a group. And in creating a stream bundle like this, a computation can specify that, for this stream group, this guy is the anchor stream. For instance, we could name the video as the anchor stream for this particular stream group. All the other streams within the same stream group are dependent streams on the anchor stream. And in PTS there's a primitive for getting a group get, say, get corresponding timestamped items from all of the streams in a particular stream group. The group get primitive is once again a way of reducing the pain point for a domain expert. And having to go out and fetch individual items from each one of these streams, and selecting temporally correlated items from all the other dependent streams. That burden is taken away by PTS by providing this group get, where you can get correspondingly timestamped items from all the streams in a given group.
</li> 
</ul>

<h2>8. Power of Simplicity</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/17.JPG?raw=true" alt="drawing" width="600"/>
</p>

<ul>
  <li>I mentioned that any system design should be simple. And power of simplicity is the key for adoption. So in this case, let's revisit the sequential program for video analytics that I showed you earlier. Converting the sequential program for video analytics into a distributed program, using the get put primitives, provided by PTS, is fairly straightforward. What you do is, you interpose between these computations, that are there in the original pipeline, channels that are named entities that can be used to hold the temporal evolution of output from a particular computation. So in this case, camera capture is a thread, and this capture computation captures images from a camera with a certain periodicity. And places it into a name channel called frames, and so the frames channel abstraction is going to contain the temporal evolution of output produced by this capture computation. And the detector can discover where this frame channel is, connect to it, and get images from this channel, process them and produce blobs that characterize the objects that it sees in any particular given frame. So this is a temporal evolution of the detector's output corresponding to the frames that it sees. And the tracker takes these blobs. And interesting blobs from that are the objects that it is tracking and it produces a temporal evolution of the location of objects that it sees and places it in its output channel and a recognizer may then look at these objects and consults. A database of known objects to see whether any of those objects that are being continuously captured, detected, and tracked, correspond to anything that may indicate that there's an anomalous situation. And those are the events that it produces, and those events may trigger an alarm. So basically, what, what I'm showing you here is converting the sequential program for video analytics into a distributed program using the channel abstraction and the get/put primitive available in the PTS programming model. So the ovals are the threads of the PTS abstraction and these rectangles are the channels that are there between any two computational entities.
</li> 
</ul>

<h2>9. PTS Design Principles</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/18.JPG?raw=true" alt="drawing" width="600"/>
</p>

<ul>
  <li>PTS provides simple abstractions, namely channel and simple perimeters to manipulate these abstractions, namely get and put. And these channels can be anywhere in the distributed system, just like UNIX sockets, these are named entitities. That are network-wide unique and therefore, you can have a large scale distributed computation in which channels exist everywhere and you can discover them and connect to them and do I/O on them using get and put. All the heavy lifting that is the systems work that needs to happen. In order to support this channel abstraction and the operations that you do want channel abstractions like the get and put operations, are all managed under the cover by the the run time system of pts. So PTS channels can be anywhere, just like unix sockets, can be accessed from anywhere, and they are network wide unqiue, and this is a similarity of the channels to unix sockets, in terms of the ubiquity,in the entire distributive system. But what makes PTS channels particularly attractive for situation awareness applications is that the run time system and the APIs, provided for manipulating the channel Treats time as a first class entity. What we mean by that is time is manipulatable by the application, in the way it specifies items to the runtime system. And it queries the runtime system using time as index into the channel, and that's what is unique about the PTS channels. The second unique property of the PTS extraction is that, it allows streams to be persistent under application control. Remember that these sensors are producing data 24/7. Therefore continuously data is being produced, and all of it cannot obviously be held in the memory of the CPU, so they have to be persisted on more archival storage like a disc, and PTS provides the ability to persist the streams under application control. The flip side of persisting streams is that when a query comes in. For a particular channel, it is actually saying get me items from a lower bound to an upper bound, and the lower bound may be yesterday, which means that we are asking for data that are historical in nature in addition to live data. So the PTS run time system and the semantics of channels, allows seamlessly handling live and historical data. The perimeters are just get and put, whether we are accessing data that is current or historical. You give a lower bone marker and an upper bone marker and the run time system gets to work in doing whatever heavy lifting that needs to be done. To bring the data that you're looking for between the lower bond marker and the upper bond marker. So that's the power of the PTS channel perimeter. It is simple to use, but at the same time it provides handles that will make the life of the domain expert that is developing. A situation awareness application, so much easier because it allows time to be manipulated as a first class entity recognized by the programming system. And it allows persistence for data beyond the lifetime of a computation by moving it to archive storage under application control. And thirdly, it allows for seamlessly integrating live and historical data.
</li> 
</ul>

<h2>10. Persistant Channel Architecture</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l10/19.JPG?raw=true" alt="drawing" width="600"/>
</p>

<ul>
  <li>Having given you the abstractions and PTS, and the simplicity of the programming model. I will now introduce you to the heavy lifting that needs to happen under the covers in order to support this simple programming model from the point of view of the domain expert developing a situation awareness application. All the computations in the application can be considered as either producers or consumers of data. Producers of data, are putting things into the system and consumers of data are getting stuff from the system. And under the covers there are worker threads in the run-time system of PTS that react to these get and put calls coming from the producers and consumers. For instance, whenever, a producer puts an item, that results in new item triggers. That are going to be generated by the worker threads to the rest of the implementation. Implementation of the channel architecture is a three layer architecture. The top layer is the live channel layer of the architecture. And this is the layer that reacts to the new item triggers coming from these worker threads, working on behalf of a put call that is coming from a producer. So the channel abstractions result in these new item triggers to be sent to the live channel layer. Of the channel architecture. And the live channel layer, is the one that is holding a snapshot of items that have been generated on a particular channel. Starting from the oldest item in the channel to the newest item that just came in because of this new item trigger At the time of creation of a channel, the creator of a channel could specify what the semantics of the data that are kept in this channel are. In particular, a creator of a channel could say that, what I want the channel to contain are live data corresponding to a certain snapshot of real time. From oldest to new. For instance, I could say keep only the last 30 seconds of data in the channel. Rest of it, you can throw it away. So there is a garbage collection trigger that is part of the run time system, that is looking at information that is in the channel, that says. What items in the channel have become old and therefore can be thrown away. Those are the Gc triggers. And the Gc triggers will move data that have become ancient, so far as this channel is concerned, and move it into this garbage list. Meaning that these items are no longer relevant from the point of view of this application. And therefore they can be garbage collected. So they're put into this garbage list. And there is another garbage collection thread that is responsible for periodically cleaning up all the garbage that has been created and throwing away stuff that is no longer relevant. For this computation. So this is all the channel book keeping that's happening, under the covers in support of an application that is using the PTS library. But there's a lot more to it than just dealing with live data and data that is no longer relevant that has to be garbage collected or thrown away. As I mentioned one of the features of the PTS architecture is the fact that an application can choose to keep data for as long as they wants, and that is the persistence property supported by PTS run time system. So once again properties of the channel An application programmer could specify that, I don't want to throw away stuff that becomes old to keep in the channel, but I want to archive them, I want to persist them. And if those properties have been associated with the channel, then when items go past the window that has to be stored in the Live Channel Layer. The Live Channel Layer results in generating what are called persistent triggers to indicate that some items have become old in this channel, and they have to be persisted. The second main functional layer in the channel architecture is the Persistence layer. The Interaction layer is just a go between the Live Channel layer and the Persistence layer of the channel architecture. And what the Persistence layer does is, based on the persistence triggers that it gets from the live channel layer. It is going to take items from the channel and decide how to persist them. Now, here again, the application can have a say in how items need to be persisted. And they do that by having a Pickling Handler. That is, the application can specify Here is a function that I want you to use every time you decide to persist some item from the channel. For example, an application may specify that don't store all the images as is on archive or storage, but Condense them in such and so fashion. And that is a function that it can supply. And the runtime system, when it works on persistence, will automatically apply the application-specified function on the items that need to be persisted to create a digest. Which will then be persistent. Items that need to be persistent necessarily have to go to non-volatile storage devices, and here again, the PTS architecture supports several different configured Backends to store items that need to be persistent. And the Backend layer is the third layer in the channel architecture. And PTS supports several different Backends to support the persistence activities and it is an application choice as to which back end layer it wants to use for its specific application. The Backend layers supported by PTS include mySQL, it can use Unix file system as a persistence layer, or it can use a file system from IBM called GPFS. So mySQL, Unix file system, and GPFS are the three different Backends that are available. For the persistence layer to store channel data that needs to be archived for later retrieval. The nice property is that all of the persistence activities happen unbeknownst to the user. All that the user has done is in the creation of a channel. Specified certain properties to associate with that channel. For example, the property that may have been associated with a channel is that any items beyond the last 30 seconds persistent on the storage, and when you persist them on the storage, apply this function. Those are the things being specified by the user creation time of the channel, once that is done the heavy lifting that needs to be done during run time, is all handled under the covers, by the run time system. Of the channel architecture that takes items from the channel, pickles them using the function that has been specified, and uses one of the configured Backends to push those pickled items onto the persistent storage. On the other side, when. An application wants to get an item. That item range may span from something that is there in the live channel part of the three layer architecture, or it could be on the archival storage. Now it is up to the run time system to retrieve all the items between the lower bond. And the upper bound specified by get primitive. So get primitive that spans both live and archived items results in get triggers being passed from the live channel through the interaction layer to the Backend so that corresponding to the get interval that is specified. The Backend layer can pull the data from the archive storage and pass it up so that it finally gets to the application. What I wanted to illustrate through this picture is there is a lot of heavy lifting that needs to happen in order to support a simple programming model. The programming model is very simple But, in order to support the simplicity, all of the heavy lifting has to be absorbed under the covers in the run time system of the PTS programing model.
</li> 
</ul>

<h2>11. PTS Conclusion</h2>

<ul>
  <li>Just as MapReduce provides a simple and intuitive programming model for the domain expert to develop big data applications, PTS provides a simple programming model for the domain expert to develop live stream analysis applications. Time-based distributed data structures for streams, automatic data management, transparent stream persistence. These are then unique features of the PTS programming model that facilitates live stream analysis. I invite you to read the paper in its entirety to understand the many systems challenges that are identified and solved by PTS in providing this programming model. This completes our discussion in this course on the topic of real-time and multimedia.
</li> 
</ul>


<!-- <h2></h2>

<p align="center">
   <img src="" alt="drawing" width="500"/>
</p>

<ul>
  <li></li> 
  <li></li> 
  <li></li> 

</ul> -->
