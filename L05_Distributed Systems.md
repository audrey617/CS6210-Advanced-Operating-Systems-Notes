# Lesson outline
- [L05a: Definitions](https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/L05_Distributed%20Systems.md#1-definitions-introduction)
- [L05b: Lamport Clocks](https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/L05_Distributed%20Systems.md#l05b-lamport-clocks)
- [L05c: Latency Limits](https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/L05_Distributed%20Systems.md#l05c-latency-limits)
- [L05d: Active Networks](https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/L05_Distributed%20Systems.md#l05d-active-networks)
- [L05e: Systems from Components](https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/L05_Distributed%20Systems.md#l05e-systems-from-components)


# L05a: Definitions
<h2>1. Definitions Introduction</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/1.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>We now launch into the study of distributed systems. This is an exciting field. Part of the fun, but there's a lot of parallels between distributed systems and parallel systems. What fundamentally distinguishes a distributed system from a parallel system is the individual autonomy for the nodes of a distributed system as compared to a parallel system, and the fact that the interconnection network that connects all the nodes in a distributed system is wide open to the world, as opposed to being confined within a rack, or a room or a box.</li>
   <li>However, as the feature size of transistors in silicon continues to shrink due to advances in process technology and break throughs in VLSI technology, many of the issues that are considered to be in the domain of distributed systems and are surfacing even within a single chip.</li>
   <li>In this lesson we are going to learn the fundamental communication mechanisms in distributed systems and what an operating system has to do to make communication efficient. As in the previous lessons, you will see the symbiotic relationship between hardware in the form of networking gear and the operating system software stack, particularly the protocol stack, to make the communication efficient. We'll start this lesson module with a definition and a shared understanding of what we mean by a distributed system. But first, a quiz to get you started.</li> 
</ul>

<h2>2. What is a Distributed System</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/2.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>The third choice is that a distributed system is one in which events that are happening on the same node here like A and B. The time between that is called the event time.</li>
   <li>The event that is happening across nodes, which is a communication event, Node N1 sends a message to node N2, it's a communciation event from A to C. </li>
   <li>The third choice is saying that communication time Tm is much significantly more than the event execution time Te.</li> 
</ul>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/3.JPG?raw=true" alt="drawing" width="500"/>
</p>


<h2>3. Distributed Systems Definition</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/4.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Number one. So a distributed system is a collection of nodes that are interconnected by a Local Area Network or a Wide Area Network. This Local Area Network may be implemented using a twisted pair, coaxial cable and optical fiber. If it is a Wide Area Network, it could be implemented using satellite communication, microwave links, etc. And the media access protocols that may be available for communication of these nodes on a Local Area Network or a Wide Area Network, maybe ATM or Ethernet and so on. That's sort of the picture of What a distributed system is.</li> 
   <li>Number two. There's no physical memory shared between nodes of the distributed system. So the only way nodes can communicate with one another is by sending messages on the local area network to one another. So there is no shared memory (no physical memory) for communication between the nodes of the distributed system. </li> 
   <li>Number three. The event computation time is the time it takes on a single node to do some meaningful processing, Te. A node may also communicate with other nodes in the system called the communication time or the messaging time, Tm. The third property of the distributed system is that the time for communication between nodes in the system, Tm is much more significant than the event communication.</li> 
   <li>So these are the three properties which I would like to think of to make sure that we have a shared understanding of what we mean by distributed systems. They are connected by some sort of local area network or wide area network. A collection of nodes with their own physically shared memory. The only communication or the only way they can communicate with one another is via messages that are sent between the nodes using the local area network. And the third property is the fact that the message communication time is significantly larger and even computation time that happens on a single node.</li> 
   <li>You probably remember a good friend, Leslie Lamport. I introduced you to him when we talked about parallel systems, and I said we would see him again. In parallel systems, he's the one who gave us the notion of sequential consistency, and the same person that we're going to be talking about in this lecture, Leslie Lamport. And in particular, Lamport has a definition for a distributed system. The definition of a distributed system verbatim goes like this, "A system is distributed if the message transmission time, Tm, is not negligible to the time between events in a single process." Cause there's a time between events in a single process, there's a message transmission time. So the definition that Leslie Lamport gives is "A system is distributed is a message transmission time, Tm, is not negligible compared to the time between events in a single process."</li> 
   <li>What is the implication of this definition? Interestingly, even a cluster is a distributed system by this definition. We talked about clusters a lot when we discussed parallel systems, and I told you that clusters are the workhorses of data centers today. Even a cluster is a distributed system by this definition because processors have become blazingly fast, so the event computation has shrunk quite a bit. On the other hand, the message communication time is becoming better but not as fast as the computation time that happens on a single processor, and therefore even on a cluster which is all contained in a single rack in a data center, the message transmission time is significantly more than the event time. So, even a cluster is a distributed system by this definition.</li> 
   <li>The importance of this inequality (Tm >> Te) is in the design of algorithms that will span the nodes of the network. What we want to make sure is that because the message transmission time is so significantly larger than the event computation time on a single node in structuring applications that run on distributed nodes of a system like this, one has to be very careful to make sure that the computation time in the algorithms you're designing is significantly more than the communication time. Otherwise, we are not going to reap the benefits of parallelism, if most of the time you're communicating. That's why this definition of the distributed system is extremely important.
</li> 
</ul>


<h2>4. A Fun Example</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/5.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>We're going to look at a fun example. This is me, and I'm going to India for Christmas holidays. And I'm going to make an airline reservation. I'm going to use Expedia to make the airline reservation. So what I'm doing on my computer, I'm sending a message to Expedia, saying, "hey, make a reservation for me". And Expedia chooses to make the reservation using Delta, so it sends a message. a to b is a message that I sent to Expedia saying I need a ticket to go to India, preferences, and so on. Expedia then sends a message to Delta booking the reservation that I want. Delta confirms by this message, e to f, that yes, Kishore's reservation is in. And once Expedia has received this confirmation from Delta, it sends me a message, g to h. And this message is telling me that I've go the airline reservation booked. So all of these are messages. a to b is a message, a is the sending of the message, b is the receipt of the message. And c is the sending of the message from Expedia to Delta. e is the confirmation that my reservation is in from Delta to Expedia and finally g to h is the message from Expedia to me saying that "yes, you have your reservation, you can go to India in December". That's good. And then, what I'm doing is, I'm directly contacting Delta, message from me, me to Delta, asking for my preference for food. Fortunately, it's an international trip, so I'm going to get a little bit more than peanuts on the Delta flight to India. So I sent a message asking for my meal preference and Delta confirms that "yes, you have your meal preference". That's the message k to l, is the message that confirms that I have my meal preference, I'm all set. So everything that I've described here is what you probably do on a routine basis, every time you're making any travel plans either contacting Expedia or some other web portal to make your airline reservation. All of this makes logical sense, right?</li> 
   <li>There are several beliefs that are ingrained in this picture here about the ordering of events in the distributed system that makes all of this work. In particular, when we look at the set of events that you're seeing here as events that I'm responsible for, we think that these events are happening in sequential order. So for instance, if you look at what Expedia is doing, it is receiving my message saying that I want an airline reservation to be made, does a bunch of bookkeeping, then sends this message over to Delta saying "well, go ahead and make this booking for him", gets the acknowledgement back from Delta. And then it does a bunch of other bookkeeping, once it gets the acknowledgement from Delta and then it tells me "okay, you've got it." And after that, it does some more bookkeeping to say "well, you know kishore's booking is done and I'm going to make some internal notes on the details of this booking." And so those are all things that are happening as events within Expedia.</li> 
   <li>So the belief that we have is that processes are sequential, that is, the events that we see happening in a given process, these are the events that are happening in a given process, we expects these events to be totally ordered, right? So for instance, you wouldn't expect given this ordering of events, that you see in Expedia's profile that this event m happened before sending this message c, right? So that's the mental model that you have, that events are totally ordered within a single process, and that's why we're calling process sequential. That is, the execution of a process is a textual order that you see. At least the apparent effect of the execution of the process that you, as a user, experience is sequential. So if I look at this particular process, h happens before i, f happens before g, and d happens before e, so all of these are things that are ingrained in our mental model of processes being sequential.</li>
   <li>The other belief is that you cannot have a receipt of a message before the send is complete, right? So you have to send the message before it can be received. In other words, the receipt of a message, which is b here, has to happen after the messages are being sent from here. Similarly, this message reception f must have happened after the message was sent from Delta. </li>
   <li>So those are the core beliefs that we have about what is happening with events in a distributed system. That events within the process are sequential. And across processes, when you have communication events, send happens before receive. So these are two core beliefs that we have about the working of a distributed system. And we call these beliefs, as the  "Happened Before" relationship.
</li> 
</ul>

<h2>5. Happened Before Relationship</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/6.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So let's dig a little deeper into what we mean by the Happened Before Relationship. I'm going to denote the Happened Before Relationship with this arrow. A happened before B. That's what this notation means. What this notation is implying is one of two things, either A and B are events in the same process which means given a belief that a process is sequential, A must have happened before B. If it was a textual order A is here and B is here then A must have happened before B and that's one possibility. Or if you're asserting that A happened before B and A and B are not events on the same process but A is an event in one process, B is an event in a different process. Then there must be a communication event that connects A and B. In other words, if A is a communication event of a message, and B is a receipt of that same message. Then, A happened before B, where A is the sender of the message and B is the receiver of the message. So, this is the implication of saying that an event in a distributed system A happened before B, and these events can be anywhere in the system. An event could be happening A, and another event could be happening B, and if we are asserting that A happened before B, what we are implying is one of these two possibilities: 1) A and B are events in the same process; 2) A is the act of a sending a message and B is the act of receiving the same message on a different node of the distributed system.</li> 
   <li>The other property of the happened before relationship is that it is transitive. What I mean by that is, if we're asserting that there is an event A that happened before B. And this event B happened before C. The implication is this relationship is transitive ad therefore A happened before C. So that's the transitivity of the Happened Before Relationship. Now that I introduced to you the Happened Before Relationship, it is time for another fun quiz.</li> 
</ul>


<h2>6. Relation</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/7.JPG?raw=true" alt="drawing" width="500"/>
</p>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/8.JPG?raw=true" alt="drawing" width="500"/>
</p>

<h2>7. Happened Before Relation (cont)</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/9.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Now that we understand the "Happened Before" relationship and the transitivity of the "Happened Before" relationship. I also want to introduce this notion of concurrent events. That is, concurrent events are events in which there is no apparent relationship between the events. So, for instance, I'm showing you two nodes of the distributed system. a is an event on one node, and b is an event on another node. Since a and b are not events on the same node, you cannot apply the sequential process condition to say that there is an ordering between a and b. By the same token, since a is an event here, b is an event here, and there is no communication between these two guys that connects these events in any shape or form, either directly or transitively. There is no ordering between a and b, these two events are concurrent events, not sequential events, not connected where they happened before relationship, but they're concurrent events. So, in other words, We cannot say anything about the ordering of a and b in the distributed system. </li> 
   <li>This is the fun thing about a distributed system. This "Happened Before" relationship (which looks at either events on the same process or events across process is connected by communication and the transitivity of "Happened Before" relationship through the native "Happened Before" relationship) gives at best a partial order for all the events happening in the system, so there's no way for us to derive a total order by looking at the events that are happening on the same process or just looking at the events that are happening on the different processes in the distributed system. </li> 
   <li>This is a very good example of why it's impossible to get a total order for all the events happening in the distributed system. Because there are events going to be concurrent, that's the nature of the game which these processors are executing asynchronously with respect to one another. Therefore, the event happening over here. You know, if I want to look at wall clock time in one execution of the distributed program, it's possible that a in real time happened before b, but when I execute the same program again, it could be that this event b happened before a. And therefore, these two events are called concurrent events.</li> 
   <li>So the important point about these concurrent events in the "Happened Before" relationship is that in structuring a distributed algorithm, it's important to recognize what events are connected by the happened before relationship and what events are concurrent events. Once you have an understanding of these two concepts, then you can build robust distributed systems and robust applications. Because if you have any assumptions about the ordering of events that are unconnected by communication like this, that can lead to an erronious program. So one of the banes of distributed programs is synchronization and communication bugs and timing bugs. And this is a classic example of a timing bug that you can have if you mentally think that a happened before b and that's the way you want it to happen. It may not happen because these two events are concurrent events. Now that I've introduced to you all important basics of events and ordering of events in the distributed system. It's time for another quiz.</li> 
</ul>


<h2>8. Identifying Events</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/10.JPG?raw=true" alt="drawing" width="500"/>
</p>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/11.JPG?raw=true" alt="drawing" width="500"/>
</p>

<h2>9. Example of Event Ordering</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/12.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Returning to our original example of me ordering a ticket to India via Expedia and Delta. Let's now identify all the events that are connected directly by the "Happened Before" relationship. So, if these are the events in my process, then we know that A happened before H. And, we know that H happened before I. And, we know that I happened for L. So that's the textual ordering, and we know the process is sequential. This is the ordering of events in my process. And similarly, we can see that if these are the events in Expedia's process. Then all of these events have to be sequentially ordered. So, B should have happened before C, C should have happened before F, F should have happened before G, and G should have happened before M. So these are the orderings of the events in Expedia's process and similarly we can derive the order of events in the delta process as sequential. </li> 
   <li>These are all the communication events that are directly relating events happening between any two processes that I'm showing you in this picture. So, for instance, E to F is a message from Delta back to Expedia confirming my reservation. So E is the act of sending the message from Delta, and F is the act of receiving the same message from Delta on Expedia. So, those are all the communication events.</li> 
   <li>You can also look at transitive events, so for instance. What is the relationship between, let's say, event E and event A? Well, it turns out that A must have happened before event E. And the reason is, if you look at A, it happened before B, B happened before C, C happened before D. All of these are communication events, pretty straightforward. So from here to here, it's not a communication event, but since the process is sequential, B should have happened before C, and C happened before D. So, since it's a communication event, sequential process D should have happened before E. And that's what gives a transitive relationship between A and E, that A must have happened before E. Similarly, we can identify other events that are transitively connected to one another because of the "Happened Before" relationship. So, for instance, D and M apparently don't have any direct connection. But, through the transitivity of events that are happening sequentially and through the communication and sequentiality of a process, we know that D must have happened before M. So those are transitive events.</li> 
   <li>Finally, let's look at concurrent events. If you think about this event M that's happening in Expedia. Basically, Expedia has confirmed that I have the booking that I want. Then it is doing some internal bookkeeping to record some information about me, maybe my preferences in terms of airlines. And from that point of view, it is making some internal bookkeeping and that's is event M. And now, if you look at this event M, it has no relationship to any of the events happening here. I'm showing G, this event H must have happened after G, but what about H and M? There is no relationship between these two guys. This could've happened much later than this in wall clock time. Or it could've happened much sooner than event M. So you can see that H is concurrent with M. In fact, all the events that you see here (HIJ) are going to be concurrent with M. And similarly, all the events(JK) that you see over here, they're concurrent with So in fact, they're concurrent with M. After this, if you look at the Delta process, after Delta has sent this message to Expedia confirming my booking, it may have done a whole bunch of events, over here. All of those events are concurrent with M, because there is no ordering between these events and the events over here. But there is an ordering between the event, G and the event J here. Because G happened before H, H happened before I, I happened before J. So, you can see that transitivity connects events across machines. But they could be events that are happening in the distributed system that are unconnected to other events. And those are concurrent events. That completes the discussion of the basics of the distributed system. Next, we're going to start talking about Lamport clock.
</li> 
</ul>


# L05b: Lamport Clocks
<h2>1. Lamport Clocks Introduction</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/13.JPG?raw=true" alt="drawing" width="500"/>
</p>

<h2>2. Lamport's Logical Clock</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/14.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>What does each node in the distributed system know? Every node knows its own events, which are the computational events happening in its own execution. It also knows about its communication events with the rest of the nodes in the distributed system. So, for instance, if this process Pi sends a message to process Pj, that's a send event. Pi knows about that. And similarly when Pj gets this event, it's a receive event. It knows about that. So, these are the only two kinds of events that every node knows about: 1) its own computational events; 2) its communication events with its peers. Process Pj would have no idea about the local computational events of Pi.</li>
   <li>Lamport's logical clock builds on this very simple idea. The idea is that we want to associate a timestamp with every event happening in every process in the entire distributed system. How are we going to do that? We're going to have a local clock, Ci over here, And Cj over here. And this local clock can be anything. It can be a simple counter. When I want to associate a timestamp with a particular event in my process, I'm going to look at this counter and see what the value of the counter is, then associate that counter value as a timestamp for this event a. For this instance, I've said that the timestamp for a is two. And this counter monotonically increases as we pile up events in our system. So once I've associated this timestamp two with this event a, I cannot associate the same timestamp with other events in my process. Therefore, I'll increment the timestamp, which completely up to your implementation as to whether you want to increase this counter value by one, or two, by a thousand, it doesn't really matter. So, in this case for instance, I've incremented the timestamp counter by two, and so the next event b is going to have a timestamp of four. So that's the idea behind having a monotonically increasing counter to associate logical timestamps with the events in my process.</li>
   <li>What about these communication events? Well, in particular, let's look at this case here. a is a communication event on process Pi (a(2)). And this communication event is going to have a timestamp of two associated with it because that's when I generated this communication event and sent this message over (Tm=2). When process Pj receives this, that's an event. And so let's say we call it event number d. And we have to associate now a timestamp with event d. How do I go about assigning a timestamp with this? We know that this timestamp that I'm going to associate with this event d has to be greater than the timestamp associated with the send of that message, right? Obviously, you cannot receive a message that has not been sent yet. And therefore, we're going to say that d should have a timestamp which is at least greater than a for sure. Now, what else does d depend on? It depends on other things happening in my own process Pj. For that, I need to know the current state of my local counter. So, for instance, in my execution, as I'm showing you here I haven't done anything meaningful yet, therefore my local counter is still pointing to zero indicating that there have been no meaningful events here (Cj = 0). So when this message comes, that's the first time I'm going to do something meaningful in this process. And I have to associate a timestamp with d, but I cannot associate the timestamp of zero with it because the timestamp that I associate with d has got to be greater than the timestamp associated with the send event on process Pi. And since the send event has this timestamp two, I have to associate something higher than that. And so I associate a timestamp three with the receipt of this message (d(3)). This particular event will have a timestamp of three. </li>
   <li>So, the two conditions that we have talked about: 
   <ul>
      <li>One is that events that are happening in the same process. I'm going to have a monotonically increasing counter and I'm going to use that to associate timestamps with the events happening in the same process. That's the first thing. If I have two events a and b in the same process. I know that a happened before b, because, textually, in the process, the process is sequential, a happened before b. Therefore the condition is that the timestamp associated with a has got to be less than the timestamp associated with b. Pretty straightforward</li>
      <li>The second condition is that if I have two events, a and b, and a happens to be one send event on one process and d happens to be the received event on another process. We know that the received event has to be after the send event. And therefore, the second condition is that Ci(a), which is a timestamp associated with a send event, has got to be less than the timestamp associated with the receipt of the same message, Cj(d). So in order to make this condition valid, we're going to choose the timestamp to associate with the receipt of a message d as the max of the timestamps. The send event is incremented by some quantity, which is Ci(a)++ (since Ci(a) = 2, Ci(a)++ = 3) and then you want to know the local counter Cj. So what we pick as a timestamp to associate with d is the MAX of the incoming timestamp with the message (Ci(a)++) and the local counter whatever it is pointing to (Cj).</li>
    </ul>
     So that's how I'll pick the timestamp to associate with a receive event. This brings up a very interesting question "what about timestamps of events happening concurrently in a distributed system?". Before I talk about that, I want to give you a little quiz.
</li> 
</ul>

<h2>3. Events</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/15.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>Let's say in the distributed system, there are two events. I don't know where they are happening. There's an event called a and there's an event called b. Somewhere in the distributed system, these two events are happening. And it so happens, when I look at the record of all the timestamp associated with the events, I see that the time stamp associated with a is less than the timestamp associated with b. So I want to make sure that you understand the premise of the problem here. What I am saying is that the timestamp that is associated with the event a happens to be less than timestamp associated with the event b. That's what I am observing, by looking at sort of a log record of all the events that took place in the system and now my question to you is. If C of a is less than C of b, does that mean that a happened before b or does it mean b happened before a or does it mean a happened before b with the condition that it's either the case that a and b are events in the same process or a is the act of sending a message and b is the act of receiving the corresponding message. So you have to pick the right choice among these three choices.
</li> 
</ul>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/16.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>This conditional statement is the right choice and the reason is because of the fact that the time stamps themselves don't give the whole story because all that we have are partial events happening on every node in the system. And we'll elaborate that a little bit more when we describe Lamport's logical clock in its entirety.
</li> 
</ul>


<h2>4. Logical Clock Conditions</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/17.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So now we're ready to describe the conditions for the logical clock proposed by Lamport. First of all, the first condition is that if I have two events, a and b, in the same process, the first condition says that the clock value associated, or the timestamp that it associated with event a has to be less than the timestamp associated with event b. In other words, we have this counter or a logical clock on every node of the distributed system that is monotonically increasing as events happen on that process. The second condition is that when we have a receipt of a message, we want to make sure that the receipt of the message has a timestamp that is greater for sure than the sending timestamp. So in other words if a and d are the act of sending a message from process Pi and d is the act of receiving the same message on process Pj, then what we are saying is the timestamp associated with the event a has to be less than the timestamp that is associated with the event d. In other words, we want to choose the timestamp associated with d as the max of the timestamp that I see in the incoming message, incremented by some value. Whatever the local counter is saying. These are the two things that I'm going to look at and decide the max of that as the timestamp to associate with event d.</li> 
  <li>If the events are concurrent, in this case, if I look at this picture here, a is the act of sending the message, d is the act of receiving the same message. b is an independent event that's happening on process Pi. It has nothing to do with this event d that is happening on process Pj. And these are concurrent events. So the concurrent events, we've already seen this when we talked about the happened before relationship. In the case of concurrent events, the timestamps are arbitrary. Just by looking at the timestamp, I cannot say that b happened before d because if I see the timestamp associated with b here, it happens to be four. Over here, we picked the timestamp for d by saying that it has to be at least greater than the incoming timestamp. So we gave it a timestamp of 3. And so if I look at these two events, b and d, d has a timestamp that is smaller than b, but that does not mean that d happened before b because these two are concurrent events, and therefore there's no way to know which event happened before the other. So, in other words, just because we find that there is an event x, which has a timestamp that is smaller than a timestamp associated with another event y, doesn't mean that x happened before y. So, while this condition is an important condition, the condition that if event a happened before b, as we show in this picture, we have to ensure that the timestamp associated with event a is less than the timestamp associated with event b. But the converse is not true. In other words, <b>if I have two events, the timestamp associated with event x is less than the timestamp associated with event y, that does not mean that x happened before y</b>. This is very important. </li> 
  <li>What that means is that Lamport's logical clock gives us a partial order of events happening in the entire distributed system. So if I take any process, I know all the events, the ordering of all the events that happened on this process, both the events that happened sequentially in this process itself as well as events that happened in which this process happened to communicate with other processes. In this case, it sends a message over here. Similarly, by looking at the record of all the events accumulated on process Pj, I can know the order in which the events happened in this process, in which process Pj had a part to play, meaning all the local events as well as communication events that Pj participated in when it communicated with the other nodes in the entire distributed system. This is what Lamport's logical clock gives you. It is a partial order of all the events that happened in the distributed system.
</li> 
</ul>

<h2>5. Need For a Total Order</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/18.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>Is partial order good enough for constructing deterministic distributed algorithms? It turns out it may be sufficient for many situations. The airline reservation example I started with would work fine with a partial order of events dictated by Lamport's clock. But there are situations where there maybe a need for a total order of events in the distributed system. Let's look at an example.</li> 
  <li>Here is an example to illustrate the need for total order. I'm going to use my own personal life example to illustrate the need for a total order. I have one car and my family consists of my wife, my son and my daughter, and we share this single car. And what we want to do is make sure that we can make local decisions on who gets dibs on using the car at any point in time. And we're going to use Lamport's clock for this. So what we do is, whenever we want to get the car for our personal use, we're going to text everyone with a timestamp. I'm going to associate a timestamp, if I'm requesting the car, I'm going to text everyone, and associate a timestamp with that request. And it is a logical timestamp and similarly my wife would do the same thing, son and daughter all of us do the same thing. And how do we pick the winner, well, locally, we can look at the timestamp of requests that have come in from others and my own request. And whoever has the earliest timestamp wins. Pretty simple, right? So pretty simple, everybody is going to make a local decision, looking at the timestamps of requests that have come in from others and say well, you know right now, it's my son's turn to use the car, or my daughter's turn to use the car and so on. But what if the timestamp, because these are locally generated by each one of us, happens to be the same. So, for instance, let's say my son makes a request sends a text message with a timestamp ten to all of us. So this is the blue arrow that's going everywhere, so that's indicating to all three of us that he wants the car, and timestamp ten is when he generated the request. So happens, my wife also makes a request for using the car, exactly with the same timestamp ten. And that's the purple arrow that you see. So, now we have a problem. And the problem is, all of us are looking at these text messages and trying to make a decision, who's got the dibs on using the car? How will my son and my wife know, given that both the timestamp is the same, which one is the winner for using this car? Now, what we do is, we're going to break the tie, and I'm going to stipulate that age wins. And therefore, in this case, if the time stamp happens to be exactly the same, then my wife, by seniority, is the winner. She gets the car. So, that's how we break the tie.</li> 
   <li>You can see, through this example, that there is a need for total order in decision making when you have a distributed system. And you want to make local decisions without bothering anyone, based on information that you have, but you have to make that local decision unambiguously because you cannot have both my son and wife thinking that they have the car at the same time. That'll be a problem. So, whenever there is a tie, we have to break that, and that's the need for the total order.</li> 
</ul>

<h2>6. Lamport's Total Order</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/19.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>So having seen the need for total order and ambiguous decision making in the distributed system, let's now introduce Lamport's Total Order formulation. This is the notation (=>) we're going to use for Lamport's Total Order. And so what we are saying here is that if there are two events a and b in the distributed system, a happens to be an event in this case on Pi And b happens to be an event, on process Pj. If we want to assert that, a totally is ordered ahead of b, that is a precedes b in the total order. This is true only under the condition that either 1) the timestamp associated with a is less than b, or 2) the timestamp happens to be the same and there is some arbitrary other functions that helps us to unambiguously decide which event precedes the other. For instance, I might say that the process ID that I associated with Pi and Pj, that maybe the one that I use to break the tie. In my family car example, I told you that the seniority of the family member makes the decision in terms of how we break the tie. So in this case if process Pi has a process ID 100 and process Pj has a process ID 200, then we could say that the arbitrary decision making in the case of a tie is that whichever process has a lower process ID, that's going to be the winner. So we might decide that in this case, if timestamp happens to be the same, then since Pi is less than Pj, I'm going to say that a precedes b. So it's an arbitrary, well-known condition for breaking a tie. So every process in the entire distributive system is going to use the same well-known, arbitrary condition in order to break to tie. </li> 
  <li>That also brings up another interesting point, and that is, there is no single total order. The single total order comes from the choice of the well-known arbitrary condition. I gave you the example of my family car, and we broke the tie in the family car situation by saying that seniority is the winner. Tomorrow as a family, we could decide that the youngest person is going to win. In that case, my daughter will have dibs over the car over everybody else in the case of a tie. The other important point to understand is that all of this idea of associating logical timestamps with events and then deriving a total order from the logical timestamp using this kind of a method of saying we are going just to believe the timestamp associated with the respective events. We are going to believe that timestamps are associated with the events and use those timestamps as a way of ordering them to develop total order. And if it happens to be a tie, everybody uses a well-known arbitrary condition to break the tie and that's how we derive a total order. Once we have derived the total order, the timestamps are meaningless after that. We don't care about them anymore. The whole idea of having these logical timestamps creating a partial order. And from the partial order deriving a total order, using this formulation (in the screenshot) for total ordering, so that we can get a particular total order. Once we get the total order, timestamps are meaningless.
</li> 
</ul>


<h2>7. Total Order</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/20.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>So, in this question, I'm showing you three processes, P1, P2 and P3. And these are the events that you can see happening in these three processes. What I want you to do is to derive a total order and in deriving the total order, we're going to use Process ID to break the tie. And smaller the Process ID, the higher the priority. In other words, P1 dominates P2 dominates P3. So, using that, go ahead and derive a total order for this set of events that I'm showing you happening in the distributed system. </li> 
</ul>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/21.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So first we can write down the partial orders that we see in the distributed system. You can see that 'a' happens before 'b', 'b' happens before 'c', 'c' happens before 'e' and these are purely coming from the chain of communication and local events that are happening over here. When we come over here, we also observe that 'f' happens before 'e' that's because of sequentiality Of this process. And similarly, we can see that in process P1, a happens before d. So these are the partial orders and we already have an ordering for the events that follow this chain because the logical timestamp are assigned in this fashion. And in order to derive a total order we said we basically will believe the event timestamps. To order them totally. So we can order these events totally. a1, b2, c3, e5. We can order them totally.</li> 
  <li>Now let's look at the concurrent events that are happening. The concurrent events that are happening is d over here is concurrent with all the other events in the other processes. D is concurrent with b and c. It is concurrent with f and e. And similarly if you look at this event f, it's concurrent with all the events that are happening in process P2 and P1, right. So this is what you as the concurrent events. So now what we have to do is, given that these concurrent events, we have to somehow Fit them into a total order. As I said before no problem in fitting these guys into a total order because they already have timestamps that distinguish them from one another. The timestamp associated with a is 1, b is 2, c is 3, e is 5. No problem with that. So the real problem comes with f and d. Now the sequentiality of this process is what made the timestamp associated with e to be 5 because the message that came over here had a timestamp of 3 but we associated a timestamp of 5 with e. Because the local event preceding e have a timestamp of 4, so we pick 5 as the timestamp to associate with e, so that the sequentiality of this process p3 is respected. So, we've got this, and now we've got f and d, and so f obviously is going to sneak in Before e, that, that's no problem, that is coming from the sequentiality, but where will we put d, do we should be put d after e? Or should we put d before e in the total order? This is where the breaking the tie using process id comes into play, Because these two guys are concurrent events in the system, we are breaking the tie using the process ID. P1 happens to be less than P3 in process ID space and therefore we are going to say in the total order, dis going to be ahead, totally ordered, before e. So the final ordering that we end up with, the total order that we end up with, is a0, b, c, F, and then d. And then e, so that's the total order that we come up with. Respecting the logical timestamp associated with the events, and breaking the tie using the process ID
 </li> 
</ul>

<h2>8. Distributed ME Lock Algorithm</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/22_1.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>Now let's put Lamport's clock to work for implementing a distributed mutual exclusion lock algorithm, and it is going to be very similar to the car-sharing example that I showed you before. And also you will notice that we've talked about locks in a shared memory multiprocessor where we have shared memory to implement the lock. But now in a distributed system we don't have shared memory. And we have to implement a mutual exclusion lock using Lamport's Logical Clock. So, essentially what is going to happen is that any process that needs to acquire this lock is going to send the message to all the processes. And of course the intent to get a lock may emanate simultaneously from several processes. That's perfectly feasible. The algorithm is as follows.</li> 
  <li>Every process has a queue data structure. And those (Q1,Q2...Qn) are the queues that are associated with each process (P1,P2...Pn). Every process has its own private queue. And the private queue is ordered by the "Happened Before" relationship that we have discussed so far. So requests for a lock are going to be timestamped and the protocol is as follows.
    <ul>
      <li>To request a lock, a process is going to send a message "I want this lock and my timestamp is such" to all the other Processes. So it's going to associate the local timestamp that it has from its counter, which is its logical timestamp. It's going to associate that timestamp as the request time for the lock and send the message to all its peers.</li>
      <li>All the peers, what are they going to do? Well, two things:
         <ul>
            <li>One is they're going to stick that request into the local queue. When a request comes from process Pn, P1 puts it into its queue in an appropriate place of the queue, because it is ordered by the timestamp. The smallest timestamp being the highest priority request pending. So it puts it in its queue. </li>
            <li>The second thing it does is every process, when it gets a request, puts it in its queue and then acknowledges the request to its peers. </li> 
         </ul>So let's look at the process P2 here and P2 generated its request at timestamp 10. What it did was first to put its request in its local queue, and then it sends a message to its peers. And these guys(peers), when they get the request, they look at their own local queue and say "Well, you know there is a request pending in my queue, which has a timestamp of 5, and this request that I just got from P2 has a timestamp of 10, so I'm going to order that behind the previous request. I put it over here. And once I do that I'm going to acknowledge this request by sending a message back to P2." And similarly, this guy sends an acknowledgment back to P2.  </li> 
     </ul>
   </li>  
   <li>So that's how the protocol works. Every request is sent to all the other processors and every process when it receives a request, it puts it ordered by Lamport's clock in its own local queue. And then acknowledges the request with an ACK message.</li> 
   <li>What happens when there is a tie? Well, when we have a tie, we break the tie by giving priority to the process that has a lower process ID so that's how this algorithm works, so that every process can unambiguously make a decision as to where to place an incoming request in the queue. So an example of the state of the queue is as shown. </li> 
</ul>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/22_2.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
   <li>The thing that should jump out at you immediately is that the state of the queue is not the same in all the processes. For instance, Process 1's queue contains its request that it generated at time 2, but I don't see it yet in the other queues. Is this possible that the queue can be inconsistent with one another? Of course, it is possible. The reason is that when a process generates a request, puts it in its queue and then sends a message out. This message is going to take some time to reach the other nodes in the distributed system. So, it sent the message and after it sent the message, it got requests from other processes and it has put it in its queue. And it is possible that this message, all the messages may not take the same amount of time to traverse a network. We have no idea what's going on in the network and therefore it so happens that P1's message is still in transit. Whereas the request messages from P2 and Pn have already made it everywhere, and it is in the queues of all the Processes, P1's message, unfortunately, is taking a slow route through the network and is still in transit. And in fact, P1 has subsequently received P2's and Pn's messages and put them into its local queue. It is just that P1's message hasn't reached its peers yet and that's how you get this situation.</li> 
</ul>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/22.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
   <li>So the whole purpose of this exercise is to unambiguously get the mutual exclusion lock for some process competing for it simultaneously. Now how does a process know that it has the lock? So I have to make the decision that I have the lock. How do I make that decision? Two things have to be true to think that I have gotten the lock. 
     <ul>
      <li>The first thing is that my request has to be at the top of the queue. So now you see the messages that I talked about, that is P1's message to P2 and Pn not having reached the destination. Eventually, they reach their destination. And they have acknowledged it. As a result of that the queues are consistent now. P1's request is at the top and it also has received acknowledgments from everybody else. This is the first condition the way you can decide that you have the lock unambiguously in the entire distributed system.</li>
      <li>The second thing is I've received acknowledgments from all the other nodes in the system. In this case, all the other nodes were not requesting this lock so they've sent me acknowledgments. And I've received all the acknowledgments and there is no other request that is ahead of me. I've also received lock requests from P2 and Pn and they are later than mine and that's how they've been ordered in the queue. </li>
      <li>So the two conditions I'm going to look for to make a decision locally that I have the lock is my request is at the top of the queue and I've either received acknowledgments from all the other nodes in the system, if nobody else is competing for the lock at the same time, or all the requests that I've gotten so far are later than my own lock request. </li>
    </ul></li>
   <li>Let's say that I haven't received the acknowledgment for my request from Q2, and Qn. Can I go ahead and assume I have the lock? Yes, I can. Why? Because even though these guys have not sent me the acknowledgment yet, it's slowly coming. But I've received lock requests from them with timestamps 5 and 10, respectively. Therefore I can make an unambiguous decision that my lock request precedes all the other lock requests at this point of time. And I can go ahead and get the lock. I'm sure you've figured it out already but since we are following Lamport's Clock in implementing this mutual execution lock algorithm, the ACK message for a particular lock request is going to have a later timestamp than the timestamp associated with the request itself. So you can see that Lamport's Logical Clock, with the addition of a way of deriving a total order from the partial orders given by the Lamport's clock, allows us to unambiguously make a decision locally based on the state of the local queue as to whether I have the lock or not. </li> 
</ul>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/23.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>     
<li>Now let's talk about how I go about releasing the lock. So if I have the lock. I have used it for a while and now I am ready to say,"well I am done with a lock, I can release it." What do I do? Well, I am going to send an unlock message to all the other guys. The first thing that I do, of course, is to get rid of the entry that I have in my queue because I am done with the lock. I can remove it from my queue. Once I remove it from my queue, I am going to send an unlock message to everybody else. So the state of the queue indicates that the unlocked message hasn't reached yet. It is in transit. It is going to eventually reach these guys. And when the peers receive the unlocked message, they're going to basically remove the entry, the corresponding entry, from the respective queues. So P1's turn with using the lock is complete now. It has done its lock and has done its unlock and now other Processes in the system, if they're competing for the same lock, can use the same decision making process to figure out whether they are the winners for getting the lock next, and using it and entering the respective critical sections. </li> 
</ul>


<br>
<br>
<p><b>-------------------------------------------------------------------------------</b></p>
<p><b>OH Note: Is there a chance that Pn thinks it has MUTEX Lock?</b></p>
<br>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/note1.JPG?raw=true" alt="drawing" width="500"/>
</p>
<p> The answer is NO. There are two conditions. We should not just look at the Qn alone. Every process P1,P2,Pn is sending the messages and the messages are in transit. Eventually, the state of Qn will be (P1,2) -> (Pn,5) -> (P2,10). In the previous example in the lecture and in this screenshot, for P1, the (P1,2) message is on the way to P2 and Pn, eventually they will reach. Remember the process will not make decision until it hears back from the acknowledgment (and P1 message is in transit). When (Pn,5) arrived at P1, what will be the acknowledgment P1 would send? The recept/acknowledgment will have a timestamp great than 5 based on the Lamport's clock. So there will be two messages going out from P1 to Pn: (p1,2) and (Pn,5+some time). Those messages are going out in order too. So the first message arrived from P1 to Pn should be (p1,2). There is no way that Pn can think it has MUTEX Lock because P1 message is still in transit and it hasn't got the ACK from P1, which are sent in order. Pn cannot make a decision until it sees that. </p>


<h2>9. Distributed ME Lock Algorithm (cont)</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/24.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So, we can informally talk about the correctness of the distributed mutual exclusion lock algorithm. The correctness is based partially on some assumptions and partially on the construction. The construction is that the Q's totally ordered by Lamport's logical clocks and the PID to break the ties. But that's part of the construction of the algorithm. But it is also based on some assumptions that we make and the assumption is that messages between any two processes arrive in order. So messages don't crisscross each other but if I send a message and I send another message, the first message is going to reach the destination first, second message is going to reach the destination second. And that's what is meant by saying that messages arrive in order at every node in the distributed system. And the second assumption is that there is no loss of messages. So every message that is sent is definitely received in order. So these are two fundamental assumptions that are responsible for this algorithm being correct. Now that you have seen Lamport's mutual exclusion lock algorithm, time for another quiz.
</li> 
</ul>
<br>
<p><b>-------------------------------------------------------------------------------</b></p>
<p><b>Extra Note: May not related to this course. From Piazza.Basically the question is how do we handle situations where the message never arrives (I.E. packet loss, system crashes before sending, etc)?</b></p>
<p align="left">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/add1.JPG?raw=true" alt="drawing" width="500"/>
</p>
<p>https://raft.github.io</p>
<p>https://lamport.azurewebsites.net/pubs/lamport-paxos.pdf</p>
<p>https://lamport.azurewebsites.net/pubs/paxos-simple.pdf</p>
<p>https://levelup.gitconnected.com/practical-understanding-of-flp-impossibility-for-distributed-consensus-8886e73cdfe5</p>

<h2>10. Messages</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/25.JPG?raw=true" alt="drawing" width="500"/>
</p>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/26.JPG?raw=true" alt="drawing" width="500"/>
</p>


<h2>11. Message Complexity</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/27.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So let's look at the messaging complexity of the mutual exclusion lock algorithm. The lock primitive, when a process makes a lock request, it sends N minus 1 request messages. Because there are N nodes in the distributed system, there are N minus 1 peers, and so every node has to send a request message to all its peers, so N minus 1 messages are the request messages sent out. And in response to these request messages, every peer is going to acknowledge a request message. So, they're going to be N minus 1 messages traversing the network, which are the acknowledgment message for this lock request. And then, the process is happy using the lock for the critical section. And it gets to the unlock primitive, and the unlock primitive, once again, we're going to send N minus 1 unlock messages, through the unlock primitive and while sending a unlock message to every one of the peers in the distributive system, so N minus 1 messages are sent over the network. The interesting thing that you notice is that there's no acknowledgment for the unlock message because of the assumption that we make that messages are never lost. And therefore, when I send an unlock message, I know that everybody will get it, everybody will remove my request from the respective queues and go on with life. And therefore, there is no acknowledgment for that. And so if you count the number of messages that are involved and a lock plus unlock, we have 3(N-1). That's the total number of messages that are incurred, that is a message in complexity, of the distributed mutual exclusion lock algorithm.</li> 
  <li>That begs the question, can we do better? And the answer is yes, and the reason is going back to the condition that I said that is used in making a decision as to whether I want the lock or not. If you recall, I told you that the condition is, my request has to be at the top of the queue, and second, I should have either gotten acknowledgements for that request from everybody else, or I've received a lock request from my peers that have a timestamp that is later than my own lock request. If the lock request that I received from my peers have timestamps that are later than mine, I know that they are going to wait for me to be served before they're going to use the lock. And therefore, if I am a receiving node for a lock request, when I see a lock request, normally I would go ahead and do an acknoweldgement. But, when I get a lock request and I see, hey, this guy's lock request is going to be after mine, so I don't have to send an acknowledgement yet. What I can do is to wait til I'm actually going to unlock. My unlock itself can serve as the acknowledgement for that particular node that has made a lock request that is later in time than my own. So in other words, we can defer the acknowledgements if my lock request precedes my peers lock request. So we're combining the acknowledgement for a lock request with the unlock. So if I do that, then I can bring the message complexity down to 2(N-1). So what we're doing is to gather the acknowledgement messages if in fact our own lock request is ahead of an incoming request that I see from up here. That's how we can reduce the messaging complexity of this algorithm to be 2(N-1). By the way, the distributed mutual exclusion lock problem have been a fertile ground for researchers to think about new algorithms that can shave the messaging complexity even further from this 2(N-1), and I invite you to look at the literature to see other works that have been done to reduce the message complexity to even smaller numbers than 2(N-1).
</li> 
</ul>

<h2>12. Real World Scenario</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/28.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So far, we've been dealing with this funny virtual time or logical time. But there are many real-world scenarios where this logical time may not be good enough. And in such situations, this logical clock may not be sufficient.</li> 
   <li>I'll give you an example. Let's say that I owe you some money. And I tell you by calling you on the phone, that I'm going to credit my account in my local branch at 5 p.m. I'm telling you on the phone that I'm going to credit my account at 5 p.m., and so any time after 5 p.m., you can withdraw money from my bank, and we'll be square. Now you're a nice guy, so you want to give me some leeway. So you tell your branch that, you know, at 8:00 PM, go ahead and debit from Kishore's account the money that he owes me. So your branch is going to basically do a debit call to the central bank server asking for the money that is owed by Kishore to be transferred to your account, so that's what is going to happen. And so you schedule that at 8 p.m. You've given me enough time to make sure that you have indicated to your bank, you have enough money, so that my debit transaction can go through. And you would think it should go through, right?</li> 
   <li>But it turns out that your branch's local time is far ahead of real time. Well, it thought it was 8 p.m., it was not quite 8 p.m. yet. Because it's way ahead of real time. And so I am exactly at 5 p.m., keeping my word, exactly at 5 p.m., my branch happens to be good with the time. It's in sync with the real time. And so at 5 p.m., I've done the credit of the amount that I owe you to my central bank server. But unfortunately, the central bank server, in real time, it got your message much earlier than the time at which I sent my message. It's not looking at any logical time. It is looking at real time, saying, well, there's a debit transaction coming in. Is there money in the bank for paying those debit transactions? No, it isn't. So your request is declined. And this is coming about because of the fact that in real world scenarios, logical clocks are not good enough. And in particular, what caused this problem is the fact that your notion, your branch's notion, of real time is completely at odds with real time. And the reason that can happen is because the computer at your local bank may have a clock that is drifting with respect to real time. So is drifting meaning that it is not keeping up with the real time. It's either going faster than real time or it is going slower than the real time. It so happens that my branch's time is is perfectly in sync with the real time, but that doesn't help me.</li> 
   <li>This is a real world scenario that you have to worry about. And such anomalies occur due to two things. One is individual clock drifts. Because if you think about a clock, clock is a piece of circuitry, and you expect that for every second of real time, your clock is also going to click a tick by the same one second. But if it doesn't, your idea of real time is going to slowly drift. That is called individual clock drift. And also, there is a possibility that there could be a relative drift between the clocks that are there in different processors. This clock can be ticking at a particular rate, and this clock would be ticking at a different rate from my clock, and that can cause a second source of discrepancy. And these anomalies are nasty things that we have to avoid in order to make sure that in real world we can have some guarantees about what goes on.
</li> 
</ul>

<h2>13. Lamport's Physical Clock</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/29.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So that brings us to Lamport's Physical Clock, and the notation we're going to use for that is this funny symbol ( |-> ) here. So, this is saying that in physical time, in real time, event a in the distributed system happened before b.</li> 
   <li>So if I want to make sure that an event a in the distributed system anywhere happened in absolute, real time before b, so Pi, there's a process Pj. Event a is happening on Pi and event b is happening on Pj. And what we want to make sure is that the timestamp associated with a has to be less than the timestamp associate with b. If I want to guarantee that a, in real time, happened before b, so a in real time happened before b, that is how you have to read this notation, that, in real time, the event a happened before b. And in order to satisfy that, the condition is, the timestamp associated with a has to be less than the timestamp associated with b. So to guarantee this, and we are talking about real time here, so real timestamp associated with a and b.</li> 
   <li>In order to ensure that the real time associated with these events give you this guarantee, you have to have certain conditions associated with the clocks that are on the machines Pi and Pj.</li> 
   <li>The first condition, which I'll call PC1, I'll refer to that as PC1 later on, the condition is a bound on individual clock drift. So PC1 is a condition which gives a bound on individual clock drift. Informally, what this condition is going to tell you, is that the clocks don't drift that much. So let's talk about this. If, what is the time that is read on process P1 at time t? If t is the real time, at real time t, I look at the clock on my machine and that is Ci of t, what should it read? Well, it should read t. Now, if it doesn't, that's when we are saying it is drifting with respect to t. And so what this equation is saying is dc_i over dt is the clock drift. The absolute value of that drift is a very, very small. So in other words, what we are saying is, all the clocks in the distributed system, whether we are talking about Ci on Pi or Cj on Pi all these clocks are running approximately correctly. So that the clock individual drift, is very very small. So k is the individual clock drift, and we are hoping that it is very, very small. And you can see that if Ci of t is equal to t. Then dCi of dt should be equal to 1 and therefore this would be a 0. So, the left hand side of this will be a zero and so that's why we're saying that k has to be a very small number.</li> 
   <li>The second condition is that the mutual drift between the clocks on different nodes of the distributed system should be very small. Should, there should be a bound on mutual clock drift, so that is captured in this condition saying that. For all ij, any pair of nodes in the entire distributed system, the difference between the time that I read on my clock and the time that I read on somebody else's clock is very, very small. because this is the mutual clock drift. As I said earlier, at real time t, my clock should also be reading t. This guy also should be reading t. If it doesn't, that's when you have a drift. What we're seeing is the mutual clock drift between any two nodes in the entire distributed system is bound by a small quantity, epsilon. So k and epsilon are the two important parameters In the physical clock condition.</li> 
   <li>Intuitively, we're going to argue that these values, the absolute values of these individual clock drift and mutual clock drift, has to be negligible compared to the inter-process communication time.
</li> 
</ul>

<h2>14. IPC Time and Clock Dirft</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/30.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So what we're going to look at now is the relationship between the inter process communication time and both the individual and the mutual clock drift that I described to you. Let μ be the lower bound on the inter process communication time. So let's now derive the conditions under which we can assert that if we have an event a on Pi. And in real time, it's supposed to precede an event b on Pj. What are the conditions that should hold in terms of μ, k( the individual clock drift time), and ϵ (epsilon, that is the mutual clock drift time)?</li> 
   <li>This first condition is pretty straightforward. It is coming from the fact that ideally, the clocks are perfectly synchronized. You know that at real time t, Ci of t and Cj of t should be exactly the same, right? You expect that if it is, if the clocks are perfectly synchronized and it is keeping with real time, Ci of t is equal to Cj of t, equal to t, where t is the real time (Ci(t) == Cj(t) == t). But they could be in the original clock drift and user clock drift, which makes them different from each other. And all that you're seeing through this first one is that this is the act of sending a message (a) and this is the act of receiving the message (b). The timestamp or the real time that I'm going to give to this, by reading my clock, it is better be higher than the time at which the message is actually sent. And in order to guarantee that we have to look at what would be the time that it takes for this message to go from here (a) to here (b). That's coming from this μ, the lower bound on IPC. So, if the message is sent at Ci(t) with respect to Pi. Then, the time on Pi when this message is received over here in Pi is going to be Ci(t+μ). This is a local reading of the clock when the message would have arrived at Pj. So this is the time elapsed between sending the message and when my peers should have received the message. So, what we are saying is in order for making sure that Pj will have a timestamp that is at least greater than Ci, you want to make sure that the time reading that I have on my local clock, t plus μ, should be greater than the time reading at the time that I sent the message. So the time that I sent the message from Pi my pierce time was Cj of t and and all that we are saying is in order to make sure that there is no anomaly the first condition has to hold that says that. The disparity between the two clocks is within this interprocess communication time. That's all this is saying, that the disparity of the mutual drift is within this interprocess communication time.</li> 
   <li>The second equation is basically a difference equation formulation of the formula that I gave you, which I called PC1. And this is basically saying that if k is zero when I read the clock at t+μ and see the difference between the clock reading now and the clock reading when I read it at time T it should exactly be μ (Ci(t+μ) - Ci(t) == μ). But because of the fact that I may have individual clock drift, it may not be exactly μ, but it may be something different from μ. Either more or less, but very small difference (Ci(t+μ) - Ci(t) != μ). And so all that we are saying is that the amount of individual clock drift should be negligible compared to the interprocess communication time.</li> 
   <li>So the first thing is saying the interprocess communication time is much bigger than any clock drift that exists between two different clocks. And the second equation is saying that the individual clock drift is very small compared to μ. And if we put all these things together, you can derive the expression for inter-process communication time. What it should be relative to mutual drift and individual clock drift. If this inequality is satisfied, you can avoid anomalies in your distributive system. So, informally, would you expect this k is very small, which means the denominator is very close to one. So, all that we're saying here is that the mutual clock drift, which is represented by epsilon ϵ, is very small compared to the interprocess communication time (μ >= ϵ), which is what is captured by this apparent condition that I laid out here.
</li> 
</ul>

<h2>15. Real World Example (cont)</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/31.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Let's return to our example earlier of me owing you money, and let's say it so happens that the mutual clock drift is 5 between you and me (ϵ = 5). So my clock, when it reads 3:00 PM, your clock reads 8:00 PM, so that is the mutual clock drift that we have. And let's say that the interprocess communication time, the lower bound on that is 2 (μ = 2). So what happens is that, as I told you earlier, I am telling you any time after 5:00 PM. You can get the money from me from my bank. And so you instructed your bank, to debit at 8:00 PM. But unfortunately, your 8:00 PM, is my 3:00 PM, because, our mutual clock drift is 5. So when your request went out, it took 2 units of time to get to the server went out and the Central Bank got your message asking for a debit transaction, but the credit is not there yet, because I'm waiting till 5:00 PM to actually send my credit advice to the bank. And therefore your request which came in at 4:00 PM, because you sent it relative to me, you are five hours ahead, and in terms of real time, you're actually six hours ahead, and so the message is received at 4:00 PM, and your request is declined. And this is coming about, because of the fact that your interprocess communication time μ is less than the mutual clock drift ϵ that we're seeing between these two clocks which is five.
</li> 
</ul>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/32.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li> On the other hand, if the mutual clock drift is less than the inter-process communication times, in this case, let's say that the clocks are more well-behaved. Epsilon is one  (ϵ = 1), meaning the mutual clock drift between you and me is just one. And so, exactly the same scenario. When it is 3:00 PM, in my branch, your branch is saying your time is 4:00 PM. Of course, you've given the advice to your branch to debit at 8:00 PM. So at my local time, 5:00 PM, I send a credit advice. Received by the bank, and at your time, 8:00 PM, which is not quite in sync with real time, and is also drifting with respect to my time, but the bound is less than the lower bound on the interprocess communication time. And therefore when you send your debit request at 8:00 PM, your local time, which actually in real time 6:00 PM, it's perfectly fine because when it is received in the Central Bank, the real time is 8:00 PM. And it is received later than the current request, and so your request is honored, you're happy and you can go home.</li> 
   <li>So the important takeaway is that in constructing distributed applications which depend on real time, it is important to make sure that are bounds on individual clock drift as well as mutual clock drift. So individual clock drift is what my clock is reading at any point of time and how far off is it from real time. So that is individual clock drift and you want that to be bound by some small value, which we call k. And the other important thing is that you want to make sure that the mutual clock drift ϵ is very very small too. So that there is no anomaly when the interactions like what we showed here and the whole thing hinges on the relationship between mutual clock drift ϵ, the individual clock drift k, and the interprocess communication time μ. Informally, as long as you make sure that the interprocess communication time is significantly higher than the clock drifts, whether it is mutual or individual clock drift, you can ensure that there are no anomalies in the system.
</li> 
</ul>

<h2>16. Lamport Clocks Conclusion</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/33.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Lamport's clock serves as the theoretical underpinning, for achieving deterministic execution in distributed systems, despite the fact that there are nondeterminism existing due to vagaries of the network and due to drifts in the clocks and so on. It's a nice feeling that we can come up with conditions that need to be satisfied in order to make sure that we can have deterministic executions and avoid anomalous behaviors using Lamport's clock, both logical clocks where it is sufficient as well as the physical clock conditions. In the next part of this lesson module. We will discuss techniques for making the operating system, communication software stack efficient for dealing with network communication.
</li> 
</ul>
<br>
<br>
<p><b>-------------------------------------------------------------------------------</b></p>
<p><b>OH Note: Can we construct deterministic distributed algorithms using Lamport's clock which is inherently non-deterministic? The answer is yes! It is a tool to use partial order, come out total order, breaking tie when you have to </b></p>
<br>
<p><b>OH Note: Lamport's ME algorithm hinges on 1) happened before relationship 2) messages going in-order between any two nodes 3) no loss of messages </b></p>
<br><br>


# L05c: Latency Limits
<h2>1. Latency Limits Introduction</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/34.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>Lamport's clock gave a fundamental ordering mechanism for events in a distributed system. This theoretical basis is all the more important in this day and age, when so many everyday services, email, social networks, e-commerce, and now even online education are becoming distributive. Incidentally, this is the same Lamport who gave us a way to order memory accesses in a shared memory multi-processors, through the sequential consistency memory model. In the next part of the lesson we turn our attention to more practical matters in distributed systems. Specifically, given that network communication is the key to performance for distributed services, the operating system has to strive hard to reduce the latency incurred in the system software for network services.</li> 
  <li>Lamport's clock serves as the theoretical underpinning for achieving deterministic execution in a distributed system, despite the nondeterminism that exists due to the vagaries of a network. In this lesson, we will discuss techniques for making the operating system softwares tag efficient for network communication. Both by looking at application interface to the kernel as well as inside the kernel in the protocol stack itself, but first a quiz.
</li> 
</ul>

<h2>2. Latency</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/35.JPG?raw=true" alt="drawing" width="500"/>
</p>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/36.JPG?raw=true" alt="drawing" width="500"/>
</p>


<h2>3. Latency vs Throughput</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/37.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So it's important to understand these two concepts of latency and throughput. Latency is the elapsed time for an event. If it takes me one minute to walk from my office to the classroom, that's the latency that I'm going to experience for that event of walking from my office to the classroom. That's the elapsed time.</li> 
  <li>Throughput is the number of events that can be executed per unit time.</li> 
  <li>Bandwidth is a measure of throughput.</li> 
  <li>So once again with this analogy of walking to the classroom from my office, if the hallway is wide enough to allow five, ten of us to walk in parallel side by side to the classroom, increases the throughput but it does nothing to the latency. The latency is going to be determined by how fast I can walk from my office to the classroom. So, the difference between latency and throughput is very important to understand. In other words, I can increase the bandwidth and that'll improve the throughput but it is not going to do anything to the latency itself. So in other words, higher bandwidth does not necessarily imply lower latency. You'll work hard to lower the latency.</li> 
  <li>RPC is a basis for client server based distributed systems. And performance of RPC is crucial, specifically in the context of this lesson. Latency refers to the time it takes for an application generated message to reach it's destination. So for instance, if you're doing an RPC call from a client to the server, then the RPC call entails sending the argument from the client to the server. And there is work to be done here (at client side), work to be done in sending the message, work to be done here (at server side) before the server can actually execute the server procedure. So it's the latency that we are concerned about. And what we will see is all the software components that comprise the latency for RPC based communication. And performance of RPC is very crucial in building client server systems. There are two components to the latency that is observed for message communication in a distributive system.The first component is the hardware overhead and the second component is the software overhead.<ul>
   <li>The hardware overhead is really dependent on how the network is interfaced to the computer. So, typically in any computer, what you have is a network controller that interfaces the network to the CPU. And typically, the network controller operates by moving the bits of the message from the system memory of the node into its private buffer, which is inside the network controller. And this part of it, moving the bits from the memory of the node into the internal buffer of the network controller is accomplished using what is called direct memory access, meaning the network controller is smart enough to move the bits directly using the bus that connects the memory to the network controller without the intervention of the CPU. And this is what is called direct memory access. And that's how the bits are moved from the memory of the system into the buffer of the network controller, and once it comes here, the network controller can then put the bits out on the wire, and this is where the bandwidth that you have connecting your node to the network comes into play. But there are also other types of network controllers where the CPU may actually be involved in the data movement, and in that case, the CPU does program I/O to move the bits from the memory into the buffer of the network controller, from which the network controller will then put it out on the network. But modern network controllers tend to be built using DMA technique, meaning that the network controller, once the CPU tells the network controller were in memory the messages to be sent on the wire, network controller does the rest in terms of moving the bits into its internal buffer, and then from the buffer putting it out onto the network.</li>
   <li>The software overhead is what the operating system tax on to the hardware overhead of moving the bits out onto the network. So the latency, if you think about the latency as a whole for doing a network transmission, there is the software overhead incurred in the layers of the operating system to make the message available in the memory of the processor, ready for transmission. Once it is ready for transmission, the hardware overhead kicks in, and the hardware, the network controller in particular, moves the bits from the memory into its buffer and then out on the wire.</li></ul>
   <li>The focus of course being an operating system designer's work, is to reduce the software overhead and take what the hardware gives you and think about how you can reduce the software overhead so that we can overall reduce the latency involved in transmission,which is a sum of the hardware overhead and software overhead.</li>
</li>  
</ul>


<h2>4. Components of RPC Latency</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/38.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Let's now discuss the components of the RPC latency. By now we are all familiar with the semantics of RPC, namely, in RPC, the client is making a remote procedure call to a server, and it has to send the arguments of the call to the server so that the server can execute the server procedure and return the results back to the client. So if you look at the components of the RPC latency, it starts with a client call. So the client call subsumes a number of things.</li> 
  <li>1. Step One - Client Call: Number one, it is setting up the arguments for the call. The client has to set up the arguments for the procedure call. And then it makes a call into the kernel. And once the kernel is being informed that it wants to make this call, the kernel validates the call, and then marshals the arguments into a network packet, and sets up the controller to actually do the network transmission. That entire set of activities that the client program and the kernel is involved in, in getting ready a network packet to send out, is subsumed in this one line, which I say is the client call.</li> 
  <li>2. Step Two - Controller Latency (hardware): The second part of the latency is the controller latency, and this is the part where the controller says, "well, there is a message to be sent out. I know where it is in memory. I have to first DMA that message into my buffer and then put the message out on the wire." That's the controller latency and so this part of it is in hardware, and as operating system designer we're going to take what the hardware gives us. Controller latency is what you have, that given by the hardware.</li> 
  <li>3. Step Three - Time on wire (distance): The third part of the latency is the time on the wire. Now this really depends, as one might imagine, on the distance between the client and the server. The limiting factor of course is speed of light. So, depending on the bandwidth that's available between the source and the destination, perhaps if you have to go through intermediate routers and so on. It is going to take a certain amount of time to go from the client to the server machine, and that we call as the time on the wire. So then, the message arrives over on the destination node, and it arrives in the form of an interrupt to the operating system.</li> 
  <li>4. Step Four - Interrupt handling (OS): So, the interrupt has to be handled by the operating system, and part of handling the interrupt is moving the bits that come in on the wire into the controller buffer. And from the controller buffer into the memory of the node. So all of that activity is subsumed in this item number four, which I call the interrupt handling. </li> 
  <li>5. Step Five -Server setup to execute call: So once the interrupt handling is complete, then we can set up the server procedure to execute the original call. Now, what is involved in that? Well, you have to locate the server procedure, and once you locate the server procedure, you have to dispatch the server procedure. And once you dispatch the server procedure, you have to unmarshal the network packet that comes in as the actual arguments for the call that the server procedure has to execute. So all of that setup is first done, and then the server procedure can actually execute the call.</li> 
   <li>So this is the five-step process. From the time the client says, I want to make a RPC call to the point of actually executing the call, these are the layers of software and, of course, hardware and time on the wire, by which time you're ready to execute the server procedure.So even though it looks like a simple procedure call from the clients point of view, there is all this latency to be incurred in executing a remote procedure call. So at the end of step five, the server is all set to execute the procedure. Let's see what happens then.</li> 
  <li>6. Step Six -Server execution plus reply: So step number six is server execution, meaning that it is actually executing the procedure. And of course, this is not under our control as operating system designer, because at this point, the amount of time that the server is going to execute this procedure depends really on the logic of the program that has been written as a client server program. And then, finally, once the server procedure has completed execution, then it says "okay, I am ready to send the reply back to the client." and that's where we pick up again, so, what happens is that at that point, you are receiving the results.</li> 
   <li>7. Step Two - Controller Latency (hardware): So, once again, just like when the client wanted to send the arguments, you have to convert the actual arguments into a network packet and send it out on the wire. Similarly,when the server is ready to reply, you have to take that reply, which is the results of the execution of this procedure, and make it into a network packet. And at this point, once it has been made into a network packet it has to be handed over to the controller, and the controller does exactly what we did on this side, which is to say the controller latency is going to be incurred. So that's why you see item number two appearing all over again in the return path.</li> 
   <li>8. Step Three - Time on wire (distance): Similar to sending the arguments over to the server on the wire, the results have to be sent on the wire back to the client. And so you see that item number three, which is the time on the wire, is reappearing on the return path as well. Come over to this side.</li> 
  <li>9. Step Four - Interrupt handling (OS):The incoming result message is going to result in an interrupt on the receiving node, the client node. And that is exactly similar to what happened on the server side item number four. So you see number item number four reappearing on the return path as well. So that is the interrupt handling part.</li> 
   <li>10. Step Seven - Client setup + receive results: once that interrupt is handled, the operating system on the client side said, "oh this was on behalf of this client, let me redispatch the client", set up the client so that the client can then receive the results, and restart execution where it left off.</li> 
   <li>So the only two new things that we added in the return path was item number six and seven. Two, three, and four was exactly the same as what we saw on the way over to the server, that is being repeated on the way back to the client.</li> 
  <li>So that's the seven-step latency involved in the RPC, not worrying about the actual execution time of the server core itself because that is not in the purview of the operating system, it is in the purview of the client server program that the app developer has done.
</li> 
</ul>

<h2>5. Sources of Overhead on RPC</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/39.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>Now that we understand the components of RPC latency, let's understand the sources of overhead that creeps in carrying out all the different functions, going from the client to the server and back to the client. So far as the client is concerned, this looks like an innocuous procedure call, right? So it just says, I want to call a procedure S.foo, and here are the arguments. Well unfortunately, this call is not a simple procedure call but it is a remote procedure call. And the sources of overhead that creeps in, in a remote procedure call, are marshaling, data copying, control transfer and protocol processing. So we'll look at each one of these things in more detail. Now how can we reduce the overhead in the kernel? What we want to do is to think what the hardware gives you in order to reduce the latency incurred for each of these components of the RPC latency.
</li> 
</ul>


<h2>6. Marshaling and Data Copying</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/39_1.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>First let's look at how we can reduce the overhead on marshaling the arguments and the data copying.</li> 
  <li>Just to jog your memory, marshaling refers to the fact that the semantics of the RPC call being made between the client and the server. It's something that the operating system doesn't have any clue about. So in other words, the arguments that are actually passed between the client and the server has semantic meaning only between the client and the server. The operating system has no knowledge of it. Therefore marshaling is the term that is used to say, "let's accumulate all of the arguments that is there in the call and make one contiguous network packet out of it, so that we can give it to the kernel and the kernel can send it out."</li> 
  <li>That's what is being described as marshaling, and the biggest source of overhead in marshaling is the data copying that's going to happen, and I'll explain that in a minute.</li> 
  <li>Potentially, in doing the marshaling, there could be three copies involved. Where are these three copies coming aboard?
     <ul>
      <li>First Copy: When a client is executing a procedure, all the arguments for the procedure call that it wants to do is live on the client's stack. And there is an entity, we'll introduce this terminology before, called the client stub. The role of the client stub is to take the arguments of the call which are living on the stack, and convert it into a contiguous sequence of bytes called an RPC message, so the RPC message has no semantic meaning. It's just a contiguous string of bytes, which you can pass to the kernel, and the kernel can then send it out on the wire, just like any other message. So that's the first thing that the stub does, and that's the first source of overhead. The client stub is making the first copy from the stack to create an RPC message. </li>
      <li>Second Copy: The client is a user program living in the user space outside the kernel. So this RPC message, which is being created by the stub, is pulled off the client's address space, which is living outside the kernel. So, this RPC message is in user space, and the kernel has to make a copy of the RPC message from the user space into its own buffer, the kernel buffer. And that's the second source of overhead. The second source of copy in doing the marshaling of the arguments. </li>
      <li>Third Copy: So now it is in the buffer of the operating system kernel, now the operating system can kick the network controller and say, " Hey, go ahead, take this buffer, send it out in the wire to the desired destination." And the network controller, at that point, is going to move the bits from the buffer, which is in the system memory, of the operating system, into its internal buffer using DMA. And this is the third copy that is happening. The copy done by the network controller, using DMA to move the bits of the RPC message, is copied from the user space into the internal buffer of the kernel. And now this movement is being orchestrated by the hardware to move it from the kernel buffer into the internal buffer of the network controller, so that it can then get out on the wire.</li>
     </ul>
     So those are the three copies involved in marshaling the arguments of the call, before it can be put out on the wire. The copying overhead is the biggest source of overhead for RPC latency.
   </li>
</ul>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/40.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>   
   <li>How do we reduce the number of copies? Well, it turns out that the third copy that you're looking at here, moving the bits from the system memory into the network controller, there's a hardware action. That is unavoidable, and therefore, we're going to live with it. Unless the network controller is completely redesigned, if the network controller is saying, "Well, I need to DMA the bits from the system memory into my buffer. Well, I have to DMA the bits from the system memory into my internal buffer, before I can put it all on the wire." Then this third copy is inevitable so we live with it. But we would like to see if we can try to reduce the number of copies involved here (client stub & kernel buffer).</li>
   <li>The first idea is, can we eliminate this copy done by the client stub? Why is that happening? It has to create a network message to send it out on the wire. As we said, that the semantics of this call is only known to the client and the server, and the client stub is taking the argument and making a network packet out of it. And it was doing it in userspace. And so what we're going to do is, we're going to marshal it directly into the kernel buffer. In other words, we've now moved this client-side stub from the user space down into the kernel. If you can move it into the kernel, a stub can directly marshal it into the kernel buffer from the stack. And so that intermediate copy that we had here creating an RPC message and copying it. Again, the kernel buffer is avoided if the stub can work directly on the stack and write it into the kernel memory. So what this means is that, at instantiation time, the client stub is installed inside the kernel. At bind time, when the client binds with the server at the bind time, what we going to do is, we are going to say  "here is the client stub, please put it inside the kernel so that, later on, you can use that in order to do the marshaling". So the synthesized procedure is installing the kernel for each client call for each client-server relationship. We synthesize the procedure, which is the client's job, install it in the kernel for use every time I make this call. This stub can be invoked to convert the argument that is living on the stack into a network message, and directly put it into the kernel buffer. So this obviously will eliminate from the two copies down to one copy, because the intermediate copy of converting the arguments into an RPC message is now eliminated.</li>
   <li>The problem with this idea is that we're saying, "let's dump some code into the kernel," and that may not be so palatable. So this is a solution that's possible if the RPC service that is being provided between the client and the server is the trusted service, and therefore, we can trust who is putting the stub into the kernel. In that case, the solution may be a reasonable one to adopt.
</li> 
</ul>

<h2>7. Marshaling and Data Copying (cont)</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/41.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>An alternative to dumping code into the kernel is to leave the stub in the users piece itself but have a structured mechanism for communication between the client stub and the kernel. And that structured mechanism is a shared descriptor.</li>
   <li>The shared descriptor is a way by which the stub can describe to the kernel that there is some stuff sitting in the userspace, and I am going to tell you how exactly you can extract this information from the user space and construct it into this buffer for transmission on the wire. Recall what I told you earlier, and that is the kernel has no idea of the semantics of this call and, therefore, it has no knowledge of the actual arguments. The data structures that are being used in the call. So we're going to use the shared descriptor as a vehicle for the stub to communicate to the kernel the data structures that need to be passed. So, for instance, let's say that the argument for the call has four parameters. Then this descriptor has four entries, and each entry says, "this is a starting point of a particular data item, and this is the length of the data item. This is the starting point of the second data item, and this is the length of the data item. Third data item, fourth data item." The kernel doesn't have to know the semantics of these data items. All it needs to know is the starting address for a particular data item and the length of the data item. That's all it needs to know. And this is the descriptor that allows the stub to inform the kernel about the arguments, how many arguments there are, and what each argument's size is. It doesn't have to tell the kernel, "oh, here is an integer, here's a floating point, here's an array". No, none of that. The stub says, "Here is the starting address for an argument, and here is the length of that argument." Because data structures are usually organized contiguously, so if you have an integer, it is occupying full contiguous bytes and memory. If you have a floating point number, it is occupying some number of contiguous bytes in memory, and therefore, what the stub is doing is, is creating the shared descriptor that is providing the information of the kernel in the layer of the arguments on the stack. Once the layer of the arguments and the stack is known to the kernel, the kernel can use these contiguous data items that are living on the stack, describe the shared descriptor, and create a contiguous packet in its internal buffer. That's a second way you can do to reduce the number of copies from two to one.</li>
   <li>So in both cases, what we have done is either the first approach of pushing the client stub into the kernel or the second approach of having a shared descriptor between the user stub which is living in user space and the kernel in order to describe the layout of the data structures that need to be assembled into a data packet by the kernel using the shared descriptor. Both of these allow us to reduce the number of copies from three down to two. Either the one copy that is happening going from this stack into the kernel buffer and this second copy, as I said, is unavailable if the network controller is requiring DMA to be done from the system memory into its internal buffer before the bits can be pushed out of the wire.</li>
   <li>So that's the first source of overhead. These are techniques that we have looked at - Two different techniques for reducing the copying overhead that is the dominant part of marshaling the arguments. And this happens on both sides. It happens when the client has to push the arguments to the server-side. And it happens again on the server-side when the server has to push the results back to the client. So the marshaling is happening on both ends, and for both ends, we can use this technique of using a shared descriptor or pushing the client's dub or the service dub into the kernel to reduce the number of copies from two down to one.
</li> 
</ul>

<h2>8. Control Transfer</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/42.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So the second source of overhead is the control transfer overhead. And that is the context switches that have to happen in order to effect an RPC call and return. And let's now look at the context switches that happen in doing the RPC.</li> 
  <li>First ctx: Let's say this is the client machine. On the client machine, the client is making a call. When the call is made, the kernel has to say, "oh, this client wants to make an RPC call, and we know that the semantics of RPC is that." The client is blocked until the results are returned. Therefore, the operating system on the client side will switch from the current client that is making the call to another process. Let's call it C1. So this is the first context switch that's going to happen on the client box.</li> 
  <li>Second ctx: The RPC call is sent out in the wire to reach the server machine. When it reaches the server machine, the server machine is executing some arbitrary process called S1. So when the call comes in, the kernel has to switch to the particular server process that is going to handle this incoming RPC call. So this is the second context switch. So, the server machine and the operating system on the server machine is currently executing some process S1, then it has to switch to S to answer the incoming RPC call. That is the second context switch.</li> 
  <li>Third ctx: Then the server procedure executes. And once the server procedure is completed execution, it's now going to send the results out. When it wants to send the results out, at that point, the work is done for the server. And so the server operating system has to switch from S to some other process S2, so that's again a context which that's going to happen because the server is done with whatever it has to do. So that's the third contact switch.</li> 
  <li>Fourth ctx: Then the RPC result comes over the wire to the client-side. When it comes back to the client-side, the kernel at that point is executing some process C2. And this particular result message is coming back, saying, "well, the original call sent out on behalf of this client C, the result has come back. Now it is time to reschedule this client so that this client can receive the results and continue with its execution." Remember that the semantic is like a procedural call, but it is a remote procedural call. The client is blocked for the result to come back. When the result comes back, the kernel can schedule the client to continue with its execution.</li>
   <li>So that's what is going on. Potentially, there are four contact switches that are going on.</li> 
   <li>Now let's look at these contact switches and figure out what contact switches are critical.
      <ul>
      <li> First ctx is not critical:  This contact switch (the first ctx) is essential to ensure that the client box is not underutilized, right? So once the client has made this call until the result comes back, the client box is underutilized. Therefore, the operating system says, "well, let me switch to some other process that can do some useful work on this node." So that is this contact switch. It is not critical from the point of view of the latency for RPC.</li>
      <li> Second ctx is critical:  Now when the message comes over here(The second ctx). This context switch is crucial, because at this point when the RPC call comes in, this guy, the server box is executing some of the process S1. So it has to switch to this server process S, which can actually execute the RPC call. So this is an essential part of the RPC latency.</li>
      <li> Third ctx is not critical:  Similarly, to this context which (the third ctx), that I talked about This contact switch is happening in order to make sure that the server's machine is not underutilized. When the server is done with the RPC call, it's going to send the results back, and therefore, we need the contact switch out of this server process to some of the process that we can utilize this server box. That's this contact switch. Again, similar to this context switch(The first ctx), this context switch is not in the critical path of RPC latency.</li>
      <li> Forth ctx is critical: The result comes back and when the result comes back the client box is executing some process C2. The kernel has to switch to this client, so that is can see the results, and continue with its execution. So this context switch, is again in the critical path of RPC latency. </li>   
      </ul>
</li> 
</ul>

<h2>9. Control Transfer (cont)</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/43_1.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li> (The result comes back and when the result comes back the client box is executing some process C2. The kernel has to switch to this client, so that is can see the results, an continue with it's execution. So this context switch, is again in the critical path of RPC latency.) (This sentence actually should be in 8.Control Transfer. added there)</li>
  <li> (Note, we will use first, second, third, fourth ctx from the four ctx flow in the below paragraph to point out which ctx the professor is talking about)</li>
   <li><b>Reduce ctx from four to two: </b>
    <ul>
      <li>So if you look at RPC call, the two contact switches that are in the critcal path of RPC latency. There's a context switch that happens here (second) and the context switch that happens here(last). So only two context switches are in the critical path of the latency, the context switch that happens on the server machine when the call comes in, and similarity, the context switch that happens on the client machine when the results come in.</li>
      <li>So this context switch that happens on the client machine to keep the client machine utilizied can be overlapped with the network communication to send the call out. So in other words, this context switch (first ctx) is not the critical path of RPC latency, therefore, do this context switch (first ctx) while the RPC call is in transmission on the wire. So it will be overlapping the context switch that happens on the client box after the call has been sent out with the communication time on the wire for the opposite call.</li>
      <li>Similarly, on the server side, once the server is completed execution and it is ready to send the results out, send it out of the wire and in parallel with sending it out of the wire, it can overlap the context switch (third ctx) that happens here in order to keep the server box busy doing useful stuff, S to S2 that can be overlapped with this network communication.</li>
      <li>So, only this contact switch (second ctx) and this contact switch (fourth ctx)  are in the critical path of latency. So, we can reduce the number of contact switches down to two. Originally we started with four, we can reduce it down to two by observing that the context which is that happened on the client box and the server box to keep them utilized can be overlapped with a network communication for sending the arguments over to the server, or sending the results over to the client.</li>      
      </ul>   
   </li>
</ul>    
<br><br>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/43.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>  
   <li>Of course we are greedy. Can we reduce it to one? Can we actually reduce the number of context switches Down to one. Let's think about this.</li>
   <li><b>Reduce ctx from two to one: </b>
    <ul>
      <li>So we said that when this RPC call was made (in the first step C was made), the operating system on the client side said, "well, this is a blocking semantic, and therefore, this guy is not going to do any useful work, so I'm going to block him and wait for the results to come in". So this context switch that the operating system did on the client side was essentially to keep this client box from being underutilized, but do we really need to do the switch? Well, it really depends.</li>
      <li>If the server procedure is going to execute for a long time, then, you know, this client box is going to be under-utilized for a long time. And in that case, it might be a good thing to context which in order to make sure that we are utilizing the resources that we have. But on the other hand, If, suppose, this RPC call, we know that this RPC call is going to come back very soon. And if it is on a local area network and the server procedure that is going to be executed is not going to take a long time. Then perhaps that RPC call will come back very quickly. </li>
      <li>If that is the case (RPC call will come back very quickly), we can get rid of this context switch (the first ctx) that we talked about here in order to keep the client box busy, we did this (first) context switch. Don't do that.We can spin instead of switching on the client side. And if you do that, then the client is waiting but is not being context switched out. It is just that the box is underutilized, so the only context switch that we incur is the context switch (second ctx) on the server because you never know when an RPC call is going to come in.So when an RPC call comes in, you obviously have to contact switch into the server context in order to execute the call. That's the necessary evil. We'll incur that.</li>
      <li>On the client side, what we're going to do is, we're going to spin instead of switching, so that even though the box is underutilized, you're not doing anything on the client side, just sending the call out and waiting. And in that case, we've gotten rid of the other context switch (fourth ctx) that you need to incur, because you never context switch it out and therefore this context switch (fourth ctx) which we said is inevitable, because it has to be done in order to receive the results for the client. Well, we can get rid of it if we never switched in the first place, and that's the trick here. To reduce the number of conflicts which is down to one, we can spin on the client side instead of switching so that we can be ready to receive. The results of the RPC call execution when the server is done.</li>    
       <li>Again, the intent here is that we wanted to reduce the latency that is incurred in the RPC call. And since these two contract switches were in the critical path of the latency, we would really like to see how we can elimintate at least one of them. And this context switch  is inevitable (second ctx in the four ctx flow/ the first ctx in the two ctx flow) and this context switch we can eliminate by spinning on the client side instead of switching in the first place (fourth ctx in the four ctx flow / the second ctx in the two ctx flow).</li>
    </ul>   
</li> 
</ul>

<h2>10. Protocol Processing & 11. Protocol Processing (cont)</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/44.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So we talked about marshaling, data copying, and content switches, and the fourth component that adds to the latency of the RPC transmission is protocol processing, and that's the next thing that we're going to look at.</li>
   <li>Now the question is what transport should we use for the RPC? And this is where we want to see how we can take advantage of what the hardware is giving us. If we are working in a local area network, the local area network is reliable, and therefore our focus should really be on reducing the latency and not worry so much about reliability. It is often the case that performance and reliability are at odds with each other. If you focus on reliability, then performance may take a back seat. So here, since the RPC performance is the most critical thing that we are worried about, we're going to focus on reducing the latency. And we're going to assume that the LAN is reliable and therefore let's not worry about the reliability of message transmission in a local area network. That's the idea behind the next thing that we're going to look at.</li> 
  <li>Let's think about all the thing that could go wrong in message transmission, and see why some of those things may not be that important given that we have a reliable local area network.
     <ul>
      <li>The first thing is, you send a message, it might get lost. But, if in a local area network, the chances that messages will actually get lost is not very high. It happens in wide area internet, because messages have to go out through several different routers, and they may be queuing in the routers, and there may be loss of packets in the wire and so on. But that's not something that you have to worry about in a local area network. So that assumption that messages may not get lost, suggests that there's no need for low level acknowledgements. Why? Because you're sending a call and the call is going to be executed and the result is going to come back. And usually in network transmission, we send acknowledgements to say, "yes, I received the message." Now, in this case, because the semantics of RPC says that the act of receiving the RPC call is going to result in server procedure execution and the result is going to come back, the result itself serves as the ACK. And therefore we don't need low level ACKs to say, "oh, I received your arguments of the call." You don't have to do that. And similarly, you don't have to have a low level ACK that says, "oh, I received the results." Because the results were not received, the caller or the client is going to resend the client call. So the high level semantic of RPC can itself serve as a way we can coordinate between the client and the server and we can eliminate low level ACKs and if we eliminate low level ACKs, that reduces latency in the transport.</li>
      <li>The second thing is in message transmission on the Internet, we worry about messages getting corrupted. Not maliciously or anything like that, but just due to vagaries of the network, messages may get corrupted in going on the wire that connects the source and destination. And for that reason, it's typical to employ checksum in the messages to indicate the integrity of the message that checksum is usually computed in software and appended to the message and sent on wire. But in a local area network things are reliable, we don't have to do extra overhead and software for generating the checksum, just use hardware checksum if it is available, just use hardware checksum for packet integrity. Don't worry about adding an extra layer of software in the protocol processing for doing software checksum. So that's the second optimization that you can make to make the protocol processing leaner.</li>
      <li> The third source of overhead that comes about in message transmission is once again related to the fact that messages may get lost in transmission. And therefore in order to make sure that if messages are lost in transmission, you usually buffer the packets. So that if the message is lost in transmission, you can re-transmit the package. Now once again let's think about the semantics of RPC. The client has made the call and the client is blocked and since the client is blocked, we don't need to buffer the message on the client's side. If the message gets lost for some reason, you don't hear back the result of the RPC from the destination, in a certain amount of time, you can resend the call, from the client side. And therefore you don't have to buffer the client side RPC message but it can reconstruct the client side message and resend the call. Therefore client side buffering is something that you can get rid of, once again, because the LAN is reliable.</li>
      <li>The next source of overhead, similar to client side buffering, happens on the server side, and that is the server is sending the results and the results may get lost. LAN reliable may not happen that often but it could happen ,and therefore we do want do the buffering. On the server side because if we don't buffer it then you have to reexecute the server procedure to produce a result and that's not something that you want to do because it involves reexecuting the server procedure which may be much more latency intensive then simply buffering the packet that corresponds to the result of executing the server procedure so you do want to buffer on the server site but the buffering on the server side can be overlapped with the transmission of the message. So in other words, the result has been computed by the server procedure. Now go ahead and send the result. While you are sending the result back to the client, do the buffering. That you can overlap the service side buffering with the result transmission, and get it out of the critical path of the latency for protocol processing. </li>      
    </ul>    
So, removing low level Acks, employing hardware checksum and not doing checksum in software, eliminating client-side buffering all together, and overlapping the server-side buffering with the result transmission are optimizations that you can do in protocol processing recognizing that the LAN is reliable. Therefore we don't have to focus so much on the reliability of message transmission, but focus rather on the latency, and how we can reduce the latency by making the protocol processing lean and mean.
</li> 
</ul>


<h2>12. Latency Limits Conclusion</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/44.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So once again recapping what we said. The sources of RPC latency are the following: Marshaling and data copying, context switches both at the client side and the server side. Similarly marshaling and data copying also happens both in the client side and the server side. And the actual protocol processing in order to send the packet on the wire. These are all the things that are happening in software. Those are the things that as OS designers, we have a chance to do something about. And what we saw were techniques that we can employ for each one of those, reduce the number of copies, reduce the number of context switches, and make the protocol processing lean and mean so that the latency involved in RPC is reduced to as minimum as possible from the software side. And we are going to take whatever the hardware gives us. If the hardware gives us an ability to do DMA from the client buffer, we'll use that but if it doesn't, then we have to anchor that. So that's what we are seeing here. As the opportunities for reducing the total latency in going from the client to the server and back to the client in RPC.
</li> 
</ul>


# L05d: Active Networks
<h2>1. Active Networks Introduction</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/45.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>In the previous lesson, we learned some tricks we can employ as operating system designers for optimizing the RPC communication software which powers client-server communication in the local area network from the point of view of reducing communication latency. Of course, user interactions go beyond the local area network to the wide area Internet. The primary issue, once a packet leaves your local node, is to route the packet reliably and quickly to the destination. Routing is part of the functionality of the network layer of the protocol stack of an operating system. What happens to a packet once it leaves your node? Well, the intermediate hardware routers between your node and the destination have routing tables that help them to move the packet towards the desired destination node by doing a table look-up. The routing tables evolve over time, since the Internet itself is evolving continually. That's the big picture. And there are lots of fascinating details which you can learn in a course that is dedicated to computer networking.</li>
   <li>For the next part of the lesson on distributed systems, we want to ask the question, what can be done in the intermediate routers, to accommodate the quality of service needs of individual packet flows through the network? Or in other words, can we make the routers en route to the destinations smart? The specific thought experiment we are going to discuss is called active networks. And then, we will connect the dots from active networks to the current state of the art, which is referred to as software defined networking.</li> 
  <li>Thus far in the course, we've been focusing on specializing operating system services for a single processor, or a multi-core or a parallel system, or a local area network. In this lesson, we will take this idea of specializing to the the wide area network. Specifically, we will study the idea of providing quality of service for network communication in an operating system by making the network active.
</li> 
</ul>

<h2>2. Routing on the Internet</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/46.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Normally, when we think about routing of packets on the internet, typically what happens is, at the source node, you create a network packet and go through the layers of software stack on the sending node, and send the packet out on the network. And this network packet has a desired destination. And of course, it has to go through a whole number of intermediate routers in order to get to its eventual destination. And the routers on the Internet that are intermediate between the source and the destination, they don't inspect the packet for the contents or anything like that. All that they're doing is when the packet comes in, they're looking at the destination known for that packet and figuring out what is the next hub that I have to send the packet to. So each router is making the determination of the next hub for the package, and it makes the determination by doing a table lookup. So every router has a routing table, and the routing table is telling that given a particular destination, what is the next hop? And that's how the packet flows from source to destination through a whole bunch of intermediate routers and finally gets to the destination. So in other words, the routers en route to the destination from the source are simply forwarding packets. That is the nodes are passive. And they're just doing a table lookup in order to figure out what is the next hop that I have to send this packet to? </li> 
  <li>Now, what does it mean to make the nodes active? What we mean by making the node active is that the next hop for sending this package towards a destination is not simply a table lookup, but it is actually determined by the router executing code that is actively as opposed to doing just a passive table lookup. So, in other words, the packet in addition to the payload that is intended for the destination also carries code with it. And the code is being executed by the router in order to make a determination as to what to do with this packet in terms of routing it towards the desired destination. This sounds really clever because it can provide customized service for networks flows that are going through the network. And every network can have its own way of choosing what may be the desired route, in terms of going from source to destination. And so in other words, we're saying "Well, this is an opportunity to virtualize the traffic flow from my network traffic independent of other network flows." This should be very familiar to you all because we've been talking about customizing operating system services in the SPIN operating system, and the Exokernel and so on. But, of course, the problem that we're talking about here is much much harder because the network is wide open. Our network traffic flow is going through the public internet infrastructure ,and we are talking about specializing the network flow for every network flow independent of others. There are lots of challenges to this values of active network. In particular, how can we write such code that we can distribute and send it over the wire so that routers can execute it. And who can write such code? And how can we be sure that the injected code does not break the network? Or in other words, for a particular network flow, there is a code that is going to be centered on. How do we make sure that is not going to in some way stymie other network flows? These are things that we have to worry about and sort of opening up the router and saying that we're going to take network flow specific decisions in each of the routers.
</li> 
</ul>

<h2>3. Active Networks Example</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/47.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Let me give you an example to motivate why this vision of active networks is both intriguing and interesting. You may all know that Diwali is a big festival in India, just like Christmas is in the Western world. And let's say that I am sending Diwali greetings electronically to my siblings, who are in India. What I can do, is I can send individually a greeting message to each of my siblings. And so, there'll be N messages going out on the internet from source to destination. So I send out N messages, and to reach any of my siblings. That's one way of doing it.</li> 
  <li>A nicer approach would be, given that all my siblings are clustered in one corner of the globe, it would be nice if I could send just one message, traverses the Internet, gets to close to the destination of where my siblings are, and the router that is at this end, demultiplexes my message and sends it to all my siblings. Obviously, the second method is more frugal in terms of using network resources. I don't have to send N messages. I can send one message, and finally at or close to the destination, an active node takes this one message, recognizes, oh, this is intended for multiple recipients, and demultiplexes them, and sends it to all the eventual recipients of this message.</li> 
  <li>We can generalize this idea and say that this idea of active router is going to be spread out throughout the Internet, so that even if my siblings, let's say, are distributed all over the world, then I could still send a single message from my source, and it gets demultiplexed along the way, depending on where all the eventual recipients are for this particular message that starts from me. So in other words, we can sprinkle this intelligence that is in this one particular router to all the routers in the Internet and that way we are making the entire Internet an active network. That's the vision behind active networks where the nodes in the internet will become, not just passive entities, but actually active in looking at the message and figuring out what to do with it, in terms of routing decisions.
</li> 
</ul>

<h2>4. How to Implement the Vision</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/48.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Now that we've motivated the vision, let's see how we can implement the vision. In order to implement this vision, the operating system should provide quality of service APIs to the application. And these quality of service APIs could be things like, "oh, this particular network flow that I'm creating has certain real time constraints, because it has video data and so on and so forth." And those are the hints that the operating system is going to use in terms of synthesizing code that corresponds to the API that the operating system is providing you to give hints to the network.</li>
   <li>The code that the operating system is going to synthesize is essentially taking the quality of service constraints and putting them as executable code that can be passed on as part of the packet. So in other words, the protocol stack of the operating system has to be enhanced to service or these quality of service requirements, and generally to synthesize the code that has to be part of the payload. So the application is not only providing a payload, but it is giving quality of service constraints. And the operating system, in addition to the payload, generates or synthesizes code corresponding to this quality of service instructions. And this slaps on the IP header for where this particular message is eventually to be delivered, and hands it over to the Internet. And in the Internet, if we assume that the Internet routers are capable of executing this specialized code, then depending on the nature of what is being requested, a particular order may say, "oh, this particular packet I have to send it to multiple destinations, so let me send this down this link, down this link", and similarly when it comes over here, this router may say, "oh, this packet has to go to multiple destinations." and that we can see that intelligent routing decisions can be taken in the network. That's sort of the roadmap of how we can take this vision and try to implement it.</li>
   <li>The problem with carrying out the vision, in terms of this implementation that I just sketched, is that changing the operating system is nontrivial, especially the protocol stacks as I already mentioned, TCP/IP has several hundred thousand lines of code, so it is nontrivial to change the protocol stack of every node in the entire universe to handle active networks. And also, the second part of the challenge is that the network routers are not open. So, in other words, we cannot expect that every router on the internet is capable of processing the code that I'm going on slap on to this payload and be able to make intelligent routing decisions. So there is an impedance mismatch between the vision and the implementation that I've sketched right here.
</li> 
</ul>

<h2>5. ANTS Toolkit</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/49.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So, the ANTS toolkit, ANTS stands for Active Node Transfer System, took a different approach to show the utility of the vision.</li>
   <li>Since modifying the protocol stack is nontrivial, instead the ANTS toolkit is really an application-level package. And this toolkit is available for the application programmer to say, "here is my payload and here is my quality of service constraints." And what the ANTS toolkit does is to create an ANTS header to this payload. So, the new payload looks like this. And this is called a capsule. A capsule consists of an ANTS header and the actual payload. And this is what is given to a normal operation system protocol stack.</li>
   <li>So this normal operating system protocol stack looks at this as the payload that has been given to it and it knows the destination address where this has to go sticks on the IP header for it. So, the new packet that is generated by the protocol stack, looks like this: It has the IP header, and the rest is payload so long as this protocol stack is concerned. But we know this payload consists of two parts: One is the normal payload that the application generated, and in addition to that there is the ANTS header that have been slapped on by the ANTS toolkit. </li>
   <li>This is what traverses the network, and when it traverses the network if a node in the network is a normal node, meaning it is not a smart node, but it is a normal IP router. Then it simply uses the IP header to say, "well, here is what I have to do in terms of sending the packet to the next hop towards the destination." On the other hand, if a node that receives this packet is an active node. Then, it can actually process this ANTS header, and say, "oh, this particular packet needs to be demuliplexed, and sent to two different routes." And it might take that intelligent routing decision based on the nature of that node. So that's the idea that we can push one of the paint points out of the operating system into an enhanced toolkit that lives above the operating system. So that's sort of the ANTS toolkit vision. That's one part.</li>
   <li>The second part, and of course the fact that the internet may not be open to opening up all of the routers to to be processing the specialized code that comes in the capsule. So, what we do is we keep the active nodes only at the edge of the network. In other words, the core IP network is unchanged, and all of the magic happens only at the edge of the network. So, once again, if I go back to my example of sending greetings to my siblings, then only the edge nodes have to do the magic in order to take my original message and process the code to deliver it to multiple destinations. So the rest of the network can remain unchanged. So the core of the IP network can be unchanged, and intelligence can be at the edge of the network. So this is sort of allowing or sort of matting this active network vision with the core IP network being unchanged.
</li> 
</ul>

<h2>6. ANTS Capsule and API</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/50.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>So having given you the high level description of what ANTS toolkit does, let's dig a little deeper and look at the structure of the ANTS capsule as well as the APIs provided by ANTS in order to do capsule processing.</li>
   <li>First of all, the header as I told you consists of three parts:
      <ul>
      <li> The original IP header, which is important for routing the package towards the destination if a node is a normal node, not an active node.</li>
      <li> The payload that was generated by the application</li>
      <li> In the middle is this ANTS header.there are two fields in this ANTS header that are particularly important. One is a type field, the other is a prev field:
         <ul>
            <li>The type field is a way by which you can identify the code that has to be executed to process this capsule. And this type field is really an MD5 hash of the code that needs to be executed on this capsule, and we'll come back to that in a minute.</li>
            <li>The second field that I said is important is the prev field. And this prev field is the identity of the upstream node that successfully processed the capsule of this type. And this information is going to be useful for us in terms of identifying the code that needs to be executed in order to process this capsule.</li>
            <li>We'll come back to how these two fields are actually used in processing a capsule once this capsule arrives at an active node. The short hint that I'll give you is that the capsule itself, as you see, does not contain the code that needs to be executed to process this capsule, but it only contains a type field. And this type field is a vehicle by which we can identify the code that needs to be executed to process this capsule. More on that in a minute.</li>      
         </ul>
         </li>   
      </ul>   
   </li>
   <li> Let's talk about the API that ANTS toolkit provides you. The most important function that we want to accomplish using the ANTS toolkit is forwarding packets through the network intelligently. So routing the capsule is the most important function that needs to be done. And that's most of what this ANTS API is all about.
      <ul>
      <li> (void routePerNode(), void deliverToApp(), void log()) : The part is contained right here saying "well, route this packet in this manner, and deliver the packet to an application." And this is the set of API calls that allows you to do routing of the capsule through the network. This is where what I said about virtualizing the network comes in, regardless of the actual topology or physical topology, I can take routing decisions commensurate with the network flow requirements contained in the capsule that arrives at a node.</li>
      <li> (object put, object get, object remove) :  The second part is the API for manipulating what is called a soft-store. Now, soft-store is storage that's available in every routing node for personalizing the network flow with respect to a particular type of capsule. And I mentioned earlier that the type is only a pointer to the code, not the code itself. And the soft-store is a place where we can store the code that corresponds to a particular capsule type. So the primitives that are available for manipulating the soft-store are things like put object and get object. The soft-store is basically key value store and in this key value store, you can store whatever is important for personalizing the network flow for capsules of this type. An obvious candidate for storing in the soft-store is the code associated with this type. So you can store the code associated with this type, so that future capsules of the same type, when it arrives at a particular node, they can retrieve the code from the soft-store and execute the code that needs to be executed for processing capsules of this type. Other interesting things that you might put into this soft-store are things like computed hints about the state of the network, which can be used for future capsule processing for capsules of the same type. </li>
      <li>(int getAddress(), ChannelObject getchannel(), Extension findExtension(),long time()) : The third category of API that's available is querying the node for interesting tidbits about the state of the network or details about that node itself, for instance, what is the identity of the node that I'm currently at, and what the local time is at this node, at this node, and so on and so forth. So these are the kinds of things that are available.</li>     
      </ul>   
      So the key thing that I want you to get out of looking at this ANTS API is that it is a very minimal setup API. So the number of API calls fits in this little table here. So that's the idea.</li>
   <li>Remember that routers are in the public Internet. And if you're talking about executing code in the router that is part of the public Internet, the router program that we're executing at a router node has to have certain important characteristics: Number one, it has to be easy to program. Number two, it should be easy to debug, maintain, and understand. And number three, it should be very quick, because we are talking about routing packets, and so the router program should not take a long time to do its router processing. So this very simple API allows you to generate very simple router programs that are easy to program because the APIs are simple, easy to debug, easy to maintain and understand. And the program itself is pretty small, that it's going to take not a humongous amount of time to do the packet processing.
</li> 
</ul>

<h2>7. Capsule Implementation</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/51.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
   <li>Let's talk about implementation of the capsule, and in particular what are the actions taken on capsule arrival at a particular node.</li>
   <li>I mentioned that the capsule does not contain code, but it is passed by reference. In other words, what the capsule contains is the type identifier which is really a fingerprint for the capsule code. And the way this type is generated is basically a cryptographic fingerprint of the original capsule code. In the original implementation of ANTS toolkit they used an MD5 hash of the code and used that as a fingerprint. And it was the case that at the time that this particular research was done, MD5 hash was a cryptographically strong hash function which was not broken. But subsequently MD5 hash has been broken. But nevertheless, the key thing that I want you to remember is that this type field is a cryptographically strong fingerprint that is derived from the capsule code. And that serves as a reference for the code itself.</li>
   <li>When a node receives a capsule, one of two things are possible. The first possibility is that this node has seen capsules of this type before. If that is the case, then it is quite likely that in the soft-store of this node the code that corresponds to this type is already existing, in which case it's a simple thing for the current node to retrieve the code from its soft-store, and execute the capsule and proceed with forwarding this capsule on to where its desired destination. On the other hand, if this capsule that arrived at this node is the first time that this node is seeing a capsule of this type, then obviously it's not going to have the code that corresponds to this type. In this case, what this current node is going to do is use the previous node field of the capsule and send a request to the previous node saying that, "hey I got this capsule of this type, I don't have the code. Do you have the code? If you have, please send it to me." And when this request comes in, the previous node, obviously it has processed this capsule before. It's quite likely that this previous node has the capsule code residing in its soft-store. And so it retrieves it from its soft-store and sends it to the next node,so that the next node has the capsule code and now can execute it, and also store it locally in its own soft-store so that future capsules of the same type when they arrive here, it can be processed using the code that is now stored in the soft-store. And the key point to takeaway is that typically when we are talking about network flows, we are sending a whole bunch of packets one after another. And therefore, even though the first packet that comes to a node may not find the code that is associated with that particular type, and we have to do a little bit of heavy lifting in terms of going and reaching back to the previous node to retrieve the code. Because network flow is a whole bunch of packets, there's going to be a whole lot of other packets that come down the pike. They're all going to be processed using the soft-store and the code that is stored in the soft-store. In other words, we are exploiting the locality for capsule processing by storing the code that arrives in response to our request back to a previous node in the local soft-store. One concern that we may have is that how do I believe that the code that I got from the previous node is actually the code that corresponds to this type or not. Well, this is where the cryptograpically strong fingerprint comes into play. What this node is going to do is, when it retrieves the code from the previous node, and when the code arrives, it is going to compute the fingerprint of the code that it just got. And see if that fingerprint matches the type field of the capsule. If it does, then it knows that this code is genuine. If it is not, then obviously somebody is trying to spoof my node by giving bogus codes so I'm going to reject it. So code spoofing can be avoided by having a fingerprint that is cryptographically strong, so that I can recompute the fingerprint match it against the type, and know that the demand load code that I got is actually the code that is associated with this particular capsule.</li>
   <li>And as I mentioned already, once I get this code, because I'm going to most likely see capsules of this type in the future as part of this particular network flow, I'm going to save the code in the soft-store for future use. So when a capsule arrives at a node. One of two things will happen. One is I will reach into my soft store and see if I have the code that matches this particular type in the capsule. If it isn't then I don't have the code, I'm going to reach back and get it from the previous node. But what if I go back to the previous node and the previous node does not have the code that corresponds to this type?</li>
   <li>So the action of a node when it cannot find the code that corresponds to a type either locally in its soft store or retrieving it from the previous node is to simply drop the capsule. Because what's going to happen is that if this capsule is dropped, higher-level acknowledgements that are happening in that particular network flow is going to indicate to the source that something did not get through, and that source node is going to retransmit that capsule. This is exactly the same thing that happens with IP routing on the Internet. That is, if a node cannot process all the packet that it gets, it simply drops the packet. That's the same semantic that is used for capsule processing also because we're relying on higher level protocols, the transfer protocol that sits on top of the network protocol do end to end acknowledgement to make sure that all the packets that they're expecting have actually reached. And therefore at the level of capsule processing, we don't have to worry if we cannot process the capsule either locally by using the code in the soft store or by retrieving the code from the previous node. Simply drop the capsule.</li>
   <li>Now, is it likely that, when we reach into the previous node, that node does not have the code for it? Because after all, it did process this capsule, and send this capsule on to me. It can happen. Because the soft-store is limited. Every router node has only a finite capacity, and all of its capacity is not going to give to a single network flow. It's going to give only a part of its storage for the network flow that is corresponding to a particular capsule type. And therefore, I have to live within my means. So, the capsule code may have to throw away stuff every once in a while if it is storing more than what its capacity is in the soft-store. So it is possible that the code that it originally stored in the soft-store, it had to replace it. Because it is like a cache, and therefore it may have replaced it, and therefore it may not have it. And this is particularly possible if this request comes at a much later time because one of the things that you associate with this kind of routing code is that it is going to be timely. So there is going to be a time associated with the validity of things that I want to keep in my soft store. So if I get a request at this node at a much later point in time because the capsule arrived at this node traversing all over the network, it may not have the code that corresponds to the type. So it is possible, so if it happens, simply drop the capsule and let the higher level entities in the protocol stack take care of retransmitting the capsule if need be.
</li> 
</ul>

<h2>8. Potential Apps</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/52.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So, how useful is active network? There are lots of potential applications that can be built using this active network's paradigm. In particular, what we want to do is, whenever we desire certain ways to visualize the behavior of the network, then active networks become very useful.</li>
   <li>For instance, for implementing protocol independent multicast reliable multicast, or noticing congestions in the network and notifying the source and the destination about the congestions, private IP, any casting. These are all the kinds of things that are useful to implement using active networks. And as you can see from this list, the kinds of things that you want to do, using active networks are things that are related to network functionality, not high-level application, but network functionality. In particular, it is useful for building applications that are difficult to deploy in the internet.</li>
   <li> When you rely on the routing and the internet, it is entirely an administrative set up, and the administrative set up tends to mirror the physical set up. But, for your particular network flow, you may want a set up that is different from the physical set up. And the example I give you of me sending a greeting that will be a single message for most of the traverse through the internet, but at some point, it may actually get demultiplexed and get sent to several different destinations. Those kinds of multicasting and things like that, are specific to network flow. So, we are in some sense, overlaying our own desire on topology or on top of the physical topography of the internet by using the active network paradigm. But the key properties that you want to have for applications that you want to build using the active networks paradigm, is that the applications should be expressible and it should be compact and it should be fast and it should not rely on all the notes being active.</li>
   <li>These are key things to note in building applications that live on top of the active networks. So all of these suggest, once again, what I said already and that is, it is for providing network layer functionality, not end application functionality. So, what you want in the network layer, that is something that again orchestrate using the active networks paradigm.</li> 
</ul>

<h2>9. Pros and Cons of Active Networks</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/53.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>So having talked about the vision and the practical implementation details of active networks, lets talk about the pros and cons.</li>
   <li>The pro is something that we have been stressing all along and that is the flexibility from the point if you have an application perspective that fact that you can ignore the physical layout of the network and slap on your own virtualization of what you want to accomplish for your network flow on top of the physical infrastructure is the key selling point for active networks.</li>
   <li>But this selling point comes at the cost of perhaps certain cons. What are all the cons?</li>
   <li>Con 1:  One concern could be protection threats.What I mean by that is that eroding infrastructures is carrying network flows, not really mine but yours and the third person and so on. Just like in an operating system when we have a process, we want to make sure that that process is not doing anything malicious to other processes in the same node. Same way, my network flow should not do anything that is detrimental to your network flow on the internet. So that's what we mean by protection threats. So there are some safeguards in the ANTS toolkit to address these protection threats.
      <ul>
      <li>The first is the runtime safety of ANTS program that is running on a router node. And the way they ensure that is by first of all implementing ANTS itself in Java and using Java sandboxing technique on the router node, so that anything that a router code is doing for capsule processing is limited to the Java's sandboxing that is executing in. And so, it cannot effect the flows of other network flows that are flowing through the same routing fabric. That's the first thing.</li>
      <li>The second protection thread is whether there could be code spoofing that can happen. We are talking about code being injected into the router, and of course there was a good reason for doing that, that I wanted a certain behavior to be observed by the network routers in response to packets flowing through the network that belongs to me. But I want to be sure that the code that is being executed is the code that I wrote, not some malicious code that is being spoofed to that node. Well here the safeguard is making sure that you have a robust fingerprint associated with the code. And so what you do is you generate a tight field for the capsule that is a crytographically strong finger print of the original code. And it always matching when you get the code from a previous known in response to your requst, you're goint to compute the fingerprint once more, check it against the fingerprint that is contained in the capsule, ensure that there is no code spoofing happening. That's how you can overcome this protection threat. </li>
      <li>The third concern can be integrity of the soft state. And what I mean by that is, the soft store that's available at an ordered node is limited in size, and you don't want any particular network flow to arbitrarily consume all of the soft state. And yet again, there is a restricted API that is provided in ANTS, that's the safeguard. For this protection threat. </li>      
      </ul>
      So these protection threats are concerns. But at least in the ANTS toolkit, they offer solutions to ensure that these protection threats are not show stoppers for active networks.
   </li>
   <li>Con 2:The second concern that one might have is Resource Management threats. What I mean by that is because we are executing code at a router. The result of that code execution could be that I proliferate packets in the internet.
      <ul>
      <li>I gave you an example of resending a message to my siblings. I send one message, and at some point, that one message becomes n message. So in some sense, we can start flooding the network with capsule processing. Is that a threat? Well it is, but internet is already susceptible to this kind of resource management threat and yes capsule adds to it, but it is not anything new that it is adding in terms of resource management threat. </li>
      <li>On the other hand, we can ask the question at each node is it going to consume more resources than it should. And this again comes back to the safegaurd that they have in the ANTS toolkit that the api is a restricted API and therefore the amount of resources that you can consume at a node is fairly restricted.So there is sort of a mixed answer to this resource management concern. At a given node, the resource management concern doesn't quite exist so long as you adhere to the restricted API of ANTS.</li>   
    </ul>
      The second concern that the capsules may flood the network can happen, but it already happens, we all experience spam on The internet so this is not adding any new problem but it is perhaps exacerbating an existing problem.</li>
   <li>So having looked at division and looking at division and the practicality of active networks, time for a quiz.
</li> 
</ul>

<h2>10. Roadblocks</h2>
<p> multiple choices </p>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/54.JPG?raw=true" alt="drawing" width="500"/>
</p>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/55.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
   <li>The right answers are the first two boxes here.</li>
   <li>If we want to do anything in the router, we need buy in from the router vendors. So this is a big challenge and convincing the router makers that,"yes please open up the network so that I can dump some code in it and execute the code," so that is a big challenge.</li>
   <li>The second is also a big challenge. If you look at the traffic on the internet today, it's just humongous. There's so much traffic on the internet, and this is the reason why routers are dumb animals. All that the do is when a packet comes in, it's all happening in hardware, they do a table look up to figure out "Given the destination, what is the next stop that I send this package to?" So the internet core, the routing fabric is operating at huge speeds, because even at the edge of the network today, we are already seeing gigabit speeds, which means that the core of the network, you have several hundreds of gigabytes of packet processing that needs to be done and therefore it is important that the core of the network be blazingly fast. And software routing is not going to be able to match the speed that is needed in the core of the network for packet processing. So these two choices are good choices.</li>
   <li>Does active network's make the Internet more vulnerable? Not really. Because the internet is already vulnerable. Perhaps, it adds to it but not particularly making it more vulnerable than what it is already.</li>
   <li>The last choice regarding code spoofing, so long as we make sure that the fingerprint that we generate to associate with the code, that is going to be used for processing the capsule when it arrives at a node is cryptographically strong, then we can make sure that this code spoofing does not happen.
</li> 
</ul>


<h2>11. Feasible</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/56.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
   <li>So let's talk about the feasibility of the vision of active networks. The reality is that router makers, like Cisco, are loath to open the network. So while the idea of active networks is fascinating, we can be frugal about the resources that we use in the Internet for different network flows and virtualize the physical infrastructure by slapping on our idea of the kind of network flow that I want for my packets, seems very attractive, but it's not going to be feasible given that we have to open up the network. So it's going to be feasible only at the edge of the network.</li> 
   <li>Secondly, when we are using active networks, we are talking about executing code in a router to determine the routing decision at that node. Or in other words, we're doing software routing. Software routing cannot match the hardware routing, because at the core of the network, there is so much of traffic being handled that you really want to do this in hardware, and doing this at software speed is not going to match the hardware speed of packet processing in the core of the network. So once again, this argues that an active network is only feasible at the edge of the network.</li> 
   <li>Finally there are social and psychological reasons why active networks is maybe a little bit hard to digest. It is hard for the user community to accept arbitrary code executing in a public routing fabric. If my traffic is flowing through the network and if the router is going to actually execute some code in order to do the processing of my packet, that worries me. Already, we talk a lot about privacy and the fact that in corporate networks, in university networks, we are losing a lot of privacy. People are watching what's going on. And now, saying that the routers are going to do something intelligent, smart processing packets, that might be a socially and psychologically unacceptable proposition.</li> 
   <li>So these are reasons why it would make it difficult to sell the idea of active networks to the wide area internet.</li>
   <li>On the other hand, the idea of virtualizing the network flow is very appealing. And if you put together the two thoughts that I had, one is the idea that we can virtualize the network and the second that active networks is only feasible at the edge of the network, that brings up a very interesting proposition, which I am going to mention in my concluding remarks.
</li> 
</ul>

<h2>12. Active Networks Conclusion</h2>
<ul>
  <li>Active network was way ahead of its time, and there was not a killer app to justify this particular line of thought. Further, active networks focused more on safety and less on performance, so in the 90s, it seemed more like a solution looking for a problem. But difficulties with network management, rise of virtualization, the right hardware support, and data center and cloud computing have all given active networks a new lease of life in the form of Software Defined Networking or SDN for short. Specifically, cloud computing promotes a model of utility computing where multiple tenants, by that I mean businesses, can host their respective corporate networks simultaneously on the same computational resources of a data center. Not that this will ever happen, but imagine Coke and Pepsicorporate networks running on the same data center resources. This means there is a need for perfect isolation of the network traffic of one business from another, even though each of the network traffic is flowing on the same physical infrastructure. This calls for virtualization of the physical network itself, hence the term software defined networking. You will learn more about SDN if you take a companion course on networking that is offered in this same program.
</li> 
</ul>


# L05e: Systems from Components

<h2>1. Systems from Components Introduction</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/57.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>I'm sure you have recognized that an operating system is a complex software system. Especially the protocol stack that sits within an operating system is several hundred thousand lines of code. And developing such a complex software system in general, Protocol stack in particular, to meet specs and also deliver very good performance is no mean challenge. Let's skip over to the other side of the fence and see how a complex hardware system, for example, a CPU chip like this one with billions of transistors on it, is built.</li>
   <li>VLSI technology uses a component-based approach to building large and complex hardware systems. Can we mimic the same method for building large complex software systems? That is, rather than start with a clean slate, can we reuse software components? It's very tempting since the component-based design will be easier to test and optimize at an individual component level. It'll also allow for easy evolution and extension through the addition and the deletion of components.</li>
   <li>The idea of component based design is orthogonal to the structure of the operating system that we've seen in previous lessons. Be it a monolithic kernel or a microkernel based design, the idea of component based design can be applied to each subsystem within an operating system. Of course, there are challenges. Can this lead to inefficiencies in performance due to the additional component level of function calls? Could it lead to loss of locality when we cross boundaries of components? Could it lead to unnecessary redundancies, such as copying, et cetera? Is it possible to get the advantages of component based design without losing performance? The short answer is 'yes' if you can put theory and practice together. We will explore this topic more in this lesson.</li>
   <li>The idea is to synthesize Network Protocol Stack from components. How do we do that? But that's what were going to see in this lesson. And in this lesson, kernel's ensemble project is used as a backdrop to illustrate how to build complex systems by using components, and using a component based approach to putting together a large complex hardware system.
</li> 
</ul>

<h2>2. The Big Picture</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/58.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>So, the idea is to put theory and practice together in the design cycle. Theory is good for expressing abstract specifications of the system at the level of individual components.</li>
   <li>Specify: For this part of the design cycle, namely specification of what we want to build. A theoretical framework called I/O automata is used and it's syntax is very similar to C-like syntax. So, in terms of writing the specification, it's very intuitive in how we go about using I/O automata to develop the system requirements and specify them using the syntax provided by I/O automata. And more importantly, the composition operator that's available in I/O automata allows expressing specification of an entire subsystem that we want to build. For example, if you want to build a TCP/IP protocol stack, then all of the functional relationship between the components of the subsystem can be expressed using the powerful specification language, primitives, available in I/O automata.</li>
   <li>Code: The second part of the design cycle is converting the specification in I/O automata to code that can be actually executed. And for this purpose, the programming language that is used is a high level programming language called OCaml, stands for object oriented categorical abstract machine language, and it has several properties that make OCaml a very good candidate for doing component based design as we are proposing to do in this lesson. Number one, the formal semantics that is available in OCaml is a nice compliment to the specification that we've done using I/O automata. That's number one reason why OCaml is a good vehicle for converting the specification into code. The second nice property that it has is it's object oriented. Being an object-oriented language, it has some very nice properties such as no side-effects for things that you do in the program. And in fact, this comes from the fact that OCaml is a functional programming language, so some of the properties of the functional programming language, including no side effects and so on, make this an appropriate vehicle to convert the specification into code. And third, perhaps the most important characteristic from an operating system designer's point of view, is that the code that you can generate with OCaml is as efficient as C code. This is super important when you're developing operating systems. Because you care about performance. Object oriented is good. Formal semantics is good. But you also want to know that you get good performance out of it. And yes, you can get pretty good performance out of OCaml. Because, the object code that we can generate from the OCaml compiler is very efficient, similar to C. All of these make OCaml a good vehicle for going from specification to actual code.</li>
   <li>At this point, what we have is an unoptimized code that faithfully implements the specification that we started out with. And remember that we are doing component-based design, so it's going to be highly unoptimized because there is a lot of craft that goes between these components that you put together, like Lego blocks.So we have to optimize it before it is ready for prime time. Now, how do we do that? Well, once again, return to theory.</li>
   <li>Optimize: Nuprl is a theoretical framework for optimization of OCaml code. The input to this framework is the OCaml code and the output that you get is an optimized version of a functionally equivalent OCaml code. So that's what this optimization framework gives you, takes the input unoptimized OCaml code and produces optimized Ocaml code. And the optimized OCaml code can be theoretically verified to be functionally equivalent to the unoptimized input OCaml code. That's the beauty of this theoretical framework. And the way it does that is a little bit beyond the scope of this course. It uses a theorem-proving framework in order to do this and verify through theorem proving that the resulting code that is generated is equivalent to the input code which is unoptimized.</li>
   <li>I've given you the big picture of the design cycle, now it is time to go into the weeds
</li> 
</ul>

<br>
<br>
<p><b>-------------------------------------------------------------------------------</b></p>
<p> <b> Note: Regarding the mentioned I/O automata & Nuprl</b></p>
<p> 1. Paper: Liu, Xiaoming, et al. "Building reliable, high-performance communication systems from components." ACM SIGOPS Operating Systems Review 33.5 (1999): 80-92.</p>
<p> 2. Nuprl: Nuprl is able to “understand” both the IOA specifications and the OCaml code, and can rewrite the code for the purpose of optimization.</p>
<p> 3. Specifications: </p>
<p> 1) Specifications of communication systems range along an axis from specifying the behavior of a system to specify its properties. When specifying the behavior, we describe how the system reacts to events. For example, we may specify that the system sends an acknowledgment in response to a data message. When specifying properties, we describe logical predicates on the possible executions of the system. An example of a property is that messages are always delivered in the order in which they were sent (FIFO).</p>
<p> 2) Specification of the behavior of both protocols and micro-protocols is done using I/O automata (IOA).The properties describe the system at the highest level. Since the properties do not specify how to implement a protocol, they are easy to compose.</p>
<p> 3) Behavioral specification provides the connection to the code by describing how to implement the properties. Behavioral specifications can be either concrete or abstract.</p>
<p> 4) Concrete specifications can be directly mapped onto executable code, while abstract specifications are nondeterministic descriptions that use global variables, and are therefore not executable. A concrete specification only involves state and events local to a single participant in the protocol (or signature in IOA terminology). A concrete specification is derived from the abstract specification by a process called refinement. This involves designing a protocol that implements the abstract requirements.</p>
<p> The difference between a concrete specification and an implementation is small: in an implementation, the programmer also needs to detect which conditions are true, and specify the order in which the corresponding actions should be executed.</p>
<p> 4. IOA: Below are examples of both abstractand concrete behavioral specifications of networks and protocols, using a variant of IOA as the specification language. The programming model is that of a state machine with event-condition-action rules. An abstract specification has a set of variables, a set of events that under certain conditions, modify these variables and allow interaction with the environment. For more information about IOA, check Book Lynch, Nancy. Distributed Algorithms, and the link https://groups.csail.mit.edu/tds/papers/Lynch/CWI89.pdf</p>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/IOA.JPG?raw=true" alt="drawing" width="700"/>
</p>
<br>
<h2>3. Digging Deeper From Spec to Implementation</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/59.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
   <li>So what I'm showing you here is sort of a software engineering road map to synthesizing a complex system.</li>
   <li>First we're going to go from specification to implementation. And the figure shows a workflow in building a complex system. In particular, what I focused on is a TCP/IP network protocol stack using this methodology of the design cycle that I presented. So I start with. What is called an Abstract Behavioral Spec. Now this is where we use this IOA . And this abstract behavioral spec is where we describe the functionality of the subsystem in terms of requirements. And we are presenting the high level logical specification of the components and the specification is really talking about the properties that we want the subsystem to adhere to, not how it is going to be accomplished, or how it is going to be executed, but the properties that it has to adhere to. For example, in the protocol, if we desire that the package should be delivered in order, that's a property that we want for the subsystem. Those are the behavioral properties that are going to express using the I/O Automata. Another behavioral specification properties that you can have are things like,"well, on every packet there should be an acknowledgement." That's another property that you want the protocol to have. You can express that using the I/O Automata. So the abstract behavioral spec is simply and lends itself to deriving properties about the behavior of the system. Not the execution, but the behavior of the system, such as FIFO and so on. It is not executable code, even though I told you that the syntax of I/O Automata is similar to C, it's not executable code.</li>
   <li>The interesting thing is that once you've expressed the abstract behavioral properties, you can actually verify whether the behavioral specification meets the properties that you want in the subsystem. So proving that the behavioral spec meets the properties is facilitated by the I/O Automata theoretical framework. That's the nice thing about this behavioral spec.  </li>
   <li>Once we know that the behavioral spec is meeting the properties that we set up for the subsystem, then we are ready to roll our sleeves and go down the path of implementing the behavioral properties.</li>
   <li>Next step in the process is getting to a concrete behavioral spec. We get to this concrete behavioral spec from the abstract behavioral spec through a whole set of refinements to this abstract behavioral spec. For instance, the refinement could be that, "if I have a queue data structure. I want FIFO property." How do I make sure that the abstract behavioral spec adheres to the additional execution condition that I want of first come, first serve. Those are the kinds of things that I can do, and refining the abstract behavioral spec to get a concrete behavioral spec. This is still not code, but it is closer to code than abstract behavioral spec. It's closer to implementation.</li>
   <li>From the concrete behavioral spec, we get to the actual implementation using OCaml as the programming language. And between the implementation and the concrete behavioral spec, there is not a whole lot of difference. It is the scheduling of the operations that we want in the concrete behavioral spec that is being detailed. When we go from this step (concrete behavioral spec) to this step (implementation) and produce OCaml code, which is finally an executable code for the original abstract behavioral spec that we started with. I already mentioned some of the reasons why they chose OCaml as the implementation vehicle, including functional programming language, formal semantics, and it also leads to compact code, high-level operations and data structures and it has features like automatic garbage collection, automatic memory allocation, and marshaling and unmarshaling of arguments. This is very important because I mentioned that we're going towards building a complex system from a specification using a component-based design approach. We're going to take these components and mesh them together, just like you take Lego blocks to build a toy. Similar to that, we are taking components and meshing them together to get the complex implementation, and when we do that We are necessarily going across different components and when we go across different components, you have to adhere to the interface specifications of those components, which means you have to marshall the arguments and unmarshal the arguments when you go between these components and OCaml has facilities for doing these kinds of marshaling and unmarshaling built into the language framework which makes it an ideal vehicle for Component-based design. And also, the programmability of OCaml is similar to C, and the definition of the primitives in OCaml makes it a good vehicle for developing verifiable code.
</li> 
</ul>

<h2>4. Digging Deeper From Spec to Implementation (cont)</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/60.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>At this point what we have is an unoptimized version of the abstract behavioral spec. Implemented, ready to run, but it is unoptimized. Remember that as component-based design, there's going to be a lot of craft that is going to be there in terms of the interfaces between the components that have been assembled together to build the complex system.</li>
   <li>One word of caution though, I mentioned that using I/O automata is fantastic from the point of view proving that the properties that we want in the original subsystem is actually met by the behavior spec. That is facilitated by the I/O automata framework. But the path that we took going from the abstract to concrete behavior spec, to the implementation finally ending up with unoptimized version, which is executable there is no guarantee that this an optimized version of code that we produced is actually meeting this behavioral spec. Or in other words, this leg of design exercise proving properties of the spec that it meets what we set out for the original subsystem, is in no way guaranteeing that those properties are actually in the implementation. There's no easy way to show that the OCaml implementation is the same as the I/O automata specification.</li>
   <li>This brings to mind a famous quote that is attributed to Donald Knuth. We know him as an expert in algorithm development. He's written several books on it. And a quote that is attributed to him is "Beware of bugs in the above code; I have only proved it correct, not tried it." So in other words, he is an expert in developing algorithm but he's saying "I need convert the algorithm to code. There's no way to prove that the code is actually faithfully reproducing the algorithm." That's sort of the same thing that is going on here as well. That we have an abstract behavioral spec. We can prove properties about the behavioral spec, and we can convince that whatever we set out for the subsystem is actually met, in terms of the specification but there is no way to prove. That this implementation is actually faithfully reproducing the behavioral spec. </li>
   <li>This is the software originating roadmap for synthesizing a complex system starting from the behavioral spec, all the way to an unoptimized implementation of the system. So those are the first two pieces of the puzzle, namely specification and then implementation. And the third piece of the puzzle of course is going from this unoptimized version to the optimized version.</li> 
</ul>

<h2>5. Digging Deeper</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/61.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>As I mentioned earlier, for going from the implementation to optimization, we once again turn to the theoretical framework. And in this case, we're going to use this Nuprl theorem proving framework. The Nuprl theorem proving framework is a vehicle by which you can make some assertions and prove some theorems about the equality of optimized and unoptimized code. let me elaborate.</li>
   <li>First of all, you start with this OCaml code, which is unoptimized and there is a tool that converts this unoptimized OCaml code to Nuprl Code. This is once again an unoptimized version of the original OCaml code, but it is a Nuprl code and this Nuprl code is one which is at the base of this theorem proving framework and what we can do with this theorem proving framework is convert this Nuprl code to optimized Nuprl code. And through a whole series of optimization theorems that are in this framework, we can actually show that the optimized Nuprl code is equivalent to the unoptimized Nuprl code. So we showed so far as the operating system design experience is concerned, we're going to treat this as magic. And if you are interested in digging deeper, I welcome you to do that. But for the purposes of this lesson, we're not going to go into the theoretical details of how this theorem proving framework does its work of ensuring that the optimized Nuprl code is the same as the unoptimized Nuprl code. Now, once this step is completed, then there is another tool that converts this optimized Nuprl code back into the optimized OCaml code. Now, we are ready to deploy this. So this is sort of the design cycle going one full round, going from specification to implementation, implementation to optimization and from optimization back to deployable code, that we can then take and put it on the system.
</li> 
</ul>

<h2>6. Putting the Methodology to Work</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/62.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Let's put this methodology to work. Specifically, we're going to look at how to synthesize a TCP/IP protocol stack using this methodology.</li>
   <li>Starting point is the IOA. We specify the protocol in all the detail that we want, abstractly. That's the abstract specification of the protocol. And we can prove that this abstract specification meets the properties that we want in the protocol using the IOA framework. And from the abstract specification, through a whole bunch of refinements, we can get to the concrete specification. We saw that already. Those are the two steps that we already mentioned how we take and synthesize in complex software.</li>
   <li>The next part, how do we synthesize the stack from the concrete spec? In other words how do we generate this OCaml code. That represents this abstract specification for the TCP/IP protocol stack. So we need to have a starting point for that. The starting point to get an unoptimized OCaml implementation starting from this concrete spec, is to use this Ensemble suite of microprotocols that the authors of this ensemble paper that I've assigned to you have synthesized at Cornell. Why use ensemble? Well, remember, our goal is to mimic the methodology that is used in building large-scale circuits. I mentioned at the outset that today building a billion transistor CPU chip, the way it works is they use a component-based design, taking components of hardware structures that have already been implemented and assemble them together in order to realize this big  mammoth chip. And we are trying to do the same thing for building complex software systems. So the starting point has got to be a set of components, and that's what Ensemble Suite of microprotocols give you. Now, if you think about a protocol like TCP/IP, that protocol has a lot of features in it, and each of those features require nontrivial amount of code building. So, for instance, TCP/IP has mechanisms for sliding window management, flow control, congestion control, scattering and gathering packets in order to assemble messages into units that can be delivered to the application. All of these things are features of the TCP/IP protocol. And the ensemble suite of microprotocols has components for each of those functionalities that you might have in any complex transport protocol, like TCP/IP. And if you recall the paper that we read earlier about optimizing RPC in a local area network, we said that one size fits all is not a good way to think about designing complex software systems. Depending on what the environment is, you have to adapt what the layers of systems off it do. And therefore, even though the TCP/IP protocol is well laid out in terms of what the functional requirements are, whether we need all of the components in a particular implementation of TCP/IP or not is something that is up for the designer to decide. And having a suite of micro protocols, gives you that freedom to mix and match the components that make sense in this specific environment for which to building a protocol stack. And that's the reason for using the ensemble suite of micro protocols. It has about 60 micro protocols that makeup the whole ensemble suite. The suite of micro protocols of ensemble written in OCaml and facilitates component-based design of a complex system such as TCP/IP protocol stack. And the micro protocols have well defined interfaces that allows composition. Every micro protocol has an interface for the layers that sit on top of it. An interface for interacting with layers that sit below it. And this is exactly the kind of component that you want so that you can actually assemble these components layer by layer in order to get the functionality that you want for the original protocol stack that you intended to build. Just to reiterate what we set as the original goal, we want to mimic VLSI design in building a complex software system. And this well defined interfaces of the ensemble, micro-protocol suite facilitates component based design.
</li> 
</ul>

<h2>7. How to Optimize the Protocol Stack</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/63.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Given a behavior spec for a protocol say TCP/IP, can a stack be synthesized as a composition of the ensemble micro protocols? Given 60 micro-protocols in the ensemble suite, there are way too many combinations for a brute force approach. Instead, in the ensemble approach, the user heuristic algorithm for synthesizing the stack given the desired behavioral spec and designer's knowledge of the micro protocols. The result is a protocol stack which is functional, but not optimized for performance. </li>
   <li>As operating system designers, we're always worried about performance. And we naturally have to think about how to optimize the protocol stack, so that it is not only functional, but also performant. In particular, the fact that we've assembled this protocol stack like Lego blocks, putting together all these micro protocols leads to layering and layering leads to inefficiencies. Now this is where the analogy to VLSI component based design breaks down. And the reason is because in VLSI component based design, even though we are building a complex chip, like a CPU, by putting together components, the components just fit together very nicely. There is no inefficiency in going between these components. But in software components, unfortunately, they have interfaces. And interfaces usually mean that there is well defined boundaries between these components and so to cross the component boundary, you may have to copy parameters at here to interface specifications of the components and so on. All of those things leads to inefficiencies and this where the VLSI component based design idea breaks down little bit when we just put together the components of software in order to build the large scale system. So we have to do the extra work that is needed in order to optimize the component-based design so that it can perform well. </li>
   <li>Fortunately there are several sources of optimization that are possible. For instance I mentioned that OCaml has implicit garbage collection. Now it is good that it has implicit garbage collection as a fall-back. But, maybe we don't want to use it all the time and we want to be explicit about how we manage our own memory that can be more efficient, that is a source of optimization. I mentioned that OCaml has ability for doing marshaling and unmarshaling of arguments to go across layers, but is also a very good thing to do, in order to have a component-based design. But, when you're going across layers, these things can add overheads and this is another source of optimization that is possible by avoiding these marshaling and unmarshaling across the layers by collapsing the layers.</li>
   <li>Another opportunity that exists especially in networking systems, is the fact that there's going to be communication and computation. If you think about TCP/IP Protocol, it has to necessarily buffer the packets that it's sending out. Because if the packets are lost for some reason, it may have to re-transmit it. This is again, a situation where we can overlap this buffering, which is computation on the node that is sending the packet with the actual transmission. So, we can overlap that. That's another opportunity for optimization.</li>
   <li>Another opportunity is compressing the header, especially when we have this layering, at every layer it might add a new header specific to that layer, and that may have common fields for instance, size of the packet or checksum for the packet and so on. Those are common things that you can eliminate when we are going across these layers. So the header compression is another possibility for optimization. Another thing that we always have to worry about, is making sure that the code that we execute fits in the caches and this has been something that we've talked about all along when we talked about single node systems to parallel systems. That locality enhancement, making sure that the working set of whatever code that is executing on a processor fits in the cache is very important. So, this again is an opportunity by identifying common code path across the different layers of the protocol stack, and co-locating that common code path across the layers, in order to make sure that we can enhance locality for processing is another source of optimization that is possible. This is all good.</li>
   <li>So there are lots of opportunities for optimization but do it by hand manually, that's tedious. So how do we automate the process of optimization so that we don't have to do it manually?
</li> 
</ul>

<h2>8. NuPrl to the Rescue</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/64.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>This is where we turn to Nuprl. As I mentioned already, Nuprl is a theoretical framework. And there is a way by which the OCaml code can be converted to Nuprl code. There is a tool that allows that transformation to be done very simply. We use that tool to convert the unoptimized OCaml code to the Nuprl code and once you have the Nuprl code, then we can roll up our sleeves and say how do we go about optimizing this Nuprl code.</li>
   <li>There are two step process for this. The first step is what is called static optimization and this requires that a Nuprl expert and the OCaml expert sit together and, in a semi-automatic manner, they go layer by layer in the protocol stack, and identify what transformation code can be applied in order to optimize each of the layers. We're not going across layers, but for every layer we're asking the question, is it possible to simplify the functionality of what is happening in one layer by looking at the Nuprl code and using the Nuprl framework and the optimization theorems that is part of that framework to come up with a more optimized implementation of each and every one of these layers. That's what we're doing in this first step. This is where both the Nuprl expert and the OCaml expert have to sit together because you want the Nuprl expert to know whether the optimization that they are attempting to do is co-share with respect to the functionality that is desired in the OCaml code. And that's why this is semi-automatic. Because the optimization itself can be done, using the theorem proving framework, but whether that optimization is an appropriate one or not is something that has to be verified with manual intervention. And the kinds of code transformation that can be done, are things like function inlining. If you have function calls, then you can make them inline. And that is one way of optimizing it within a layer and directed equality substitution is another optimization that can be done. There are some things that are very specific to functional programming, leading to code simplicity. And all of those things are what is being done in the static optimization of every layer in the protocol stack. Remember we are not going across layers, but every layer we're doing the static optimization. This is good, but unfortunately, that's not enough. Because we have lots of layers between a message arriving and the application getting that message or vice versa. And what we want to be able to do is, if possible, collapse all these layers. Because when you go through these multiple layers we're adding latency to the overall processing of any message. And this is where we are going to turn to the power of the Nuprl theorem proving framework.</li>
   <li>So the next step is dynamic optimization. Which is attempting to collapse these multiple layers, and it is completely automatic. The previous one I said, layer by layer, we're doing semi automatic fashion, there's completely automatic, and it is the power of that theorem proving framework which is actually going do that for you. And in order for that theorem proving framework to do this work, what we need to identify is, what are common things that happen in order to do this collapsing of layers.
</li> 
</ul>

<h2>9. NuPrl to the Rescue (cont)</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l5/65.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>The trick to this dynamic optimization is recognizing what the state of the protocol is at any point of time, and how the protocol has to react to an input event. Whether the event is coming from the top or from the bottom, how the protocol has to react to that event is the trick, and this is what is called the common case predicate. CCP for short.</li>
   <li>For example, if we receive a packet and the state of the protocol says that if a particular sequence number is received, then we are ready to deliver the packet to the application. If that is the state of the protocol, that is what is called a Common Case Predicate. These Common Case Predicates are predicates that can be derived from the state of the protocol by looking at the conditional statement. So, from the conditional statements in the microprotocol that have been implemented and available to you. We can actually synthesize these common case predicates. And once these common case predicates have been synthesized, then we can say, "well, if the common case predicate is satisfied, then you don't have to go through all the craft indicated by the multiple layers of the micro protocols assembled on top of one another. But instead, we can do a much simpler processing and that's what is called a bypass code." So, in the dynamic optimization, once these common case predicates have been identified by looking at the conditional statements in the micro-protocol, dynamic optimization framework generates these bypass code and inserts it into this framework.</li>
   <li>So I'm going to complicate this picture a little bit now, so what we have is A Common Case predicate. For instance it says, is the sequence number in the packet what I am expecting it to be. Is this the particular sequence number? And if that is the sequence number I am looking for, then I can execute this bypass code and completely eliminate all these multiple layers of protocol and go directly to the upper layer perhaps all the way up to the application. On the other hand, if it is not the common case predicate that is being satisfied, then, you have to do the normal processing, of giving the packet to this micro protocol and it does its own thing. And you can see that this kind of framework can be applied to every layer. So you can take, this is a single layer, and find out what the common case predicate for this micro-protocol, and use that common case predicate to signify whether we want to use the bypass code, or whether we want to go to the micro-protocol. And we can collapse multiple layers like this. And derive a common case predicate that collapses all of these layers into a single predicate, and if it is satisfied, then it can eliminate processing the packet to all of these layers, and simply go through the bypass code to get to the upper layers. That's the beauty of the dynamic optimization, and it's completely automated. And it comes from the power of the theorem proving framework of Nuprl. And the optimization theorem that is in the Nuprl framework proves the equivalence of the bypass code, to the layers of protocol that it is replacing. That's the beauty of the Nuprl framework, that in the theoretical world of the theorem proving framework, we can actually prove through optimization theorems. That this bypass code does exactly the same job as all of these multiple layers of micro protocols that were to process this message.</li>
   <li>So once we have done this dynamic optimization, and collapsing the layers, generating the bypass code, starting of course with the common case predicate derived from the micro-protocols, then we are ready to convert the optimized Nuprl called back to OCaml. And again, I mentioned that there is a tool, that's available. Straightforward tool that converts the Nuprl code to the OCaml code, and the final OCaml code that we generate through this conversion process is the optimized version of the original Ocaml code and the theorem proving framework can assert that the original unoptimized ocaml code is equivalent to the new optimized Ocaml call, because of the power of the theorem proving framework.</li>
   <li>This is where we've sort of put together theory and practice to get the best of both worlds. A word of caution, however. There's a difference between optimization and verification. All that the Nuprl framework is doing is optimization, not verifying whether the Ocaml code is adhering to behavioral spec of IOA. That's not what is being done. What we're doing here, is saying that "We've taken unoptimized Ocamel code and produce an optimized Ocamel code. And through the theoretical framework, we can assert that the two are functionally equivalent."</li>
   <li>So this exercise has shown the path to synthesizing complex system software, starting from specification, to implementation to optimization, putting theory and practice together.
</li> 
</ul>

<h2>10. Systems from Components Conclusion</h2>
<ul>
  <li>As operating system designers, the natural question that comes up is,"okay, all this sounds good. But do I lose out on performance for the convenience of component-based design?" This is the same question that came up when we wanted to go for a microkernel-based design away from a monolithic design of an operating system. The Cornell experiment takes it one step further and argues for synthesizing individual subsystems of an operating system from modular components, just like putting together lego blocks to get the desired functionality. I encourage you to read the paper from Cornell in full. That shows that this methodology, applied to one specific subsystem, results in a performance competitive implementation of the protocol stack, compared to a monolithic implementation.
</li> 
</ul>
