# Lesson outline
- [L07a: Global Memory Systems](https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/L07_Distributed%20Subsystems.md#l07a-global-memory-systems)
- [L07b: Distributed Shared Memory](https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/L07_Distributed%20Subsystems.md#l07b-distributed-shared-memory)
- [L07c: Distributed File Systems](https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/L07_Distributed%20Subsystems.md#l07c-distributed-file-systems)

# L07a: Global Memory Systems 
<h2>1. Global Memory Systems Introduction</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/1.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>We started the discussion of the distributive systems with definitions and principles. We then saw how object technology, with its innate concepts of inheritance and reuse, helps in structuring distributive services at different levels. In this next lesson module, we will discuss the design and implementation of some select distributed system services. Technological innovation come when one looks beyond the obvious and immediate horizon. Often this happens in academia because academicians are not bound by market pressures or compliance to existing product lines. You'll see this out of the box thinking in the three subsystems that we're going to discuss as part of this lesson module. Another important point, often the byproducts of a thought experiment may have more lasting impact than the original vision behind the thought experiment. History is ripe with many examples. We will not have the ubiquitous yellow sticky note but for space exploration. Now close to home, for this course, Java would not have happened but for the failed video-on-demand trials of the 90s. In this sense the services we are going to discuss as part of this lesson module, while interesting in their own right made themselves be not as important as the technololgical insights they offer on how to construct complex distributed services. Such technological insights are the reusable products of most thought experiments. 
</li> 
  <li>There's a common theme in the three distributed subsystems we're going to discuss. Memory is a critical, precious resource. With advances in network technology leveraging the idle memory of a peer in a Local Area Network may help overcome shortage of memory, locally available in a particular note. The question is how best to use the cluster memory, that is the physical memory of peers, tend to be idle at any particular point of time in a local area network. </li> 
  <li> Global memory system, GMS, asks the question, how can we use the peer memory for paging across the Local Area Network. And later on we will see DSM, which stands for distributed shared memory, asks a question, if shared memory makes the life of application programmers simple in a multiprocessor, can we try to provide the same abstraction in a cluster and make that cluster appear like a shared memory machine. And a third work that we will see is this distributed file system, in which we ask the question, can we use the cluster memory for cooperative caching of files. Anyhow, back to our first part of this three-part lesson module, namely the Global Memory System.
</li> 

</ul>

<h2>2. Context for Global Memory System</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/2.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Typically, the virtual address space of a process running on a processor, let's say your desktop, your PC, your laptop, and so on, is much larger than the physical memory that is allocated for a particular process. So the role of the virtual memory manager in the operating system is to give the illusion to the process that all of its virtual address space is contained in physical memory. But, in fact, only a portion of the virtual address space is really in the physical memory. And that's called the working set of the process. The virtual memory manager supports this illusion for the process by paging in and out from the disk the pages that are being accessed by a process at any particular point of time so that the process is happy in terms of having its working set contained in the physical memory. </li> 
  <li>When a node or a desktop is connected on a local area network to other peer nodes that are also on the same local area network it is conceivable that at any point of time, the memory pressure, meaning the amount of memory that is required to keep all the processes running on this node happy, may be different from the memory pressure on another nodes. In other words, this particular node may have a much higher memory pressure because the workload on this load consumes a lot of the physical memory whereas this work station may be idle and therefore all of this memory is not being utilized at this point for running anything useful because no applications are running on these nodes. This opens up a new line of thought.</li> 
  <li>Given that all these nodes are connected on a local area network and some nodes may be busy while other nodes may be idle. Is it possible if a particular node experiences memory pressure, can we use the idle cluster memory, and in particular, can we use the cluster memory available for paging in and out the working set of the processes on this node? Rather than going to the disk, can we page in and out to the cluster memories that are idle at this point of time? It turns out that with advances in local area networking gear, it's already made possible for gigabit ethernet connectivity to be available for desktops. And pretty soon, 10 gigabit links are going to be common in connecting desktops to the local area network. This makes sending a page to a remote memory or fetching a page from a remote memory faster than sending it to a local disk. Typically, the local disk access speeds are in the order of 200 megabytes per second in terms of transfer rate, but on top of that you have to add things like seek latency and rotation latency for accessing the data that you want from the disk. So all of this augers well for saying that perhaps paging in and out through the local area network to peer memories that are idle may be a good solution when we have memory pressure being experienced at any particular node in the cluster.</li> 
  <li>The global memory system or GMS for short uses cluster memory for paging across the network. In normal memory management, if virtual address to physical address translation fails then the memory manager knows that it can find this virtual address on the disk, meaning the page that contains this virtual address on the disk, it pages in that particular page from the disk. Now in GMS, what we're going to do is, if there is a page fault that is this virtual address to physical address translation, fails then that means that the page is not in the physical memory of this node. In that case, GMS is going to say, "well it might be there in the cluster memory in one of the nodes of my peers, or it could be on the disk if it is not in the cluster memory of my peers." So that's the idea that GMS is sort of integrating the cluster memory into this memory hierarchy.</li> 
  <li>Normally, when we think about memory hierarchy in a computer system, we say there's a processor, there's the caches, and there's the main memory, and then there is the virtual memory sitting on the disk. But now in GMS, we're sort of extending that by saying in addition to the normal memory hierarchy that exists in any computer system, that is processor, caches and memory, there is also the cluster memory. And only if it is not in the cluster memory, we have to think of going to the disk in order to get the page that we're looking for. That's sort of the idea.</li> 
  <li>So GMS trades network communication for disk I/O. And we are doing this only for reads or for reading across a network. GMS does not get in the way of writing to the disk, that is the disk always has copy of all the pages. The only pages that can be in the cluster memories are pages that have been paged out that are not dirty. And if there are dirty copies of pages, they are to be written onto the disk just like it will happen in a regular computer system. In other words, GMS does not add any new causes for worrying about failures because even if a node crashes all that you lose is clean copies of pages belonging to a particular process that may have been in the memory of this node. But those pages are on the disk as well. So the disk always has all the copies of the pages. It is just that the remote memories of the cluster is serving as yet another level in the memory hierarchy.</li> 
  <li>So just to recap the top level idea of GMS. Normally, in a computer system if a process page faults that means that the page it is looking for is not in physical memory, it'll go to the disk to get it. But in GMS, what we're going to do is if a process page faults, we know that it is not in physical memory but it could be in one of the peer memories as well. So GMS is a way of locating the faulting page from one of the peer memories, if it is in fact contained in one of the peer memories. If not, it's going to be found on the disk. So that's the idea behind that. So when the virtual memory manager that is part of GMS decides to evict a page from physical memory to make room for the current working set of processes running on this node, then what the virtual memory manager is going to do is, instead of swapping it out to the disk, it is going to go out on the network and find a peer memory that's idle, and put that page in the peer memory so that later on if that page is needed, it can fetch it back from the peer and memory. That's sort of the big picture of how the global memory system works.
</li> 
</ul>




<h2>3. GSM Basics</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/3.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>Let's introduce some basic terminologies. In GSM, when we talk about cache, what we mean is physical memory. We're not talking about the processor caches. We're talking about physical memory that is the dynamic random access memory, or DRAM for short, which is the physical memory. That's what we mean when we use the term cache.</li> 
  <li>There is a sense of community to handle page faults of a particular node. We'll get to that in a minute. </li> 
  <li>I mentioned that we are going to use peer memories as a supplement for the disk. In other words, we can imagine that the physical memory at every node to be split into two parts. One part is what we'll call "local", and local contains the working set of the currently executing processes at this node. So this is the stuff that this node needs in order to keep all the processes running on this node happy. Now, the "global" is similar to a disk. This global part is where community service comes in, that is, I'm saying that out of my total physical memory, this(Local) is the part that I need to keep all the processes happy in my node, and this is the part(global) that I'm willing to use a space for holding pages that are swapped out from my fellow citizens on the local data network. And this split of local and global is dynamic in response to memory pressure.</li>
   <li>As I mentioned earlier, the memory pressure is not something that stays constant, right? So over time, depending on what's going on in a particular node, you may have more need for holding the working set of all your processes, in which case the local part may keep increasing. On the other hand, if I go off for lunch, my workstation is not in use. And in that case, my local part is going to shrink, and I can house more of my peers swapped out pages in my global part of the physical memory.</li> 
  <li>The global part is a spare memory that I'm making available for my peers and local part is the part that I need for holding the working set of the currently active process at my node, and this boundary keeps shifting depending on what's going on at my node. Pretty simple.</li>
   <li>Normally if all the processes executing in the entire local area network are independent of one another, all the pages are private. You know, I'm running a process, my process, my pages, and the contents of that pages are private to my process. On the other hand, you could also be using the cluster for running an application that spans multiple nodes of the cluster, in which case it is possible that a particular page is shared, and in that case, that page will be in the local part of multiple peers, because multiple peers are actively using a page.</li> 
  <li>So we have two states for a particular page. It could be private, or it could be shared. If a page is in the global part of my physical memory, then it is guaranteed to be private. Because the global part is nothing different from a disk. So when I swap out something, I throw it onto the disk. Similarly, when I swap out something in GMS, I throw it into my peer memory as the global cache, and therefore what is in the global cache is always going to be private copies of pages, whereas what is in the local part can be private or can be shared, depending on whether that particular page is being actively shared by more than one node at a time.</li> 
  <li>Now one important point, the whole idea of GSM is to serve as a paging facility. In other words, if you think about even a uni processor, if it had multiple processes sharing a page, the virtual memory manager has no concern about the coherence about the pages that are being shared by multiple processes, that's the same semantic that is used in GSM, and that is coherence. For shared pages, it's outside GSM, it's an application problem. If there are multiple copies of the pages residing in the local parts of multiple peers, maintaining the coherence is the concern of the application. That is not GSM's problem. Only thing that the GSM is providing is a service for remote paging. That's important distinction that one has to keep in mind. In any workshop memory management system, what you do is when you have to throw out a physical page, you use a page replacement algorithm and the page replacement algorithm that is typically employed in computer systems is some variant of an LRU or a least recently used algorithm. GSM also does exactly the same thing, except it integrates cluster memory management of the lowest level across the entire cluster. So the goal in picking a replacement candidate in GSM is to pick the globally oldest place for replacement. If let's say that the memory pressure in the system is such that I have to throw out some page from the cluster memory onto the disk. The candidate page that I'll choose is the one that is oldest in the entire cluster.</li> 
  <li>Managing age information is one of the key technical contributions of GSM - how to manage the age information so that we pick a globally oldest page for replacement in the community service for handling page faults.
</li> 
</ul>


<h2>4. Handling Page Faults Case 1</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/4.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Before we dive into the nuts and bolts of how GMS is implemented and architected, let's first understand at the high level what's going on in terms of page fault handling. Now that we have this community service underlying GMS.</li> 
  <li>In this picture, I am showing you two hosts, host P and host Q. You can see that the physical memory on this host is divided into the local and the global part. Similarly, the physical memory on host Q is divided into the local part and the global part. And we already mentioned that these are not fixed in size but the size actually fluctuates depending on the activity and that's what we are going to illustrate through a example situations.</li> 
  <li>So the most common case is that I am running a process on P and that page faults on some page X. When that happens, you have to find out if this page X is in the global cache of some node in the cluster. Lets say that this page happens to be in the global cache of node Q.</li> 
  <li>So what will happen in order to handle this page fault is the GMS system will locate, oh this particular page it's on host Q. So it'll go to host Q. And the host Q will then send the page X over to node P and clearly, if there there was a page fault that means that the memory pressure on host P is increasing and therefore, it is going to add X to it's current working set. That is it's local allocation of the physical memory is going to go up by one but it cannot go up by one without getting rid of something here. Because the sum of the local and global is the total amount of physical memory available in this node and therefore, what P is going to do is, pick the oldest page that's available in the global part and send it over to node Q. So, in other words, what we are doing so far, as host Q is concerned is saying, well X happens to be currently in the working set, then resend it to host P. And host P says, well, my working set is increasing. Therefore, I have to shrink my community service and we going to reduce the global part by one. Pick the oldest page. Lets say it's Y. Send it over to host Q. So that the host Q can host this new page Y in the global cache on this node. </li> 
  <li>The key take away for you is that, for this particular common case, the memory pressure pressure on P is increasing, so the local allocation of the physical memory goes up by one, and the global allocation (the community service part) goes down by one on host P. Where as on host Q it remains unchanged because all that we have done is we have traded Y for X.
</li> 
</ul>

<h2>5. Handling Page Faults Case 2</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/5.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>The second case is simliar to the common case, but the memory pressure on P is exacerbated in this case. There's so much memory pressure that all of the physical memory that's available at P is being consumed by the working set, while hosting all the applications running on host P. There's no community service happening on host P. And now, if there is another page fault, for some new page on P, then there is no other option on host P except to throw out some page from its current working set in order to make room for this missing page.</li> 
  <li>So, we're going to do exactly similar to what we did in the previous case, with the only difference that the candidate that is being chosen as a victim or the replacement candidate (called a victim) in the management of virtual memory system, is coming from the local part of host P itself.</li> 
  <li>So we get the missing page and we send out the oldest page from local part of host P, recognizing that the local part is zero right now. So in this case you can see that there is no change in the distribution of local and global on P, because global is already zero, it's not going to be anything less than that. So the distribution remains unchanged. And as in the previous case, there's no change on host Q as well in terms of the split between local and global.
</li> 
</ul> 

<h2>6. Handling Page Faults Case 3</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/6.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>The third case is where the faulting page is not available in the cluster memories at all. In other words, the only copy exists on the disk.In this case, what has to happen is, when we have the page fault on node P for this x, we have to go to the disk to fetch it. And we're going to fetch it, which means that the working set on node P is growing, similar to the first case. And so the local part is going to go up by one. In order to make room for that, I have to necessarily shrink the global part as in the first case. So, I am going to shrink that global part by 1. By the way, I can pick any page from the global part, and both in the first case, as well as in this case, we can pick any page from the global part and send it out to a peer memory. And that's what we're doing here, so we are saying, "Here is a page that I have to get rid of. Who do I send it to? Well, I am going to send it to the guy that happens to have the globally oldest page in the entire cluster."</li> 
  <li>Let's say there is a host R that has the globally oldest page in the entire cluster, and that globally oldest page in the host R could be on the local part or the global part of this host. So what we're going to do is to tell that guy, "hey, I'm going to give you a page to hold, because this used to be in my global part, I don't have room anymore, because my local is increasing by one because of this page fault and adding x to my working set now. So please hold on to this page that I'm going to give you in your global cache." Now this guy has a split like this. So if it has to make room for this new page that is coming in from its peer, clearly it has to get rid of something. Where it will get rid of? Well, it has to throw it on the disk.</li> 
  <li>The interesting part is, if the oldest page on host R happens to be in the global cache of R, what can we say about that page z? Well, it has to be cleaned, because global part is nothing but a paging device. And therefore if it is here, it must be cleaned, and therefore, I can discard it without worrying about it. Just simply dump it. Drop it on the floor. That's what I'm going to do. On the other hand, if it is on the local part, it could be dirty. That is, if the oldest page happens to be on host R, and it also happens to be in the local part of host R, it is conceivable that this page has been modified, in which case, that modified copy has to be written out to the disk. So in other words, when we pick host R to send this replacement page from my host(Host P), this guy(Host R), what he's going to do is, he's going to say, "I have to get rid of a page in order to make room for this incoming global page, because I have the globally oldest page. And if I have the globally oldest page, then let me get rid of it by throwing it out onto the disk if it happens to be dirty. If it happens to be clean, simply drop it on the floor, because I know that all the pages are contained on the disk." </li> 
  <li>That's the fundamental assumption we started with, that all the pages of the currently active processes are on the disk. It is just that the global cache of every node is acting as a surrogate for the disk, because it can be faster to access from the peer memory than from the disk. So similar to the first case, in this case also, the local portion of the physical memory allocation on host P is going to go up by one, and the global portion is going to go down by one. What about host R, well it really depends. If the oldest page on host R happens to be in the global cache, then there is no change, because I am reading the old page z for another page that is coming in from host P, that is y that is coming in. In that case there is no change in the allocation between local and global on host R. But on the other hand, if the globally oldest page happens to be from the local part of host R, what that means is that even though originally we thought z to be part of the working set of host R, clearly, the processes that were using it are either no longer active or they're complete or whatever, and therefore, we're throwing out this page. The local part is shrinking. If the local part on host R is shrinking, what that means is, I can use more of the memory that's available in host R for community service. That's the important message I should get out. That's the important message I want you to get out of looking at this particular page fault scenario.
</li> 
</ul>


<h2>7. Handling Page Faults Case 4</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/7.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So far the cases that we considered are cases in which a page that we are faulting on is private to a process. Let's consider the case where a page is actively shared, that means that both of host P and host Q are actively sharing a page X, and currently some processes on host Q is using that page X, so it is in the local part in the working set portion of host Q. And for host P, the process is page faulting on the same page X. So when a page faults happens, GMS says, "oh, let me find out what it is." Well, it finds that x is in the local part of host Q and because it is in the current working set of host Q. We don't want to yank it away from there. We want to leave it there. We want to make a copy of this page into the local part of host P so that the faulting process on host P will have access to the page x.</li> 
  <li>Again, we have the same situation that the working set on host P is increasing. The local part has to allocate one more page in physical memory for the active working set for the processes on P. So the global part has to shrink by one. So, this local goes up by one, this global goes down by one. Pick again some arbitrary page from the global part. It doesn't have to be the LRU page, because all that your going to do is you're going to put this into the peer memory somewhere. In that sense, what we are doing is we're going to say "well, take this y and host in some of the peer cache". So who are we going to send it to? Well, we're going to send it to the host R that happens to have the globally oldest page.</li> 
  <li>Remember that in this situation, the total memory pressure of the entire cluster is going up by one, not just this host P, because x is going to be still present in the working set of host Q, and x is also going to be present in the working set of host P, which means the total memory impression in the entire cluster is going up by 1. And that is the case, then I have to necessarily pick a page from the entire cluster and throw it all under the disk, and that's what we're doing here. What we are saying is that host R happens to have the globally oldest page, and this page that I am replacing from host P in order to accommodate the missing page and bring it into my local cache on host P is to throw out why over to host R and host R in order to make room for this guy in it's global cache has to necesarily pick a victim from its physical memory and the victim it picks is going to be The LRU candidate, and that could come from the Local part or from the Global part. Again, if it comes from the Global part, we know it's private and we throw it away. if it comes from the Local part, it could be that this is a dirty copy, in which case we have to write all to the disk, or drop it on the floor if it is a clean copy because the disk always has all the copies of the pages. In either case the important message is that if the LRU candidate that we're going to throw out onto the disk or drop on the floor comes from the local part of R. That means similar to the previous case, the working set on local R is shrinking. It is coming down by one, which means host R can do more community service in the future. </li> 
  <li>Now the x is present in the working set of both host Q and host P which means actively some processor on host P, some process on host Q are accessing this page X. I mentioned earlier that this is not something that GSM worries about. If coherence is to be maintained for the copies of the same page existing on multiple nodes, that's not in the domain of GSM because GSM is simply a paging device. So it doesn't worry about coherence maintenance. There has to be something that the application or some higher level of the software has to worry about. So we can talk about the relative split between local and global. In this particular case, well, in this case host P local goes up by one, global goes down by one. In host Q nothing changes because there is an actively shared page which may relieve the copy here, just give a copy over to P, so the balance between local and global doesn't change. On the other hand in host R if the replacement candidate came out of the global part, then then there is no change in the split between local and global and host R either. On the other hand, if the replacement candidate came from the local cache of host R, what that means is that the working set, memory pressure on host R is decreasing. And therefore L will go down by one and G will go up by one.</li> 
  <li>I want you to think through all these four cases very carefully. The first three cases that we looked at were cases in which the pages were all private. And this is the only case where the page is potentially shared because it is in the local cache of Host Q and in the local cache of Host P as well, and now it is time for a quiz.
</li> 
</ul>


<h2>8. Local and Global Boundary</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/8.JPG?raw=true" alt="drawing" width="500"/>
</p>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/9.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>ANS: I'm sure your memory recall capability is very good and you would have come up with the right answers for all of these entries. Let me just quickly summarize what I'm sure that most of you have gotten already.</li> 
  <li>In all cases except where the global part of the faulting node Ps cache is empty, the local part of course is going to go up by one, the global part is going to come down by one. Only when the global part is empty already, there's no change.</li> 
  <li>And in both cases (in Q's global), there's no change to the balance between L and G on both the node Q, which is supplying the page X. And the node R that happens to have the LRU page, because it is not even part of the action, in terms of the page fault handling, for these two cases.</li> 
  <li>Now the third case, is where it is on the disk and therefore Node Q is immaterial. Right? Because it's not in any cluster memory right now. It is on the disk, so it's not applicable. Well, the question is what happens to the Node R that has the globally oldest page. If it is on the disk and we have to bring it into the faulting node, then necessarily we have to make space in the cluster memory as a whole for this extra page. Because this guy's balance is going to shift, and I have to throw this global page somewhere. I am picking node R as the replacement candidates, because he has the globally oldest page.</li> 
  <li>But what is going to happen on Node R? Well, it really depends on whether the LRU page comes from the global part or the local part. If it comes from the global part, then there is no change on R, because you're basically throwing this into the global part, and he's throwing one of those global pages out onto the disk, right? If on the other hand, the global page that I sent over here results in replacing a local page on node R, because the local page happens to be the LRU candidate then L goes down by one and G goes up by one. So we had able to do more community service in this case</li> 
  <li>In the last case when it is actively shared, again, even though we find the missing page in note Q, there's no change in the balance of L and G because it is coming from the active part of L, because it is shared. It indicates if it is shared it has to be, from the L part of Q. Since it is actively shared, there's no change in the split between L and G and of course, this guy has to send one of its global pages to some other node because the memory pressure as a whole is increasing on node P. This guy is going to send it to node R that has the LRU page, and just like in this case, where I have to make room for the cumulative memory pressure on the cluster by throwing out a page onto the disk. And similar to this case, if the candidate replacement page comes from the local part of Node R, then it is going to result in the local part shrinking by one and the global part increasing by one to accommodate the page that came from here.
</li> 
</ul>


<h2>9. Behavior of Algorithm</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/10.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>From the high level description of what happens on page faults and how it is handled, you can see that the behavior of this GMS global memory management is as follows:</li> 
  <li>So overtime, you can see that if there is an idle node in the LAN, then that idle node's working set is continuously going to decrease as it accommodates more and more of its peers pages that are swapped out to fit in its global cache. And eventually a completely idle node, becomes a memory server for peers on the network. So that's the expected behavior of this logarithm.</li> 
  <li>The key attribute of the algorithm is the fact that the split between Local and Global is not static but is dynamic, depending on what is happening at a particular node. For instance, even the same node, if it was serving as a memory server for my peers, because I had gone out to lunch, I come back, and start actively using my workstation. In that case, I'm going to go back, and the community service part of my machine is going to start decreasing. So the Local Global split is not static, but it shifts depending on what local memory pressure exist at that node.
</li> 
</ul>


<h2>10. Geriatrics!</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/11.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Now that we understand the high level goal of GMS, the most important parameter in the working of GMS system is age managemet because we need to identify what is the globally oldest page whenever we want to do replacements. Let's see how that algorithm works and how it is managed.</li> 
  <li>In any distributed system, one of the goals is to make sure that any management work is not bringing down the productivity of that node. In other words, you want to make sure that management work is distributed and not concentrated on any one node and you know in the GMS system, since we are working with an active set of nodes. Over time the node that is active may become inactive and a node that was inactive may become and so on. So you don't want the management of age information to be assigned to any given node. But it is something that has to shift over time. And that's sort of the fundamental tenant of building any distributed system is to make sure that we distribute the management work so that no single node in the distributed system is overburdened. That's a general principle, and we'll see how that principle is followed in this particular system.</li> 
  <li>So we break the management both along the space axis and time axis into what is called epoch. There are two parameters that govern an epoch. One is T which is the maximum duration of an epoch. That is an epoch is in some sense a granularity of management work done by a node. And that management work done by a node is either time bound, maximum T duration, or space bound, maximum M replacements. So if in the working of the system, if M space replacements have happened, then that epoch is complete, so we go to a new epoch. There'll be a new manager for age management. Or if the duration T is complete, then again, the epoch is complete, and so we pick a new manager to manage the next epoch. We'll see in a minute how we pick the new manager. And T maybe on the order of a few seconds. And M, which is the number of replacements, maybe on the order of thousands of replacements.</li> 
  <li>So at the start of each epoch what happens is every node is going to send the age information to the initiator. That is every node is going to say what is the age of the pages that is resident at this node - all the local pages and all the global pages /what is the age information associated with the universe of all the pages that exist at this node. Remember that the smaller the age, the more relevant the page is. So the higher the age, the older the page. So, in picking a candidate, we're always going to pick an old page to replace. So, that's the age information that each of these node is sending to the initiator. So, N1 sends its set of pages, N2 sends its set of pages and so on. Everybody is sending to the manager node that I'm calling the initiator, the age information.</li> 
  <li>What the initiator node is going to do is two things.
     <ul>
      <li>It's going to find out what is the minimum age of all the M pages that are going to be replaced. Remember that smaller the age, the better. So what it is going to say is out of all the pages that exist in the entire cluster, what are the oldest M pages that are going to be replaced in the upcoming epoch, and for those M old pages, out of those M old pages, what is the minimum age? So any page with less than the minimum age are pages that are active and that are going to survive this upcoming epoch. Whereas, any page whose age is older than that minimum age is part of this set of M pages that are going to be replaced in the upcoming epoch, and those are the replacement candidates. That's minimum age. So it computes the minimum age.</li>
      <li>It also computes given the minimum age and given the distribution of the age demographics coming from N1, I know out of these pages coming from N1 what fraction of the pages that belong to N1 are going to be replaced in the upcoming epoch. And that is the weight parameter for this particular node. For instance, if it turns out that N1 has 10% of the candidate pages that are going to be replaced in the next epoch, then its weight is 0.1. If N2 is going to account for 30% of the replacements, 30% of the M replacements in the upcoming epoch. Then N2's weight is going to be W2 and so on.</li>
    </ul>
     So what this initiative does is it computes this min age and it also computes the weight for each one of the nodes, and that's what is sent back to each node. So each node is going to see the min age and also the weight. Each load is not only receiving its own weight, that is its own fraction of the M pages that are going to be replaced, but it is also getting the fraction of the pages that are going to be replaced from each of its peer nodes in the entire cluster. We'll see how this information is going to be used by each one of these nodes. And of course, we don't know the future, all that the initiator is doing. That is saying that it is expected replacement. W1 is expected share of replacement that's going to happen in N1. W2 is expected share of replacement that's going to happen in N2 and so on. And when the next epoch starts actually, that can be different, depending on what happens in these nodes. But, that's the best that we can do is use the past to predict the future. That's what the initiates doing. It is using the past, the age information that it got from all the notes, in order to predict the future in terms of where these N replacements are going to happen. What is the minimum age of the pages that are going to be replaced in the next epoch. So that's what is being done by the initiator.
</li> 
</ul>

<h2>11. Geriatrics! (cont)</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/12.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>I mentioned that this management function that this initiator is doing, you don't want it to be statically assigned to one node, because that would be a huge burden, and besides this node will suddenly become very active, and if that happens, then you must time in the computations that are going to happen here to do this management function.</li>
   <li>Now intuitively if you think about it, if a particular node, lets say node N2 has a weight of point eight, what does that mean? What that means is that in the upcoming Epoch, 80% of the M pages are going to be replaced from the node N2, in other words, N2 is hosting a whole bunch of senior citizens, so it's a node that's not very active. Right? If an inactive node, then the replacement's candidates are probably not going to come from that guy and in predicting the future the initiator is saying that well, most of the replacements are going to come from here, so it's weighted very high. So this weight information for all the nodes is coming to every one of these guys. So intuitively if we think about it, the guy that has the highest weight is probably the least active. Why not make him the manager? So the initiator for the next Epoch is going to be the node with the maximum weight. Now how do you determine that? Well, every one of these nodes is receiving not only the min age of the candidate pages that are going to be replaced in the next epoch but also the weight distribution that says what is the percentage of these replacements that are going to happen in each one of these nodes. So locally each one of these guys can look at the Wi vector that it got back from the initiator and say, "I know, given that node N3 has the maximum rate, that's going to be the initiative of the next epoch." So locally you can make the determination so that for the next epoch, in order to send the age information, you know locally who to send it to.</li> 
  <li>Let's look at how this minimum age information is going to be used by each node. Remember that min age represents the minimum age of the M oldest pages. The smaller the age, the more recent that page is. And so those are the active pages. So if you look at this as the age distribution of all the pages, then if you draw a line here, then the pages to the left of the minimum age line are the active pages, and the pages to the right of this is the M oldest pages that are the candidate pages going to be replaced in the next epoch. This is the information that we're going to use in the way we manage page replacements at every node.</li> 
  <li>Let's see what the action is at a node when there is a page fault. If upon a page fault, I have to evict a page y, then what I'm going to do is to look at the age of this page y. If the age of this page y is more than the minimum wage, I know it's going to be replaced. Even if I sent it to a peer node, it's going to be replaced because that is part of the candidate M pages that are going to be replaced in the upcoming epoch. And therefore locally I'll make a decision that page y has an age older than minimum age, and therefore, I'm going to simply discard it. Don't worry about sending it to a peer. Remember that, in the description of how a page fault is handled, I said that, when you have a page fault in the node, I pick one of the replacement candidates, and I send it to a peer node to store it in the global cache of the peer node. Well, we don't want to do that. If that page is going to be eventually replaced during this upcoming epoch, meaning it has to be thrown out onto the disk. In that case, you might as well discard it right now. So if the age of the page that you're evicting from your node happens to be greater than MinAge, simply discard it.</li> 
  <li>On the other hand, if the page happens to be less than the MinAge, then you know that it is part of the active set of pages for the entire cluster, you cannot throw it away. You send it to Peer Node Ni. How do you pick Ni? This is where the weight distribution comes in. I know the weight of the nodes. At the end of this computation, the manager sent me the weight distribution for the upcoming epoch of all the nodes. So I can pick a node to send this page to based on the weight distribution. Of course I could say, well, send it to the guy that has the highest weight because that's the guy that is likely to replace, but remember that there's only an estimate of what is going to happen in the future. So, you don't want to hard code that. Instead, we're going to use some information drawn from these weights. We're going to factor that weight information into making a choice as to which peer we want to send this eviction page to. Chances are that the node has a higher weight is a likely candidate that'll pick. But it will not always be the node with the highest weight. Because if that decision is made by everybody, then we are going to have a situation where if the prediction was not exactly accurate, we would be making wrong choices.</li> 
  <li>Basically you can see that this age management, geriatric management, is approximating a global LRU. Not exactly a global LRU, because global LRU computing that on every page fault is too expensive, so we don't want to do that. Instead, we are doing an approximation to global LRU by computing this information at the beginning of an epoch, and using that information locally in order to make decisions. So we think globally in order to get all the age information, and compute the minimum age and compute the weight for all the nodes as to how much of the fraction of the replacements are going to come from each one of these nodes. But once that computation has been done, for the duration of the epoch, all the decisions that are taken at a particular node is local decisions in terms of what we want to do, with respect to a page that we are choosing as an eviction candidate. Do we discard it, or do we store it in a peer, global cache? This might sound like a political slogan but the important thing in any distributed system is as much as possible to use local information in decision making. So think globally but act locally. So that's the key and that's what is ingrained in this description of the algorithm.
</li> 
</ul>


<h2>12. Implementation in Unix</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/13.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>From algorithm description, we go to implementation. Now this is where rubber meets the road in systems research. In any good systems research, the prescription is as follows: You identify a pain point. Once you identify the pain point. You think of what may be a clever solution to that. Then a lot of heavy lifting actually happens in taking that solution, which may be a very simple solution, but implementing that is the hard part. if you take this particular lesson that we're looking at. The solution idea is fairly simple. The idea is that instead of using the disk as a paging device, use the cluster memory. But implementing that idea requires quite a bit of heavy lifting, and one might say that in systems research these technical details of taking an idea and working out the technical details of implementing that idea is probably the most important nugget. Even if the idea itself is not enduring, the implementation tricks and techniques that are invented in order to do their implementation of their idea maybe reusable knowledge for other systems research. So that's a key takeaway in any systems research and this true for this one as well.</li> 
  <li>In this particular case, the authors of GSM used a DEC Digital Equipment Corporation operating system as the base system. The operating system is called OSF/1 operating system. And there are two key components in the OSF/1 memory system, which is what we're talking about here.</li> 
  <li>First component VM: The first one is the virtual memory system, and this is the one that is responsible for mapping process virtual address space to physical memory, and worrying about page faults that happen when a process is trying to access the stack and heap and so on. So that it can bring those missing pages perhaps from the disk. And these pages are sometimes referred to as anonymous pages because a page is housed in a physical page frame and when a page is replaced, that same physical page frame may host some other virtual page and so on. So the virtual memory system is devoted to managing the page faults that occur for process virtual address space, in particular the stack and the heap.</li> 
  <li>Second component UBC:The unified buffer cache (UBC) is the cache that is used by the file system. Remember the file system is also getting stuff from the disk. But the file system wants to cache it in physical memories so that it is faster, so the unified buffer cache is serving as the extraction for the disk resident files when it gets into physical memory and user processes are going to do explicit access to files. When they do that, they're actually accessing unified buffer cache. So reads and writes of files go to this unified buffer cache. In addition to that, Unix systems offer the ability to map a file into memory, which is called memory mapped files. And if you have a memory mapped file, you can also have page faults to a file that has been mapped into memory. So the unified buffer cache is responsible for handling page faults to map files, as well as explicit read and write calls that an application process may make to the file system.</li> 
  <li>Normally this is the picture of how the memory system hangs together in any typical implementation of an operating system. You have the virtual memory manager, you have the unified buffer cache, you have the disk, and reads & writes from the virtual memory manager go to the disk, and similarly reads & writes from the unified buffer cache go to the disk</li>
   <li>Free list: when a physical page frame is freed, you throw it into the free list. So that it is available for future use by either the virtual memory manager or the unified buffer cache.</li>
   <li>Pageout Daemon: Pageout Daemon's role is to look at every once in a while what are pages that can be swapped out to the disk so that you make room for page faults to be handled without necessarily running an algorithm to free up pages.</li> 
  <li>So that's the structure of a memory management system, and what the authors of GMS did was to integrate GMS into the operating system, and this is where I said that there is heavy lifting to be done, because you are modifying the operating system to integrate an idea that you had for global memory system.
</li> 
</ul>


<h2>13. Implementation in Unix (cont)</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/14.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>This calls for modifying both the virtual memory part as well as the UBC (unified buffer cache) part of the original system to handle the needs of the global memory system. In particular, what you notice is that the VM and the unified buffer cache, when it wanted a missing page, it used to go to the disk. But here we're going to make it go to the GMS Because GMS knows whether a particular page that is missing in the address space of a process is present in some remote GMS. And similarly if a page is missing from the file cache, it knows whether that page is perhaps in the remote GMS. So that's why we modify the virtual memory manager and unified buffer cache manager to get their calls for getting and putting pages.</li> 
  <li>Getting page is when there is a page fault I need to get that page. I would go to the disk normally, but now I go to GMS. That GMS worry about getting it for me. Similarly, if a page is missing in the unified buffer cache, I go to the GMS to say "get me that page for me." And he will then do the work of finding out whether it is in remote GMS or if it is not anywhere in the cluster as we've seen in some of the cases. It could be under disk, it will get it from the disk.</li> 
  <li>Notice writes to disk is unchanged. Originally, whenever the VM manager decides to write to the disk, it's because a page is dirty. It writes to the disk. That part we don't want to mess with, because we are not affecting the reliability of the system in any way. Writes remain unchanged. Only when you have a page fault, you have to get it from the disk, you don't go to the disk anymore, but you come to GMS, and GMS then is integrated into the system so that it figures out where to get the missing page from.</li> 
  <li>Remember that the most important thing that we have to do in implementing this global LRU/approximation to global LRU is collecting age information. That's pretty tricky.</li>
   <li>Collecting age information with UBC: The case of file cache is pretty straightforward, because it is explicit from the process when it is making read & write calls, we can insert code as part of integrating GMS into this unified buffer cache, to see what pages are being accessed based on the region writes that are happening to the unified buffer cache. Because these calls are explicit from the application going into the operating system. And therefore, we can intercept those calls as part of integrating GMS to collect age information for the pages that are housed in the unified buffer cache.</li> 
  <li>Collecting age information with VM: On the other hand, VM is very complicated. Because memory access that a process does is happening in hardware on the CPU. The operating system does not see the individual memory access that a user program is making, so how does GMS collect age information for the anonymous? That's why these pages are called anonymous pages. In this case(UBC), the pages are not anonymous because we know exactly what pages are being reached in by a particular read or write by a file system, but the reads and writes that a user process is making to its virtual address space are the pages that are anonymous so far as the operating system is concerned. So, how does the operating system collect the age information? In order to do that, the authors of GMS what they did was to have a daemon part of the memory manager in the OSF/1 implementation. The daemon's job is to dump information from the TLB. If you recall, TLB, the translation lookaside buffer, contains the recent translations that have been done on a particular node. So the TLB contains information about the recently accessed pages on a particular processor. Periodically, say every minute, it dumps the contents of the TLB into a data structure that the GMS maintains so that it can use that information in deriving the age information for all the anonymous pages that are being handled by the VM. So that is how GMS collects the age information at every node. So this is where I meant, the technical details are very important. Because this idea of how to gather age information is something that might be useful if you're implementing a completely different facility.</li> 
  <li>Similarly, when it comes to the pageout daemon. The pageout daemon, normally, what it would do is, when it is throwing out pages. If they are dirty pages, it'll write it on to the disk. And if it is clean pages, it can simply discard it. But now, when it has a clean page and wants to discard a clean page, a pageout daemon will give it to GMS, because GMS will say, "well, don't throw it out. I'll put it into cluster memory so that it is available later on for retrieval rather than going to the disk."</li> 
  <li>So this is how GMS is integrated into this whole picture, interacting with the Unified Buffer Cache, interacting with the VM Manager, interacting with the Pageout Daemon and the Free List Maintenance as well in terms of allocating and freeing frames on every node.
</li> 
</ul>


<h2>14. Data Structures</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/15.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
   <li>Some more heavy lifting. Let's talk about the distributed data structures that GMS has in order to do this virtual memory management across the entire cluster.</li>
   <li>First thing that GMS has to do is to convert a virtual address, which is a local thing so far as a single processor's concerned. And convert that virtual address into a global identifier, or what we'll call as a universal ID (UID). And the way we derive the universal ID from the virtual address is fairly straightforward, if you think about it. We know which node this virtual address emanated from, IP address. We know which disk partition contains a copy of the page that corresponds to the virtual address. That we know. What are the i-node data structure that corresponds to this page? And what is the offset? So if you put all of these entities together, you get a universal ID that uniquely identifies a virtual address. This is the offset within a page. So given a virtual address, the first three parts uniquely identify the page, and the fourth part identifies the offset within that page for that virtual address. And this we can derive it from the virtual memory system as well as the UBC.</li> 
  <li>There are three key data structures: PFD (Page frame Directory), GCD (Global Cache Directory) and POD (Page Ownership Directory). These three data structures are the workhorses that make this cluster wide memory management possible. Let's talk about each one of these things.</li> 
  <li>PFD: PFD is like a page table. Normally in a page table what you do is you give it a virtual address and the page table says "oh, I know what is the physical frame that backs this particular virtual address." That is the translation between a virtual page number and a physical page frame that hosts that virtual page is contained in a page table. Similar to that, this PFD, it's called the page frame directory. It has a mapping between a UID, because your virtual address has been converted to a UID. Given a UID, it says, what is a page frame that backs that particular UID? That's this data structure. Because we're doing cluster-wide memory management, the page itself can be in one of four different states. It could be in the local part of that node. And if it is in a local part of a node. Then that page could be a private page or it could be a shared page. These are two possibilities, there is living in the local part of the physical memory of a node. If it is in the global part, we know by definition, global part is only hosting clean pages, so the content of global cache is always going to be private pages and so the state of the page that happens to be the global cache of a particular node is guaranteed to be a private state. And the last possibility is that it's not in the physical memory of a node, but it is on the disk. So the page frame directory just like a page table says that either this page that you are looking for go from VA to UID, that page is in physical memory and it is one of these three states or it is not in the physical memory but on the disk.
</li> 
</ul>

<h2>15. Data Structures (cont)</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/15.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>GCD: Given a UID, I know that some PFD in the entire cluster has the mapping for the UID saying "What is the physical frame number that corresponds to it if it happens to be on that node or it's on the disk." That information is contained in some PFD in the entire cluster. So, if I have a page fault, I convert my VA to UID, then which PFD will I go lookup? I could broadcast to everybody and say, "how do you have it?" That will be very inefficient. You don't want to do that. Or we can say," There is some way of magically mapping, given a UID, which node will have the PFD for me to look up?" And find out the physical backing page. But we don't want to do a static binding of UID to the node that manages that UID. Because if we make a static mapping, then it pushes the burden on one node to take that if some page has become hot and everybody wants to use that. What we want to do is distribute this management function. Just like the age management, we did not want to concentrate it on a single node. We want to distribute the management of giving this mapping between UID and which node has the PFD that can tell me information about the missing page. And that data structure is this Global Cache Directory, GCD. GCD is a hash table, it's a cluster-wide hash table, so it's a distributed data structure. And the rule that GCD performs is given a UID, It will tell you which node the PFD has that corresponds to this UID. That's the role of this data structure. It's a partition hash table, so given a UID, I can say, "Well, I go to the GCD." And the GCD will say, "what is the PFD that has this UID?" Because of the partition hash table, even though a part of this GCD is on every node, every node may not be locally able to determine where the PFD is. Given a UID, it can go and it has to know which GCD it has to consult. </li> 
  <li>POD: To know which node has the PFD that corresponds to this UID, we need another data structure that tells us, given a UID, which node has the GCD that corresponds to this UID. And that is the Page Ownership Directory. So, the page ownership directory says, "Given a UID, which node has the GCD that corresponds to this UID." And this data structure the page ownership directory is replicated on all the nodes. It's an identical replica that is on all the nodes.</li>
   <li>So, when I have a page fault, first thing that I'm going to do, is go to my POD and that is a replicated data structure and it's completed & up to date information. So I go to this POD and asked this question, "Given this UID, how do I find out the global cache directory that has information about the PFD that can help me to map my virtual address to a physical address?" Remember that we could have simply gone from here(PFD) to here(POD), but that would have been a static mapping, and this one level of indirection (GCD) is giving a way by which we don't have to statically map a PFD to a UID, but this intermediate step, allows us to move the responsibility of hosting a particular PFD to different nodes, using this intermediary which is a distributive hash table.</li>
   <li>I said that this page ownership directory is replicated data structure. Can it change? Well it can change over time because what this page ownership directory is saying is the following: The UID space is something that spans the entire cluster. If you take the virtual addresses of all the possibilities of the entire cluster. That universe of all the virtual addresses is this UID space because it is being mapped from a a virtual address of a single process to this UID space and this spans the whole cluster and what we have done is we have partitioned that UID space into set of regions of ownership and that's what is called the page ownership. So every node is responsible for a portion of the UID space and that is this global cache directory. Now if the LAN never revolves. In other words, if the set of nodes on the LAN is fixed, the page ownership director also remains the same. But if nodes, if new nodes are added and deleted and so on, that's when the page ownership directory is going to change. And if that happens, then we have to replicate again, we have to redistribute the page ownership directly. But this is not something that's going to happen too often. It's very rare that node is going to come down or new node is going to be inserted into a LAN. And therefore this page ownership directory does not change very often. And that's where it's replicated data structure that you can believe at every node. But if it changes, there is also a way of handling that. We'll see that in a minute.</li> 
  <li>The path for page fault handling is if you have a page fault you convert VA to UID and once you have this UID then you can go to your page ownership directory that's on your node. And ask the question, "please tell me who has the PFD that corresponds to this UID?" And GCD is going to tell me "oh here is the note that contains the PFD for the UID that you're looking for". Then I can go to that PFD, and from that PFD I can get the page that I am looking for which might be in that note or it might say "well, it's not in my note any more. It's on the disk". So this is the path for page fault handling.
</li> 
</ul>

<h2>16. Putting the Data Structures to Work</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/16.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So now that we understand the data structures, let's put the data structures to work. Let's first look at what happens on a page fault.</li> 
  <li>On a page fault, the node first coverts the virtual address to a UID. And once it converts it to the UID, it goes to the page ownership directory. And as I mentioned, the page ownership directory is something that I can trust. It'll tell me, given this UID, I'll tell you who the owner of this page is. And you go to him because he has the GCD data structure for this. So, node A finds out the identity of the owner for this UID, and that happens to be node B. So it sends the UID over to node B.</li> 
  <li>Node B, because it is the owner for this UID, it looks up its GCD data structure and says, "oh, the PFD that can translate this UID which actually represents this virtual address is actually contained in this particular node, node C." So, that's the content of this data structure, given a UID what is the node ID that contains the PFD. Remember that PFD is equivalent of a page table in the normal system, and therefore that's the node that I want to send this UID to so that we can do the translation for this virtual address.So node B sends the UID over to node C.</li> 
  <li>Node C contains the PFD that has the mapping between the UID and the page frame number that is backed by this node (Node C) for this UID, retrieves the page. It's a hit. Sends it over to node A. Node A is happy. It can then map this virtual address into its internal structure and the page fault service is complete and it can resume the process that was blocked for this page fault.</li> 
  <li>You can see that, potentially, when a page fault happens I can have three level of network communication. Of course the first lookup of the POD is local to my node because this POD data structure is replicated on every node. So from the UID, I can find the owner for the UID. But then I have to send a network message over to the node that contains the GCD for this UID. And then he'll then send it to the node that has this page so that that page can come back.</li>
   <li>Now this network communication (Node B -> Node C -> Node A) I am willing to tolerate because it is equivalent to performing the role of what the disk would have done. And maybe it is much better than going to a disk in order to get the missing page. So, it is happening only on page fault and since it is on a page fault, this network communication is okay. But, this (Node A -> Node B) is an extra network communication.</li>
   <li>Fortunately, the common case is a page fault is servicing the request of a local process on node A, and so the page is a non-shared page and if it is a non-shared page, most likely the UID space that corresponds to the missing page is also managed by this node (Node A). In other words, both the POD and the GCD corresponding to a particular UID is mostly on this node itself. And this is true for non-shared pages, and so A and B are exactly the same node. So there is no network communication to go from the POD to GCD to find out the PFD. So hopefully the only network communication that happens on every page fault is a network communication to go to this node that is serving the role of a disk and get the page from him. That's okay to incur because it probably is much lesser than going to the disk in order to do the page access. So the page fault service for the common case is going to be pretty quick.</li>
   <li>So the important message that I want you to get out of this is that even though these distributed data structures may make it look like you're going to have a lot of network communication, the important thing to note is that it happens only when there is a page fault. And since most of the page faults, the common case, is going to be non-shared pages. The data structure POD and GCD are probably co-resident on the same node. So, even though I've shown two different nodes here, A and B may be exactly the same node. So looking up the PFD that currently is backing this particular UID is going to be local to this node and so we can directly go to the node that contains the PFD and make the page fault service pretty quick.
</li> 
</ul>

<h2>17. Putting the Data Structures to Work (cont)</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/17.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Now the interesting question is, once I go to the node that I think is going to give me the page that I'm looking for, or at least information about the page that I'm looking for. Whether it is, if it has it, it's going to give it to me, or if it doesn't have it, it's going to tell me that it is on the disk. But, in either case, I'm hoping I'll get exact information about this missing page from this node which is supposed to have the page foame directory, the PFD that corresponds with this UID. Is that possible that I go to this guy, and he says "no, I don't have it."? Yes, it is possible in two cases.</li> 
  <li>One case is, let's say, while this guy was sending this request over, this guy has made a decision to evict that page that corresponds to this UID because it had to make space for itself. In that case, that UID may have been thrown away from the PFD. And if it has been thrown away from the PFD, what he would have done is to inform the guy who has the ownership for this UID, this node is the owner for this UID. If he evicts that page this guy has to tell this node that, "hey, you know what, I used to back this UID in my PFD but I got rid of it. And I got rid of it by sending it to some other node, let's say, node D". So that is something that I have to communicate to this GCD, but it's a distributed system. Things are happening asynchronously. He may not have communicated that yet, that information is not there in the GCD of this node. This is the owner for this UID space. But the owner doesn't yet know that the PFD that corresponds to a particular UID has moved to some other node out here somewhere. And if it has moved to some other node, he will know about eventually, but he doesn't have it at this point. That's why this request was routed here, and this guy says "I don't have it." It can have a miss. That's one possibility.</li> 
  <li>Second possibility is the uncommon case that the POD information that I had is stale. When can that happen? That is when the POD is being recomputed for the local area network as a whole, either because there are new additions or new deletions of nodes. And therefore we are recomputing the redistribution of the UID space and deciding which node is responsible for which UID. That can happen. And in that case, it is possible that the information that I started with was incorrect. Because I went here thinking that he has the GCD, he did have it at that point, but it is changing. And eventually I'm going to find out. So if there is a miss, either case.</li>
   <li>The first case is, this guy replaced that page, or the second case is, my POD information misled me. Both cases, I'll have a miss. And I'll say, "oh, it was a miss. And I know that is probably the uncommon case. I'm going to retry that by looking up my POD again. And by that time, the POD may have been updated, I'll go to the right GCD this time." Or, "the GCD would have been updated and so I'll go to the same GCD, but the GCD will have the more relevant information of which PFD is currently hosting it. So, I'll go to him in order to get the page that I'm looking for."</li> 
  <li>The important point I want to leave you with is that the common case is when both the POD and GCD are coresident on the same node. And in that case, you don't have a network communication to look up the GCD, and also the miss happening when you do reach the PFD. That is also uncommon. It can because happen because of replacement that has happened on that node, or because the POD has changed. And this is something that is going to happen relatively infrequently compared to the activities that we're talking about in terms of page faults.
</li> 
</ul>

<h2>18. Putting the Data Structures to work (cont)</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/18.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>The next thing I want to talk about is what happens in the system on page evictions. Remember that on page eviction, when a node decides that it wants to throw out a page, it sends it in the algorithm that I described to the node, which is candidate node for hosting that page, and it might use the weight information to make the decision of which node to send that page that it is discarding from it's own node. When a page fault happens, the key thing is to make sure that we service the page fault so that we get the page, and restart the execution that has been stalled on this node. That's the important thing to do.</li> 
  <li>The less important thing, but something that needs to happen in the universe of things that is being managed by GMS, is to also send the evicted page from this node to the target node. That's part of the algorithm for eviction on page fault. However, we don't want to do it on every page fault, but we want to do it in an aggregated manner. For that reason every node has a paging daemon. This is typical of virtual memory systems that when a page fault happens, that's not the time the virtual memory manager is running around trying to find the page free. It always has a stash of free page frames to allocate to service this particular page fault. </li> 
  <li>As I mentioned earlier, the paging daemon in the virtual memory manager is integrated with the GMS system. And what the paging daemon is going to do is, when the free list falls below a threshold, then the paging daemon is going to do put page of the oldest pages on this node. And remember that in the integration of GMS with the virtual memory manager in the put page, I said that the paging daemon is also modified to work with the GMS. This is where the modification comes in. Normally what the paging daemon would've done is when the free list goes below threshold, it would take a bunch of pages, LRU candidate pages, and put it all onto the disk. But with the integration with GMS, what the paging demon is going to do is, for the same condition when the freelist falls below threshold, it's going to basically do putpage of the oldest pages that it has on this node. And when we do the putpage, we're going to choose the candidate node, where we are going to do the putpage based on the weight information that we got as part of the geriatric management that we talked about in the beginning. And so we do a putpage of UID into this PFD of this node so that this guy will be the one that will be backing this particular UID. And once I do this, I also have to update the GCD to indicate that the new PFD for this particular UID is C. So this update message is being sent to the owner of this UID. That is this node, and the owner the guy that has the portion of the UID space that is managed by this node. So the GCD data structure contains the mapping of the UID to the node that contains the PFD for that particular UID. And so I send this update message saying please update the GCD to indicate that this particular UID is now backed by PFD that sites on note C. So that's the information I'm sending to this guy. And this is not being done on every page eviction, but it is done by the paging demon in a aggregated, coordinated manner. When the free list falls below a threshold.</li> 
  <li>So we've covered a lot of ground from just sort of the principal behind the thought experiment. That is, using network memory as a paging device rather than disk, because the networks have gotten faster. And we came up with an algorithm for age management globally in the entire cluster, and how to have that age management done in a manner that doesn't burden any one node. But it picks the node that is lightest in terms of load at any point of time. And we also saw given the solution for cluster-wide memory management for paging, how to go about implementing it. And all of the heavy lifting that needs to be done in order to take an idea and put it into practice.
</li> 
</ul>


<h2>19. Global Memory Systems Conclusion</h2>
<ul>
  <li>You'll see that in every systems research paper, there is heavy lifting to be done to take a concept to implementation. Working out the details and ensuring that all the corner cases are covered is a non-trivial intellectual exercise in building distributed subsystems. Like this one, the global memory system. What is enduring in a research exercise like this one? The concept of paging across the network is an interesting thought experiment, but it may not be feasible exactly for the environment in which the authors carried out this research, namely workstation clusters connected by a local area network. Each workstation in that setting is a private resource owned by an individual who may or may not want to share his resources, memory in particular. On the other hand, data centers today are powered by large scale clusters on the order of thousands of processes connected by a local area network. No node is individually owned in that setting. Could this idea of paging across the local area network be feasible in that setting? Perhaps. Even beyond the thought experiment itself what is perhaps more enduring are the techniques, distributed data structures and the algorithms, for taking the concept of implementation. In the next part of this lesson module, we will see another thought experiment to put cluster memory to use.
</li> 
</ul>

# L07b: Distributed Shared Memory

<h2>1. Distributed Shared Memory Introduction</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/19.JPG?raw=true" alt="drawing" width="500"/>
</p>


<ul>
  <li>In an earlier part of this lesson module, we saw how to build a subsystem that takes advantage of idle memory in peer nodes on a local area network, namely, using remote memory as a paging device, instead of the local disk. The intuition behind that idea was the fact that networks have gotten faster. And therefore access to remote memory may be faster than access to an electromechanical local disk.</li> 
  <li>Continuing with this lesson, we will look at another way to exploit remote memories, namely, software implementation of distributed shared memory. That is, create an operating system abstraction that provides an illusion of shared memory to the applications, even though the nodes in the local area network do not physically share memory.</li> 
  <li>So distributed shared memory asks the question, if shared memory makes life simple for application development in a multiprocessor, can we try to provide that same abstraction in a distributed system, and make the cluster look like a shared memory machine?
</li> 
</ul>


<h2>2. Cluster as a Parallel Machine (Sequential Program)</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/20.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Now suppose the starting point is a sequential program. How can we exploit the cluster? We have multiple processors, how do we exploit the cluster if the starting point is a sequential program?</li>
   <li>One possibility is to do what is called automatic parallelization. That is, instead of writing an explicitly parallel program, we write a sequential program. And let somebody else do the heavy lifting in terms of identifying opportunities for parallelism that exist in the program and map it to the underlying cluster. And this is what is called an implicitly parallel program. There are opportunities for parallelism, but the program itself is not written as a parallel program. And, now it is the onus of the tool, in this case an automatic parallelizing compiler, to look at the sequential program and identify opportunities for parallelism and exploit that by using the resources that are available in the cluster. </li> 
  <li>High Performance Fortran is an example of a programming language that does automatic parallelization, but it is user-assisted parallelization in the sense that the user who is writing the sequential program is using directives for distribution of data and computation. And those directives are then used by this parallelizing compiler to say, "oh, these are opportunities for mapping these computations onto the resources of a cluster." So it puts it on different nodes on the cluster and that way it exploits the parallelism that is there in the hardware, starting from the sequential program and doing the heavy lifting in terms of converting the sequential program to a parallel program to extract performance for this application.</li>
   <li>This kind of automatic parallelization, or implicitly parallel programming, works really well for certain classes of program called data parallel programs. In such programs, for the most part, the data accesses are fairly static, and it is determinable at compile time. So in other words, there is limited potential for exploiting the available parallelism in the cluster if we resort to implicitly parallel programming.
</li> 
</ul>


<h2>3. Cluster as a Parallel Machine (Message Passing)</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/21.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So we write the program as a truly parallel program, or in other words, the application programmer is going to think about his application and write the program as an explicitly parallel program. And there are two styles of writing explicitly parallel programs. And correspondingly, system support for those two styles of explicitly parallel programs.</li> 
  <li>One is called message passing style of explicitly parallel program. The run time system is going to provide a message passing library which has primitives for an application thread to do sends and receives to its peers that are executing on other nodes of the cluster. So this message passing style of explicitly parallel program is true to the physical nature of the cluster. The physical nature of the cluster is the fact that every processor has its private memory. And this memory is not shared across all the processors. So the only way a processor can communicate with another processor is by sending a message through the network that this processor can receive. This processor cannot directly reach into the memory of this processor. Because that is not the way a cluster is architected. So, the messaging passing library is true to the physical nature of the cluster that there is no physically shared memory.</li> 
  <li>Lots of examples of message passing libraries that have been written to support explicit parallel programming in a cluster. They include MPI, message passing interface, MPI for short, PVM, CLF from digital equipment corporations. So these are all examples of message passing libraries that have been built with the intent of allowing application programmer to write explicitly parallel programs using this message passing style. And to this day, many scientific applications running on large scale clusters in national labs like Lawrence Livermore, and Argonne National Labs and so on, use this style of programming using MPI as the message passing fabric.</li> 
  <li>The only downside to the message-passing style of programming is that it is difficult to program using this style. If you're a programmer who's written sequential programs, the transitions paths to writing an explicitly parallel program is easier if there is this notion of shared memory, because it is natural to think of shared data structures among different threads of an application. And that's the reason making the transition from sequential program to parallel programming, using for instance the Pthread library on SMP is fairly intuitive and easy pathway. On the other hand, If the programmer has to think in terms of coordinating the activities on different processes by explicitly sending and receiving messages from their peers. That is calling for a fairly radical change of thinking in terms of how to structure a program.
</li> 
</ul>


<h2>4. Cluster as a Parallel Machine (DSM)</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/22.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>This was the motivation for coming up with this abstraction of distributed shared memory in a cluster. The idea is that we want to give the illusion to the application programmer writing an explicitly parallel program that all of the memory that's in the entire cluster is shared. They are not physically shared, but the DSM library is going to give the illusion to the threads running on each one of these processes that all of this memory is shared. And therefore they have an easier transition path, for instance, from going from a sequential program or going from a program that they've written on an SMP to a program that runs on the cluster, because they don't have to think in terms of message passing. But they can think in terms of shared memory, sharing pointers across the entire cluster, and so on.</li>
   <li>Since we are providing a shared memory semantic in the DSM library for the application program, there is no need for marshaling and unmarshaling arguments that are being passed from one processor to another and so on. All of that is being handled by the fact that there is shared memory. So when you make a procedure call, and that procedure call is touching some portion of memory that happens to be on a remote memory. That memory is going to magically become available to the thread that is making the procedure call.</li>
   <li>In other words, the DSM abstraction gives the same level of comfort to a programmer who's used to programming on a true shared memory machine when they moved to cluster. Because they can use same set of primitives, like locks and barriers for synchronization, and the Pthread style of creating threads that will run on different nodes of the cluster. And that's the advantage of DSM style of writing an explicitly parallel program.
</li> 
</ul>


<h2>5. History of Shared Memory Systems</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/23.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Now having introduced distributed shared memory, I want to give sort of a birds eye view of the history of shared memory systems over the last 20+ years. My intent is not to go into the details of everyone of these different systems, because that can take forever, but it is just to give you sort of the space occupied by all the efforts that have gone on in building shared memory systems both in hardware and in software. I encourage you to surf the web to identify papers and literature on these different systems that have been built over time, just to get a perspective on how far we have come in the space of building shared memory systems.</li> 
  <li>Software DSM: A few thoughts on the software side, software DSM was very first thought of in the mid 80s. The Ivy system that was built at Yale University and the Clouds Operating System that was built at Georgia Tech and there were similar systems built at UPenn. This I would say is the beginning of Software Distributed Shared Memory.Later on, in the early' 90s, systems like Munin and TreadMarks were built. I would call them perhaps a second generation of Distributed Shared Memory systems.In the later half of the 90s, there were systems like Blizzard, Shasta, Cashmere and Beehive. That took some of the ideas from the early 90s even further.</li> 
  <li>Structured DSM: And in parallel with the software DSM, I would say there was also a completely different track that was being pursued. And that is, providing structured objects in a cluster for programming. And systems such as Linda and Orca, were done in the early 90s. Stampede at Georgia Tech was done in concert with the Digital Equipment Corporation in the mid 90s and continued on, later on, into Stampede RT and PTS, and in fact, in a later lesson, we'll talk about Persistent Temporal Streams. And this particular axis of development of structured distributed shared memory is attractive because it gives a higher level abstraction than just memory to computations that needs to be built on a cluster.</li> 
  <li>Hardware DSM: Early hardware shared memory systems such as BBN Butterfly and Sequent Symmetry appeared in the market in the mid 80s and, the synchronization paper that we saw earlier by Mellor-Crummey and Scott used BBN Butterfly and Sequent Symmetry as the experimental platform for the evaluation of the different synchronization algorithms. KSR-1 was another shared memory machine that was built in the early 90s. Alewife was a research prototype that was built at MIT, DASH was a research prototype that was built at Stanford and both of them looked at how to scale up beyond an SMP, and build a truly distributed shared memory machine. And commercial versions of that started appearing. SGI silicon graphics built SGI origin 2000 as a scalable version of a distributed shared memory machine. SGI Altix later on took it even further, thousands of processors exist in SGI Altix as a large-scale shared memory machine. IBM Bluegene is another example. And today, if you look at what is going on in the space of high performance computing. It is clusters of SMPs which have become the work horses in data centers.</li> 
  <li>I very much want you to reflect on the progress that has been made in shared memory systems. And invite you to look at some of the details of machines that have been built in the past, either in the hardware or in software, so that you can learn the progress that has been made.
</li> 
</ul>


<h2>6. Shared Memory Programming</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/24.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>I've already introduced you to shared memory synchronization. Lock is a primitive and particularly the mutual exclusion lock is a primitive that is used ubiquitously in writing shared memory parallel programs to protect data structure so that one thread can exclusively modify the data and release the lock so that another thread can inspect the data later on and so on. And similarly, barrier synchronization is another synchronization primitive that is very popular in scientific programs and we have covered both of these in fairly great detail in talking about what the operating system has to do in order to have efficient implementation of locks as well as barriers.</li>
   <li>Now the upshot is, if you are writing a shared memory program, there are two types of memory accesses that are going to happen. One type of memory access is the normal reads and writes to shared data that is being manipulated by a particular thread. The second kind of memory access is going to be for synchronization variables that are used in implementing locks and barriers by the operating system itself. It may be the operating system, or it could be a user level threads library that is providing these mutual exclusion locks, or barrier primitives, but in implementing those synchronization primitives, those algorithms are going to use reads and writes to shared memory.</li>
   <li>So there are two types of shared memory accesses going on in the execution of a parallel program. One is access to normal shared data and the other is access to synchronization variables.
</li> 
</ul>


<h2>7. Memory Consistency and Cache Coherence</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/25.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Recall that in one of our earlier lectures, we discussed memory consistency model and the relationship of memory consistency model to cache coherence, in the context of shared memory systems. Memory consistency model is a contract between the application programmer and the system. It answers the when question, that is, when a shared memory location is modified by one processor, when, that is how soon that change is going to be made visible to other processes that have the same memory location in their respective private caches. That's the question that is being answered by the memory consistency model.</li>
   <li>Cache coherence, on the other hand, is answering the how question, that is, how is the system, by system we mean the system software plus the hardware working together, implementing the contract of the memory consistency model? In other words, the guarantee that has been made by the memory consistency model, to the application programmer has to be fulfilled by the cache coherence mechanism. So coming back to writing a parallel program, when accesses are made to the shared memory, the underlying coherence mechanism has to ensure that all the processes see the changes that are being made to shared memory, commensurate with the memory consistency model.
</li> 
</ul>


<h2>8. Sequential Consistency</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/26.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>I want you to recall one particular memory consistency model that I've discussed with you before, that is sequential consistency. And in sequential consistency, the idea is very simple. The idea is that every process is making some memory accesses, all of these, let's say, are shared memory accesses. And from the perspective of the programmer, the expectation is that, these memory accesses are happening in the textual order that you see here and that's the expecation so far as this programmer is concerned. Similarly, if you see the set of memory accesses that are happening on a different process of P2. Once again, the expectation is that the order in which these memory accesses are happening are the textual order.</li>
   <li>Now, the real question is, what happens to the accesses that are happening on one processor with respect to the accesses that are happening on another processor if they are accessing exactly the same memory location? For instance, P1 is reading memory location a, P2 is writing to memory location a. What is the order between this read by P1 and this write by P2? This is where sequential consitency model says that the interleaving of memory accesses between multiple processors, here I'm showing you two, but you can have n number of those processors, making accesses to shared memory all in parallel. When that happens you want to observe the textual program order for the accesses and the individual processes but the interleaving of the memory accesses coming from the different processors is arbitrary.</li>
   <li>So in other words, the sequential memory consistency model builds on the atomicity for individual read-write operations and says that, individual read-write operations are atomic on any given processor, and the program order has to be preserved. And, in order to think about the interleaving of the memory axises that are happening on different processors. That can be arbitrary and that should be consistent with the thinking of the programmer. I also gave you the analogy of a card shark to illustrate what is going on with a sequential consistency model. So the card shark is taking two splits of a card deck and doing a perfect merge shuffle of the two splits, and that's exactly what's going on with sequential consistency. If you can think of these memory accesses on an individual processor as the card split but instead of a two-way split you have an n-way split, and we are doing a merge way shuffle of all the n-ways. Splits off the memory accesses to get the sequentially consistent memory model.
</li> 
</ul>


<h2>9. SC Memory Model</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/27.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>With the sequentially consistent memory model, let's come back to a parallel program. So, a parallel program is making read write accesses to shared memory, some of them offer data, and some of them offer synchronization. Now, so far as the sequentially consistent memory model it does not distinguish between accesses coming from the processors as data accesses  or synchronization accesses. It has no idea. It only looks at the read write accesses coming from an individual processor and honoring them in the order in which it appears and making sure that they can merged across all these processors to preserve the SC guarantee.</li>
   <li>So the upshot is that there's going to be coherence action on every read write access that the model sees. If this guy writes to a memory location, then the sequentially consistent memory model has to ensure that this write is inserted into this global order somewhere. In order to insert that in the global order somewhere, it has to perform the coherence action with respect to all the other processors. That's the upshot of not distinguishing between normal data accesses and synchronization accesses that is inherent in the SC memory model.
</li> 
</ul>


<h2>10. Typical Parallel Program</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/28.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Now, let's see what happens in a typical parallel program. In a typical parallel program that you might write, you probably get a lock, and you have, mentally, an association between that lock and the data structures that are governed by that lock. Or in other words, in writing your parallel program, you decided that access to variables a and b are governed by this lock. So if I wanted to read or write variables a and b, I'll get a lock and then I will mess with the variables that are governed by this lock. Once I'm done with whatever I want to do with these shared variables, I'll unlock indicating that I'm done. And this is my critical section. So within the critical section, and we're allowed to do whatever I want on these data structures that are governed by this particular lock, because that is an association I as the programmer has made in writing the parallel program.</li>
   <li>So if another processor let's say P2 gets the same lock. It's going to get the lock only after I release it. So only after I release the lock, this guy can get this lock because the semantics of the lock, it is a mutually exclusive lock. And therefore, only one person can have the lock at a time. And consequently, if you look at the structure of this critical section for P2, it gets a lock. And it is messing with the same set of data structures that I was messing with over here. But, by design, we know that either P1 or P2 can be messing with the data structure at any point of time. And that's a guarantee that I know comes from the fact that I designed the pilot program. And the lock is associated with these data structures. So, in other words, P2 is not going to access any of the data that is inside this critical section until P1 releases the lock.</li> 
  <li>We know this because we designed this program, but the SC memory model does not know about the association between these data structures and this lock. And, in particular, doesn't even know that memory accesses emanating from the processor due to this lock primitive is a different animal compared to the memory accesses coming from the processor as a result of accessing normal data structures.</li> 
  <li>So the cache coherence mechanism that is provided by the system for implementing the memory consistency model is going to be doing more work than it needs to do because it's going to be taking actions on every one of these accesses, even though the coherence actions are not warranted for these guys until I release the lock. So what that means is that there's going to be more overhead for maintaining the coherence commensurate with the SC memory model, which means it's going to lead to a poorer scalability of the shared memory system.</li> 
  <li>So in this particular example, since P2 is not going to access any of these data structures until P1 has released the lock, there's no need for coherence action for a and b until the lock is actually released.
</li> 
</ul>


<h2>11. Release Consistency</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/29.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>This is the motivation for a memory consistency model, which is called release consistency. I'm sure just from the keyword release some of you may have already formed a mental model of what I'm going to say. Basically, we're going to look at the structure of the program as follows that the Parallel program consists of several different parallel threads P1 is one such, and if it wants to mess with some shared data structures, it is going to acquire a lock, we'll call it a1, and in the mind of the programmer there is an association between this lock and the data structures governed by it. So, so long as they hold the lock, they can modify the data structure and r1 is the release of this lock. So every critical section you can think of as composed of and acquire followed by data accesses governed by the lock and then release. If the same lock is used by some other process at P2, and if the critical section of P1 preceded the critical section of P2. Or in other words, P1's release operation P1-r1. The release operation and P1 happened before. This most be familiar to you from our discussion of Lampert's logical clock. P1-r1 happens before P2-r2, that is the acquire operation that is being done by P2 if this acquire operation for the same lock happened after the release by P1-r1. All we have to ensure is that all the coherence actions prior to this release of the lock by P1 has to be complete before we allow P2 to acquire this lock before we allow P2 to acquire the same lock L. That's the idea of release consistency.</li>
   <li>So we take the synchronization operations that are provided by the system whether it is hardware or software and we label them as either an acquired operation or a release operation. So, it's very straightforward when you think about mutual exclusion lock, acquiring the lock primitive is an acquire operation. And the unlock primitive is a release operation. So if there is a lock primitive and there is a preceding unlock primitive, so we have to ensure that all the coherence actions happen before I do the unlock, so that when this guy gets the lock and accesses the data, the data that he is going to see are going to be data that is consistent with whatever modifications may have been made over here. That's the idea behind the least consistent memory operation.</li>
   <li> Other synchronization operations can also be mapped to acquiring release. If you think about barrier, arriving at a barrier is equivalent to an acquire, and leaving the barrier is equivalent to a release. So, before leaving the barrier, we have to make sure that any changes that we made to shared data structures is reflected through all the other processes through the cache coherence mechanism. Then we can leave the barrier. So, leaving the barrier is a release operation, in the case of barrier synchronization. So, what that means is that, if I do a shared memory access within this group of sections, and that shared memory access would normally result in some coherence actions on the interconnect reaching to the other processes and so on, and if we use the SC memory model, you will block process P1 until that particular memory access is complete with respect to all the processors and the shared memory machine. But if we use the release consistent memory model, we do not have to block P1 in order for coherence actions to be complete to let the processor continue on with its computation. We only have to block a processor at a release point to make sure that any coherent actions that may have been initiated up until this point are all complete before we perform this release operation. That's the key point that I want you to get out of this release consistent memory model. So the release consistent memory model allows exploitation of computation on P1, with communication that may be happening through the coherence mechanism for completing the coherence actions corresponding to the memory accesses that you're making inside the critical section.
</li> 
</ul>


<h2>12. RC Memory Model</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/30.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So now we come back to our original parallel program and the parallel program is making normal data accesses and synchronization accesses. There are different threads running on all these processors. They're all making these normal data accesses and synchronization accesses.</li>
   <li>And if the underlying memory model is an RC memory model, it distinguishes between normal data accesses and synchronization accesses. And it knows that if there are normal read/write data accesses, it doesn't have to do anything in terms of blocking the processes. It may start initiating coherence actions corresponding to these data accesses, but it won't block the processor for coherence actions to be complete until it encounters a synchronization operation, which is of the release category. If a synchronization operation which is a release operation hits this RC memory model, it's going to say, "ah-ha. In that case all the data accesses that I've seen from this guy, I want to make sure that they're all complete globally communicated to all the processors." It's going to ensure that before allowing the synchronization operation to complete. So the coherence action is only when the lock is released.
</li> 
</ul>


<h2>13. Distributed Shared Memory Example</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/31.JPG?raw=true" alt="drawing" width="500"/>
</p>


<ul>
  <li>So let's understand how the RC memory model works with a concrete example. So let's say the programmer's intent is that one thread of his program is going to modify a structure A. And there is another thread that is going to wait for the modification, and then it is going to use the structure A. So this is the programmer's intent. So P2 is going to wait for the modification to use it, and this guy P1 is the guy that is modifying that particular structure A. And these are running different on processors, and therefore we don't know who may be getting to their code first.</li>
   <li>Let's say that P2 executes the code that corresponds to this semantic. That is, it wants to wait for the modification. So in order to do that, it has a flag, and this flag has a semantic with 0 indicating the modification is not done and 1 when the modification is actually done. To make sure that we don't do busy waiting, we use a mutual exclusion lock. We lock a synchronization variable, let's call it L. And if the flag is 0, then it is going to execute the equivalent of a pthread_cond_wait. pthread_cond_wait has the semantic that you're waiting on a condition variable and you're also releasing the lock that is associated with this condition variable. So you execute this pthread wait call, and the semantic you know is that at this point, thread P2 is blocked here, the lock is released, and he's basically waiting for a signal on this condition variable c. Who's going to do that, well, P1 is the guy that is modifying the structure, so it is the responsibility of P1 to signal him (P2). So let's see what happens.</li>
   <li>So P1 is executing the code for modifying the data structure A, and once it is done with all the modification, then it is going to inform P2. So in order to inform P2, what it does is acquires this lock L, and it sets the flag to 1. And the flag is 1 that I inspected over here (In P2) to know that, "oh, the modification is not yet done here, and I'm waiting on this condition variable." So, P1 sets the flag to 1 and signals on the condition variable c. And you know that signaling on the condition variable is going to wake up P2. And, of course, it cannot start executing here until P1 has released the lock, and once the lock has been released, that lock will be acquired implicitly by the operating system on behalf of P2, because that is a semantic of this condition wait here. So when I wake up, I'll go back, and as a defensive mechanism, I'll recheck the flag to ensure that the flag is now not 0 indicating that the modification has been done, so I'm now ready to get out of this critical section. I unlock L, come out of the critical section. Now I can use this modified data structure. So that's the semantic that I wanted, and I got that with this code fragment that I'm showing you here.</li>
   <li>So the important thing is, if you have an RC memory model, then all the modifications that I'm making here that are modifying shared data structures can go on in parallel. With all this waiting that may be going on here, I don't have to block the processor to do every one of these modifications. The only point at which I have to make sure that these modifications have been made globally visible is when I hit the unlock point in my code. So just before I unlock L, I have to make sure that all the read write accesses to shared variables that I've made here in my program have all been taken care of in terms of the coherence actions being communicated to all my peers. Only then, I have to unlock it. So, in other words, this code fragment is giving you pictorially the opportunity for exploiting computation in parallel with communication. If the model was an SC memory model, then for every read-write accesses that are being done in modifying this data structure A, there would have been coherence actions that would have gone on, and those coherence actions, each of them has to complete before you can do the next one and so on. But with the RC memory model, what it is allowing you to do is, you can do the data structure modification you want, and the coherence actions inherent in those modifications may be going on in the background, but you can continue with your computation until you hit this unlock point. At this point, the memory model will ensure that all the coherence actions are complete before releasing the lock, because once the lock is released, this guy's going to get it, and immediately he'll start using the data structure that has been modified by me. So it is important that all the coherence actions be complete prior to unlocking. So that's the intent of the RC memory model. And that's how you can exploit computation going on in parallel with communication if the memory model is an RC memory model.
</li> 
</ul>


<h2>14. Advantage of RC over SC</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/32.JPG?raw=true" alt="drawing" width="500"/>
</p>


<ul>
  <li>So to summarize the advantage of RC over SC, is that, there is no waiting for coherence actions on every memory access. So you can overlap computation with communication. So the expectation is that you will get better performance in a shared memory machine if you use the RC memory model compared to an SC memory model.
</li> 
</ul>


<h2>15. Lazy RC</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/33.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Now I'm going to introduce you to a lazy version of the RC memory model. And it's called LRC, stands for lazy RC. Now this is the structure of your pilot program so a thread acquires a lock, does the data accessing, releases the lock. Another thread may acquire the same lock. And if the critical section for P1 precedes the critical section for P2, then the RC memory model requires that, at the point of release, you ensure that all the modifications that have been made on processor P1, that is, the coherence actions. That are commensurate with those data acceses, are all communicated to all the peer processes including P2. Then you release the lock. That's the semantic. And this is what is called eager release consistency, meaning that at the point of release you're insuring that the whole system is cache coherent. The whole system is cache coherent at the point of release, then you release from the lock. And the cache coherence is with respect to the set of data accesses that have gone on on this process up to this point, that's what we are ensuring has been communicated and made consistent on all the processes.</li>
   <li>Now let's say that the timeline looks like this and P1's release happened at this point. And P2's acquire of the same lock happened at a much later point in time. That's the luck of the draw in terms of how the computation went, and so there is this time window between P1's release of the lock and P2's acquisition of the same lock. Now if you think about it, there's an opportunity for procrastination. Now we saw that procrastination often helps in system design. We've seen this in mutual exclusion locks. If you insert delay between successive trials of trying to get the lock, that actually results in better performance. We saw that in processes scheduling too. Instead of eagerly scheduling the next available task, maybe you want to wait for a task that has more affinity to your processor. That results in performance advantage. So procrastination often is a good friend of system design. So here again there is an opportunity for procrastination.</li>
   <li>So Lazy RC is another instance where procrastination may actually help in optimizing the system performance. The idea is that, rather than performing all the coherence actions at the point of release. Don't do it, procrastinate. Wait till the acquire actually happens. At the point of acquire, take all the coherence actions before allowing this acquire to succeed. </li>
   <li>So the key point is that you're deafening the point at which you ensure that all the coherence actions are complete to the point of acquisition as opposed to the point of release. Even if all the coherence actions commensurate with the data accesses that have gone on up until this release point, are not yet complete when we hit the release, go ahead. Release the lock, but if the next lock acquisition happens, at that point, make sure that all the coherence actions are complete. So in other words, you get an opportunity to overlap computation with communication once again in this window of time between release of a lock, and the acquisition of the same lock.
</li> 
</ul>


<h2>16. Eager vs Lazy RC</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/34.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So the Vanilla RC is what is called the eager release consistent memory model and the new memory model is called LRC or Lazy release consistent memory model. Let's see the pros and cons of LRC with respect to Vanilla RC, or Lazy RC with respect to Eager RC.What I am showing you here are timelines of processor actions on three different processors, P1, P2, and P3. </li> 
  <li>This (top) picture is showing you what happens in the Eager version of the RC model in terms of communication among the processors. So when processor P1 has completed its critical section, does the release operation, at the release point what we're going to do is all the changes that we made, in this example I'm showing you to make it simple, I'm showing you that in this critical section that I wrote in this variable x, so the changes to x is going to be communicated to all the processors P2 and P3. It could be depending on whether it is an invalidation based protocol or an update based protocol, what we are saying is we are communicating the coherence action to all the other processors. That's what these arrows are indicating. Now then P2 acquires the lock, and after it acquires the lock it does its own critical section. Again, let's say we're writing to the same variable x, and it releases the lock. And at the point of release once again we broadcast the changes that we made. Notice what is going on. P1 makes modifications, broadcasts it to everybody. But who really needs it? Well, only P2 needs it. But unfortunately the RC memory model is Eager, and it says "I'm going to tell everybody that has a copy of x that I have modified x." And so it's going to tell it to P2. It's going to tell it to P3 as well. P3 doesn't care, because it's not using that variable yet, and P2 cares, and it of course is using that. But when it releases its critical section, it's once again going to do exactly the same thing that happened over here, and that is it's going to broadcast the changes it made to shared memory locations to all the other processes, in this case P1 and P2 (Actually should be P1 & P3). And then, finally, P3 does its acquire and then reads the variable. So, all these arrows are showing you the coherence actions that are inherent in the completion of shared memory accesses that are happening in the critical section of programs.</li> 
  <li>Now let's move over to the Lazy version. In the Lazy version, what we are doing is when we release a lock, we are not doing any global communication. We simply release a lock. Later on, the next process that happens to acquire that same lock. The RC memory model. The first thing it's going to say is, "oh, you want to get this lock? I have to go and make sure that I complete all the coherence actions that I've associated with that particular lock. In this case the previous lock holder had made changes to the variable x, so I'm going to pull it from this guy and then I can execute my critical section." And then when P3 executes its critical section, it's going to pull it from P2 and complete what it needs to do. So, the important thing that you see is that there is no broadcast anymore. It's only point-to-point communication that's happening between the processors that are passing the lock between one to the other. So, in other words, the number of arrows that you see are communication events. You can see that there's a lot more arrows here. Forget about the arrows that I introduced. But the black arrows that you see are the arrows that are indicating communications commensurate with the coherence actions needed for this set of critical section actions. And correspondingly, the black arrows here are showing the communication actions for the same set of critical section actions shown in both the top and the bottom half of this particular figure.</li> 
  <li>You can see, there's a lot less communication happening with the Lazy model. It's also called a pull model, because what we're doing is at the point of acquisition, we're pulling the coherence actions that need to be completed over here. Whereas, this is the push model in the sense that we're pushing all the coherence actions to everybody at the point of release. Having introduced the Eager and the Lazy RC models, it's time for a quiz.
</li> 
</ul>


<h2>17. Pros and Cons of Lazy and Eager</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/35.JPG?raw=true" alt="drawing" width="500"/>
</p>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/36.JPG?raw=true" alt="drawing" width="500"/>
</p>

<br>
<br>
<p><b>-------------------------------------------------------------------------------</b></p>
<p><b>Note: Regarding lazy & eager RC model</b></p>
<p> Paper: Keleher, P., Cox, A. L., & Zwaenepoel, W. (1992). Lazy release consistency for software distributed shared memory. ACM SIGARCH Computer Architecture News, 20(2), 13-21.</p>
<p>A system is release consistent if: 1) before an ordinary access is allowed to perform with respect to any other processor, all previous acquires must be performed 2)before a release is allowed to perform with respect to any other processor, all previous ordinary reads and writes must be performed 3) special accesses are sequentially consistent with respect to one another </p>
<p>Eager release consistency: A processors delays propagating its modification to shared data untile it comes to a release. At that time, it propagates the modifications to all other processors that cache the modified pages. No consistency related operations occur on an acquire. </p>
<p>Lazy release consistency: The propagation of modifications is further postponed until the time of the acquire. At this time,the acquiring processor determines which modifications it needs to see according to the definition of RC. It is an algorithm for implementing release consistency that lazily pulls modifications across the interconnect only when necessary. The simulation in the paper indicates that lazy release consistency reduces both the number of messages and the amount of data transferred between processors. These reductions are especially significant for programs that exhibit false sharing the make extensive use of locks.</p>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/note1.JPG?raw=true" alt="drawing" width="500"/>
</p>
<br>
<br>

<h2>18. Software DSM</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/37.JPG?raw=true" alt="drawing" width="600"/>
</p>
<ul>
  <li>So far, we've seen three different memory consistency models. One is the sequential consistent memory model, the release consistent memory model. Strictly speaking, I would say, the eager version and the lazy version are just variants of the same memory model, namely the release consistent memory model. Now we're going to transition and talk about software distributed shared memory, and how these memory models come into play in building software distributed shared memory.</li> 
  <li> We're dealing with a computational cluster, that is, in the cluster, each node of the cluster has its own private physical memory, but there is no physically shared memory. And therefore, the system, meaning the system software, has to implement the consistency model to the programmer. In a tightly coupled multiprocessor, coherence is maintained at individual memory access level by the hardware. Unfortunately, that fine-grain of maintaining coherence at individual memory access level will lead to too much overhead in a cluster. Why? Because on every load or store instruction that is happening on any one of these processors, the system software has to butt in and implement the coherence action in software through the entire cluster. And this is simply infeasible. So what do we do to implement software distributed shared memory?</li> 
  <li>The first thought is to implement this sharing and coherence maintenance at the level of pages. So the granularity of coherence maintenance is at the level of a page. Now, even in a simple processor or in a true multiprocessor, the unit of coherence maintenance is not simply a single word that a processor is doing a load or a store on. Because in order to exploit spatial locality, the block size used in caches in processors tend to be bigger than the granularity of memory access that is possible from individual instructions in the processor. So we're taking this up a level and saying, "if you're going to do it all in software, let's keep the granularity of coherence maintenance to be an entire page." And you're going to maintain the coherence of the distributed shared memory in software by cooperating with the operating system that is running on every node of the processor. So what we're going to do is, we're providing a global virtual memory abstraction to the application program running on the cluster. So the application programmer views the entire cluster as a globally shared virtual memory. </li> 
  <li>Under the cover, what the DSM software is doing is partitioning this global address space into chunks that are managed individually on the nodes of the different processors of the cluster. From the application point of view, what this global virtual memory abstraction is giving is address equivalence. And that is, if I access a memory location x in my program, that means exactly the same thing whether I access the memory location x from processor 1, processor 2, and so on and so forth. That's the idea in providing a global virtual memory abstraction.</li> 
  <li>The way the DSM software is going to handle maintenance of coherence is by having distributed ownership for the different virtual pages that constitute this global virtual address space. So you can think of this global virtual address space as constituted by several pages, and we're going to say some number of these pages are owned by processor 1. Some number of these pages are owned by processor 2, some number by processor 3  and so on. So we split the ownership responsibility into individual processors. Now what that means, is that the owner particular page is also responsible for keeping complete coherence information for that particular page and taking the coherence actions commensurate with that page. </li> 
  <li>The local physical memories are available in each one of this processors is being used for hosting portions of the global virtual memory space in the individual processors commensurate with the access pattern that is being displayed by the application on the different processors. So for instance, if processor 1 accesses this portion of the global virtual memory space, then this portion of the address space is mapped into the local physical memory of this processor so that a thread that is running on this processor can access this portion of the global address space. And it might be that same page (in local P1) is being shared with some other processor n over here (in local Pn). In that case, a copy of this page is existing in both this processor (local P1) as well as this processor (local Pn).</li> 
  <li>Now it is up to the processor that is responsible for the ownership of this particular page to worry about the consistency of this page, that is now resident in multiple locations. For instance, if this node (Local Pn) is the owner for this page, then this node (Pn) will have metadata that indicates that this particular page is currently shared by both P1 and Pn. So that is the directory that is associated with the portion of the global virtual memory space that is being owned and managed by this particular processor. So statically, we are making an association between a portion of the address space and the owner for that portion of the address space in terms of coherence maintenance for that portion of the global virtual memory space.
</li> 
</ul>


<h2>19. Software DSM (cont)</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/38.JPG?raw=true" alt="drawing" width="600"/>
</p>

<ul>
  <li>So this is the abstraction layer seen by the application that is giving this illusion of a global virtual memory. This layer is the DSM software implementation layer that implements this global virtual memory abstraction. In particular, this DSM software layer, which exists on every one of these processors, knows that the point of access to a page by a processor who exactly to contact as the owner of the page, to get the current copy of the page.</li> 
  <li>For instance, let's say that there was a page fault on this processor 1 for a particular portion of the global address space. That portion of the global address space is currently not resident in the physical memory of processor 1. So, there is a page fault over here and there is cooperation, as I mentioned earlier, between the operating system and the DSM. So, when the page fault happens, that page fault is going to be communicated by the operating system to the DSM software saying "here is the page fault, you handle it."</li> 
  <li>What the DSM software is going to do is, it knows the owner of the page, and so it's going to contact the owner of the page and ask the owner of the page to get the current copy of the page. So the current copy of the page resides over here (in P3). So the owner, either it itself has the current copy of the page or it knows which node currently has the current copy of the page, is going to send the page over to the node that is requesting it. The current copy of the page is over here (P3) is going to come over to this guy (P1).</li> 
  <li>Recall what I said about ownership of the pages. The residency of this page (in P3) doesn't necessarily mean that this (P3) is the owner of the page. The owner of this page could have been this node (P2), so the DSM software would have contacted the owner. And the owner(P2) would have said, "Oh, you know what, that particular page's current copy is on this node (P3)." So the DSM software would go to this node, and fetch this page, and put it into this processor so that this processor is happy.</li> 
  <li>Once the page has been brought in to the physical memory (P1), then the DSM software contacts the virtual memory manager and says, "I've completed now, processed the page fault, brought the page that is missing and put it into a particular location in the physical memory. Would you please update the page table for this guy, so that he can resume execution?" Then the VM manager gets into action and updates to page table for this thread to indicate that the faulty virtual page is now mapped to a physical page, and then the process or the thread can resume its execution.</li> 
  <li>So this is a way coherence is going to be maintained by the DSM software. The cooperation between DSM software and the VM manager. And the coherence maintenance is happening at the level of individual pages.</li> 
  <li>An early examples of systems that built software DSM include Ivy from Yale, Clouds from Georgia Tech, Mirage from UPenn, and Munin from Rice. All of these are distributed shared memory systems and they all used coherence maintenance at the granularity of an individual page, and they used a protocol which is often referred to as a single writer protocol.</li> 
  <li>Single-writer multiple-reader protocol: I mentioned that the directory associated with the portion of the virtual memory space managed by each one of these nodes and the directory has information as to who all are sharing a page at any point of time. Multiple readers can share a page at any point of time, but a single writer is only allowed to have the page at any point of time. So, if there is the writer for a particular page, let's say that this page which is now currently in the memory of two different processors, if this guy wants to write to this page, then he has to inform through the DSM software abstraction, inform the owner for this page. Let's say this guy is the owner for this page "I want to write to this page." And at that point the owner is going to invalidate all copies of that page that exist in the entire system, so that this guy has exclusive access to that page so that they can make modifications to it. So this is what is called single writer multiple reader protocol. Easy to implement. At the point of right to a page, what you do is, you go through the DSM software, contact the owner. And the owner says, "I know who all have copies of the page.I'll invalidate all of them." Once it has invalidated all the copies, then the guy who wants to write to that page can go ahead and write to it because that'll be the only copy.</li> 
  <li>False sharing: The problem with the single writer protocol is that there is potential for false sharing. We've talked about this already in the context of shared memory multiprocessors. Basically the idea of false sharing or the concept of false sharing is that data appears to be shared even though programmatically they are not. Let's consider this page-based coherence maintenance. In the page-based coherence maintenance, the coherence maintenance that is done by the software, DSM software, is of the granularity of a single page. A page may be 4K bytes or 8K bytes,depending on the architecture we're talking about. And within a page, lots of different data structures can actually fit. So, if the coherence maintenance is being done at the level of an individual page, then we're invalidating copies of the page in several nodes in order to allow one guy to make modifications to some portion of that page. And that can be very severe in a page based system due to the coarse granularity of the coherence information. So for example, this one page may contain ten different data structures, each of which is governed by a distinct lock. So far as the application programmer is concerned. But, even if I get a lock, which is for a particular data structure that happens to be in this page. And this guy has a lock for a different data structure which is also on the same page. When I get the lock that is going to manipulate a particular data structure in this page, and if I want to make modifications for it, I am going to invalidate all the other copies. When he wants to make a change, he's going to come and invalidate my copy of the page. So the page can be ping-ponging between multiple processes. Even though they are modifying different portions of the same page, still the coherence granularity being a page, will result in this page shuttling back and forth between these two guys, even though the application program is perfectly well behaved in terms of using locks to govern access to different data structures. Unfortunately, all of those data structures happened to fit within the same page resulting in this false sharing.</li> 
  <li>So, page-level granularity and single-writer multiple-reader protocol don't live happily together. They will lead to false sharing. And they will lead to ping ponging of the pages due to the false sharing among the threads of the application across the entire network.
</li> 
</ul>

<br>
<br>
<p><b>-------------------------------------------------------------------------------</b></p>
<p><b>Note: Regarding the challenges & protocols</b></p>
<p>Paper: Amza, C., Cox, A. L., Dwarkadas, S., Keleher, P., Lu, H., Rajamony, R., ... & Zwaenepoel, W. (1996). Treadmarks: Shared memory computing on networks of workstations. Computer, 29(2), 18-28.</p>
<p><b>The challenges</b></p>
<p>The provision of memory consistency is at the heart of a DSM system: The DSM software must move data among processors in a manner that provides the illusion of globally shared memory. When a page is not present in the local memory of a processor, a page fault occurs.The DSM software brings an up-to-date copy of that page from its remote location into local memory and restarts the process.IVY furthermore distinguishes read faults from write faults. With read faults, the page is replicated with read-only access for all replicas, while with write faults, all existing copies are invalidated and the writer retains the sole copy. However, the IVY implementation of DSM can cause a large amount of communication to occur. Communication is very expensive on a workstation network. Sending a message may invovle traps into the OS kernel, interrupts, context switches and the execution of possibly several layers of networking software. Therefore, the number of messages must be kept low</p>
<p>Problem1 for IVY: SC model enforced by IVY. Sequential consistency requires writes to shared memory become visible "immediately" to other processors. </p>
<p>Problem2 :False sharing occurs when two or more unrelated data objects are located in the same page and are written concurrently by separate processors,causing the page to ping-pong back and forth between the processors. Since the consistency units are large(VM pages),false sharing is a potentially serious problem. </p>
<br>
<p><b>Multiple-Writer Protocols. All part 5 contents</b></p>
<p>single-writer protocols allow multiple readers to access a given page simulaneously, but a writer is required to have sole access to a page before performing any modification. It's easy to implement because all copies of a given page are always identical and page faults can always be satisfied by retrieving a copy of the page from any other processor that currently has a valid copy. Unfortunately, this simplicity often comes at the expense of message traffic. Before a page can be written, all other copies must be invalidated. These invalidations can then cause subsequent access misses if the processors whose pages have been invalidated are still accessing the page's data. False sharing can cause single-writer protocols to perform even worse because of interference between unrelated accesses. DSM systems typically suffer much more from false sharing than do hardware systems because they track data accesses at the granularity of virtual memory pages instead of cache lines.</p>
<p>Multiple-writer protocols allow multiple writers to simultaneously modify the same page, with consistency traffic deferred until a later time, in particular until synchronization occurs</p>
<p>TreadMarks uses the virtual memory hardware to detect accesses and modifications to shared memory pages. Valid pages are initially write-protected. When a write occurs, TreadMarks creates a copy of the virtual memory page, or a twin, and saves it in system space. When the modifications need to be sent out to another processor, the current copy of the page is compared with the twin on a word-by-word basis and the bytes that vary are saved into the diff data structure. Once the diff has been created, the twin is discarded. With the exception of the first time a processor accesses a page, its copy of the page is updated exclusively by applying diffs; a new complete copy of the page is never needed.</p>
<p>The primary benefit of using diffs is that they can be used to implement multiple-writer protocols, but they can also significantly reduce overall bandwidth requirements because diffs are typically much smaller than a page</p>
<p>How protection faults are used to create diffs below</p>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/note2.JPG?raw=true" alt="drawing" width="500"/>
</p>
<br>
<br>

<h2>20. LRC with Multi Writer Coherence Protocol</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/39.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>That brings us to a new coherence protocol, which is multiple writer protocol. So, the idea is we want to maintain coherence information at the granularity of pages. Because that is the granularity at which the operating system operates, and therefore, the DSM can be integrated with the operating system if the granularity of the coherence maintenance is at the level of a page. But, at the same time, we want to allow multiple writers to be able to write to the same page, recognizing that an application programmer may have packed lots of different data structures within the same page. So we are going to see how the multiple writer coherence protocol works, and in particular we're going to use that in concert with these lazy release consistency.The background for what I'm going to describe is covered in the paper that is assigned for you, which is the Treadmarks paper. I encourage you to read that paper to get all the details. But here, I'm going to give you a high level view of how LRC is integrated with multiple writer protocol in the Treadmarks system. </li> 
  <li>The processor P1 acquires a lock and makes modifications. This notation that I'm using is to indicate that these pages, X, Y, and Z, actually they are data structures that are being modified, but we are maintaining coherence of the level of pages so we'll say that the data structures that we're modifying within this critical section are contained in pages X, Y and Z, and so those are the pages that are being modified within this critical section when processor P1 executes this piece of code. Now the operating system has no knowledge of the association between the lock, L, and the pages that have been modified. All that it knows, is that within the critical section these are the pages that were modified. That's what the operating system knows. And we're going to create a diff of the changes that were made to the pages X, Y and Z in this critical section.</li> 
  <li>So we know at the beginning of this critical section what the contents of the page X, Y and Z is. And at the end of this critical section we're going to find out what is the difference that has been made or what are the changes that have made, and compute the diffs between the original page and the modified page. Xd, Yd and Zd are the diffs to pages X, Y, and Z respectively as a result of executing this critical section.</li> 
  <li>The coherence protocol we are going to use is LRC or lazy release consistency. So the next time the same lock L is requested by some other process of P2, we're going to first invalidate the pages we know were modified by the previous lock holder, because this is information that is available to the DSM that at the point of unlock it knows that these were the pages that were modified by this critical section. It doesn't know what part of the pages are modified. That's contained in the diffs. But it knows that pages X, Y, and Z are associated with this lock L, and therefore when P2 makes the lock request L, the DSM is going to first invalidate. If copies of pages X, Y, and Z are locally resident in the processor P2, then the DSM software is going to invalidate those pages X, Y, and Z at the point of lock acquisition. That's consistent with the lazy release consistency model.</li> 
  <li>So, once we've invalidated these pages, then you can allow this guy (P2) to get the lock and start getting into its critical section. Now once it is in the critical section, it can do whatever it wants. But if it tries to access page X, at that point we know that page is invalid because we've done that at the beginning of this critical section, invalidated page X. And at this point, the DSM software also knows that the previous lock holder has the modifications that need to be made to the original page to get the current version of the page. The current version of the page is with some owner for this page. I mentioned this ownership-based protocol in the DSM software. So the DSM software knows who the owner of the page is from the owner of the page. I can get the original content of X. I'll do that, but I'll also go and get the diff that is created by the execution of the previous critical section by the previous lock holder. So  at the point of access to X, the DSM software brings Xd and the original version of the page from the owner of the page, it can then create the current version of the page for use by P2.
</li> 
</ul>

<h2>21. LRC with Multi Writer Coherence Protocol (cont)</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/40.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Some of you may have thought of this already, and that is, prior to P2 getting its lock, it is possible that maybe another processor, say P3, also used the same lock. And so when it executed its critical section, maybe in its critical section, it modified the page X again, and it creates its own diff, let's call it Xd prime. Because all of these locks are the same, when the DSM software knows that now there are two diffs associated with this lock, L, one diff is with the processor P1, and another diff is residing with processor P3, and therefore when processor P2 tries to access X, the DSM software has not only to get the diff from P1, but it also needs to get the diff from P2 and apply it to the original, pristine version of the page that is with the owner of the page so that it can create the current version of the page. And it can extend this to any number of processors that may have made their own modifications to this page under the provision of the lock L. All of those diffs are going to be applied in order for the process of P2 to access the page as the current page. </li> 
  <li>If after accessing the page X and its execution, P2 touches page Z. at that point once again the diff software knows "oh, I know that Z was modified by the previous lock holder Zd is the diff, I know where to find it I'll bring the original copy of Z from the owner of Z and apply the diffs to it before letting P2 access Z." So you can see, that even though the invalidation was done right at the beginning, we're procrastinating getting the this until the point of access.</li> 
  <li>So this is what LRC allows you to do is just bring in what this guy needs. So for instance, inside this critical section maybe only X is accessed, Y and Z are not accessed at all, in which case we never bring the diffs from P1 to P2 for Y and Z. On the other hand, it is possible that P2 as part of its execution of its critical section modifies another page Q, different from X, Y and Z. So now, the DSM software knows that this particular lock is associated not just with X, Y, and Z, but it is also associated with Q. So future lock request for L will result in invalidating X, Y, Z, and Q because all of those may have been modified and the next critical section that wants to access this lock L has to get the current versions of all the pages that were ever associated with L.
</li> 
</ul>


<h2>22. LRC with Multi Writer Coherence Protocol</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/40.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So let's talk about the Multiple-Writer part of it. Note that it could be multiple user data structures present in a given page X. If that is the case, the programmer probably has different locks for accessing different portions of the data structures that happened to all fit within this page X.</li> 
  <li>So it is conceivable that when all of this was going on, there was another processor, let's say P4, and a thread that is running on the processor P4 got a completely different lock, let's say L2. And it is accessing some data structure that happens to be in the same page X. This is perfectly fine. The DSM software is not going to do anything in terms of the diffs that it has created with respect to the page X because of lock acquisition L. That's completely different set of actions compared to a different lock acquisition, say L2.</li> 
  <li>So if in fact that other thread that is running on P4 executed in parallel with P1, got its lock L2 and modified X. When P2 gets its lock L, the DSM software is going to bring the diff only from the previous users of the same lock L. P4 was not using L, it was using L2 even though it accessed the same page and modifying a different portion of that page. And therefore the DSM software is going to assume that that change made by P4 to X is irrelevant so far as P2's critical section is concerned. So that's the important thing, and that is where the multiple-writer coherence protocol semantic comes in.</li> 
  <li>Simultaneously the same page could be modified by several different threads on several different processors. And that is perfectly fine as long as they're using different locks. So the association between the set of changes to a page is only to specific lock which is being used to govern that critical section and this is the reason why this is called a Multiple-Writer Coherence Protocol. And we saw how this Multiple-Writer Coherence Protocol lives in concert with LRC to reduce the amount of communication that goes on in executing critical sections of an application.
</li> 
</ul>

<h2>23. Implementation</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/41.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>I've always said that the fun part of any systems research is the technical details and implementing an idea. So let's look at some of the implementation details here.</li> 
  <li>When a process or a thread on a processor tries to write to a page X. At the point of writing to that page X, the operating system is going to say "this guy wants to write to this X, I'm going to make a twin for this page." So there's the original page, and then the twin for the same page, and the original page is writeable by this process that mapping is there in the page table. This new copy, a twin, has been created in physical memory. It's not mapped into the page table of any process. It is just additional copy of the same page created by the operating system as a twin. So this page has been made writable and therefore the thread can make changes to the X, which is the original copy of the page. So the thread reaches the release point.</li> 
  <li>When the thread reaches the release point, the DSM software is going to compute the diff between the changes that have been made and the original version. The original version, we created a twin, right? So this is the twin, and this is the original page, but this original page we made modifications to. X has now become X prime, and this is the twin, which is containing the page as it was before The thread started writing to it. So the DSM software at the release point, is going to compute the diff between the original copy of the page, and the modified copy of the page. And the diff is going to be computed as a runlength encoded diff, meaning that all of the page is not been modified. It's only this portion and this portion of the page that have been modified. So the diff is going to be computed as "oh, the page is changed starting from here up until here and starting from here up until here. This is the starting point for the change and this is the amount of change and this is the content of the change." The diff in the data structure that had been created by the DSM software to remember the changes that had been made to this data X prior to release.</li> 
  <li>As you may have imagined already, when the same block that governs accesses to this page which was released over here is acquired by a different processor at the point of acquisition, we're going to invalidate all the pages that were touched in this critical section, including X. So X will be invalidated at the point of acquisition of the same lock that is governing this critical section. And when that processor has a page fault for page X. A DSM software knows that "oh, there is a diff lying around on this node which is needed in order to update the page and give it to the current lock acquirer." So, that's part of what goes on under the covers in the implementation.</li> 
  <li>But so far as this node is concerned when the release operation is done, at that point, the DSM software is going to compute the diff between changes made to this page and its original copy of the page and keep that as a diff data structure. And there are multiple pages, all of the diffs will be created at the point of release. And once this thread that was in this critical section has completed its release operation, we will write protect this page X. We're write protecting it to indicate that this guy cannot write to it anymore unless he gets in the critical section and we have to do the coherence actions again. And that's the implementation of the protocol.</li> 
  <li>And at this point we write-protect the original page and we can also get rid of the twin. The use for this twin is complete. We only needed it in order to compute this diff. We have computed it, and we write-protected the original page, and everything that needs to be done on this node is complete, and then we can get rid of this twin. And getting rid of this twin essentially means that we are freeing up the physical memory that we allocated for creating the twin in the first place.
</li> 
</ul>


<h2>24. Implementation (cont)</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/42.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
   <li>I mentioned earlier it's a multiple writer protocol, which means that this action that's going on can be happening simultaneously for the same page X on different nodes of the cluster. That's perfectly fine so far as the protocol is concerned, because the assumption is that the user has an association between locks. And the data structures governed by the lock. So, even if the same page is being modified, hopefully different portions of the same page has been modified because concurrently if a page is being modified, that means that different locks are protecting the portions of the page that are being modified by the different processes.</li> 
  <li>Now if writes are happening to the same portion of a page under different locks, that's a user's problem. That's a data race. That's not the problem of the DSM software. It's an application problem because it represents a data race that should not have been there if the application is constructed correctly.</li> 
  <li>If the application is constructed correctly and the multiple data structures are hosted in the same page and the data structures are all governed by different locks, DSM software has a way of ensuring that changes made to a critical section under a particular lock is propagated from one processor to the next processor where the first processor is the current owner of the lock, and the next processor is the next user of the lock.</li> 
  <li>So this implementation that I've detailed here is an example of the cooperation between the distributed shared memory software and the operating system to make it all happen. And in particular, TreadMarks implemented this LRC multiple writer coherence protocol on a Unix system.</li> 
  <li>In the Unix system, the operating system generates an exception called SIGSEGV in the operating system layer when a shared page is accessed by a thread. This exception is caught by the TreadMarks' runtime handler. And at that point the DSM software get into gear, contacts the owner of the page, checks the status of the page. And if the page is invalid then it gets the page and the diffs for that page and once it brings in the contents of the page and the diffs it creates a current version of the page. And if the process that is trying to access the page is making a read access, then there is no problem. But if the process that wants to use that page wants to write to it, at that point it creates a twin and does all the things that I just mentioned.</li> 
  <li>So one thing that you will notice is that there is space overhead for the creation of the twin at the point of the write. And then at the point of release, you can get rid of the twin, but you have to create a diff data structure. So, the twin and the diff are all data structures of the implementation of distriuted shared memory.</li> 
  <li>As time goes by, there could be a lot of these diffs that are lying around in different nodes. Imagine that a page was touched by ten different processors. In that case, there are going to be diffs lying around in ten different processors and if eleven processor wants to access the same page the DSM software has to go and bring the diffs from this ten prior users of the page, get the original page from the owner, apply the diffs to create the new page. A lot of latency is involved before the guy who needs the page can start using it. And also there is a lot of space overhead in the fact that all these diffs are lying around.</li> 
  <li>So one of the things that happens in the DSM software is garbage collection. And that is, you keep a watermark of what is the amount of diffs that have been created in the entire system. If it exceeds a threshold then you start applying these diffs to the original copy of the page at the owner, so that you can then get rid of the diffs completely. Why do you need that? The diffs that going to be lying it on till a long time till the next time the pages access by someone, you don't want that. So, what you're trying to do is, you're reducing the space overhead in the DSM implementation by periodically doing this garbage collection. And applying the diffs to the original copy of the page so that he can get rid of it from the system. You don't want to do it too eagerly, but you don't want to wait too long also, because if a page hasn't been accessed for a long time, diffs are going to be lying around for a long time. So there will be a daemon process in every node that every once in a while wakes up and sees how much diffs have been created in my known. If it exceeds the threshold then it says "okay time to get to work. Let me go and apply this diffs to the original copy of the page so that I can get rid of the diffs."
</li> 
</ul>


<h2>25. Non Page Based DSM</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/43.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>In this lesson I've covered distributed shared memory and particularly I've given you a specific example of a distributed shared memory system called Treadmarks that uses lazy release consistency and multiple writer coherence. I just want to leave you with some thoughts about non-page based DSM systems before concluding this lesson.</li> 
  <li>There have been systems that have been built that do not use granularity of a page for coherence maintenance. I mentioned earlier that if you want to maintain granularity not at the page level, then you have to track individual reads and writes that is happening on a thread.</li> 
  <li>So one approach is what is called a library-based approach. Here the idea is that, the programming framework, the programming library, is going to give you a way by which you can annotate shared variables that you're going to use in your program. Whenever you touch a shared variable part of creating the executable is to cause a trap at the point of access to the shared variable so that the DSM software will be contacted, and the DSM software can then take the coherence action at the point of access to that shared variable. So, in this case, there is no operating system support needed because in the binary itself we are making sure that at the point of access we're going to result in a trap that will get us into the trap handler that is part of DSM software so that it can take the coherence actions. And examples of systems that use this mechanism include Shasta, that was done at Digital Equipment Corporation, and Beehive which was done at Georgia Tech. And because we are doing this sharing at the level of variables, you don't have any false sharing which is possible with page based systems and single write or cache coherence protocol. So once the DSM software takes the coherence action, which might include fetching the data that is associated with the variable you are trying to access, then the DSM software can resume this thread that caused this trap in the first place.</li> 
  <li>Another approach to providing shared abstractions is not at the level of memory locations, but at the level of structures that are meaningful for an application. And, this is what is called structured DSM. So, the idea is that there is a programming library which actually provides abstractions that can be manipulated in an application program. And the abstractions can be manipulated using API calls that are part of the language runtime. So when the application makes the API call, at that point, when an application makes those API calls that point, the language runtime gets into gear and says what coherence actions do I need to make in order to satisfy this API call. All of those coherence actions are going to be taken at the point of that API call, and that might include fetching data from a remote node in the cluster. And once the semantics of that API call have been executed by the language runtime, then it's going to resume this thread that made the call in the first place. Again, there is no OS support needed for this. And the structured DSM is a very popular approach that has been used in systems such as Linda, Orca, and Stampede that was done at Georgia Tech, and successors to Stampede called Stampede RT and, and PTS. And in this course later on, we're going to see PTS as an example of a structured DSM system.
</li> 
</ul>


<h2>26. Scalability</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/44.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>DSM is providing the application developer with a programming model on a cluster that is akin to Pthreads on an SMP. It looks and feels like a shared memory threads package. That's good. But what about performance? Will the performance of a multi-threaded app scale up as we increase the number of processors in the cluster? </li> 
  <li>From an application programmer's point of view, your expectation is that, as you add more processors, you're going to get more performance. That's your expectation. And what we are doing is, we're exploiting the parallelism that is available both in the application because it's structured in that way. And in the hardware, in order to get increase performances as you increase the number of processors. But the problem is as you increase the number of processors because things are happening in software, there is increased overhead as well. And this overhead increases as the number of processors, so the actual performance is going to be actually much less than your expectation mitigated by the increasing overhead with the number of processors. And this buildup of overhead with a number of processes happens in a true memory multi-processors. And this is even more true in the case of DSM which is implementing the shared memory extraction in software on a cluster.</li> 
</ul>


<h2>27. DSM and Speedup</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/45.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>So we have the allusion of shared memory, which is implemented by physical memories that is strewn all over the entire cluster, and the hope is that the application that is running on the different nodes of this cluster will actually get speed up with increasing number of processors, but such speed up is not automatic. If the sharing that we're doing, even though DSM gives you the ability to share memory across the network, recall what Chuck Thacker told us "Shared memory scales really well when you don't share memory." So, if the sharing is too fine-grained, then no hope of speed up, especially with DSM systems. Because it is only an illusion of shared memory via software, not even physical shared memory. Even physical shared memory can lead to overheads. So, this illusion through software can result in even more overhead so you've gotta be very careful on how you share and what you share.</li> 
  <li>So the basic principle is that, the computation to communication ratio has to be very high if you want any hope of speed up. So in other words, the critical sections that are execute to modify data structures better be really, really hefty critical structures before somebody else needs to access the same portion of the data. So what does this mean for shared memory codes? Well basically, if the code has a lot of dynamic data structures that are manipulated with pointers, then it can lead to a lot of implicit communication across the local area network. You think you're executing code accessing a pointer, but the pointer happens to be pointing to memory that's in a remote processor. So that implicit access to a data structure pointed to by a pointer in your program, can result in a network communication across the network. Fetching something from here, into your local memory. This is the bane of distributed shared memory. That pointer codes may result in increasing overhead for coherence maintenance, for distributed shared memory in a local area network. So you have to be very careful on how you structure codes that can execute efficiently in a cluster using DSM as the vehicle for programming.</li> 
</ul>


<h2>28. Distributed Shared Memory Conclusion</h2>
<ul>
  <li>In reality, DSM as originally envisioned, that is a threads package for a cluster, is dead. Structured DSM, namely, providing higher level data abstractions for sharing among threads, executing on different nodes of the cluster, is attractive to reduce the programming pain for the developers of distributed applications on a cluster. We will discuss one such system, called persistent temporal streams, as part of a later lesson module.
</li> 
</ul>

# L07c: Distributed File Systems
<h2>1. The First NFS</h2>
<ul>
  <li>All of us routinely use file systems. Quite often, not the one that is on our local machine, be it a desktop or laptop, but one that is on a local area network, connecting us to a workplace or university. NFS, which stands for Network File System. And just like the word xerox is used often as a verb to denote copying, NFS has become a generic name to signify any file system that we access remotely.</li> 
</ul>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/46.JPG?raw=true" alt="drawing" width="500"/>
</p>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/47.JPG?raw=true" alt="drawing" width="500"/>
</p>


<h2>2. NFS</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/48.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Network file systems has evolved overtime, but the idea is still the same. You have clients that are distributed all over the local area network and you have file servers sitting on the local area network and these file servers are central, so far as each client is concerned. Of course, the system administrator may partition these servers and say that there is one server designated for a certain class of users.</li> 
  <li>For instance, if you take a university setting, you might have one server serving all the faculty's needs, and maybe another server serving all the student needs, but so far as a single client is concerned, it is still a centralized view, access to a central server over a local area network. Now, since the disk being electromagnetic is slow, the server will cache the files that it retrieves from the disk in memory, so that it can serve the clients better by serving it out of the file cache that is in memory rather than going to the disk all the time. So this is a typical structure of a network file system.</li> 
  <li>A centralized server, which is the model used in NFS, is a serious source of bottleneck for scalability. A single server has to field the client requests coming from the group of users that it is serving and manage all the data and metadata for all the files that are housed on this particular server. And the data and the metadata of files are persistent data structures, and therefore, the file server has to access these data structures over the I/O bus, which is available for talking to the disk subsystem. So, with a centralized server like this, there is limited bandwidth that's available for the server to get the data and the metadata in and out of the disk. And the file system cache is also limited, because it is confined to the memory space that's available in a given server. So, instead of this centralized view of the file system, can we implement the file system in a distributed manner? What does that mean?
</li> 
</ul>


<h2>3. DFS</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/49.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>The vision with a distributed file server is that there is no central server anymore. Each file is distributed across several servers. What does it mean?</li> 
  <li>To avoid the unscalability of a central server, we want to take a file and distribute it across several different nodes in the local area network. Since the DFS is implemented across all the disks in the network, if a client wants to read or write a file then it actually is contacting all the servers potentially to get the data that is looking for. And which means that since each file is distributed across all of these different servers, the I/O bandwidth that's available cumulatively across all of these servers can be used to serve the needs of every individual client. Also, this allows distributing the management of the metadata that is associated with the files among the server nodes that are available. Furthermore, we have more memories available in all of these servers cumulatively, which means that we have a bigger memory footprint available for implementing a file cache, including all of the server memories plus the memories that may be there in the clients as well. And that's where we can actually go towards cooperative caching among the clients as well.</li> 
  <li>In the extreme, we can treat all the nodes in the cluster, whether we call them s1 or c1, we can look at all of the nodes and say, they're all the same with interchangeable roles as clients or servers. That is, we can actually make this DFS a serverless file system. If we allow the responsibility of managing the files, serving the files, caching the files, equally distributed among all the nodes of the cluster here so the nodes are interchangeable between clients and servers.</li> 
</ul>


<h2>4. Lesson Outline</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/50.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>That brings us to this specific lesson that we're going to cover in this lecture. DFS wants to intelligently use the cluster memory for the efficient management of the metadata associated with the files. And for caching the file content cooperatively among the nodes of the cluster for satisfying future requests for files. In other words, what we would like to do, is since we know that a disk is slow, we would like to avoid going to the disk as much as possible. And retrieve the data from the memory of a peer in the network if in fact that peer has previously accessed the same file. And that's the idea behind cooperative caching of files. But in order to fully appreciate the question that we're asking and the answer that we're going to discuss in this lesson, it is important that you have a very good understanding of file systems. If you don't have it, don't worry. We have supporting lectures that will help you get up to speed, and you can get this knowledge by diving into a good under graduate textbook that covers this material well. And the textbook that is used in the undergraduate systems course CS 2200 at Georgia Tech is a good resource for that.
</li> 
</ul>


<h2>5. Preliminaries (Striping a File to Multiple Disks)</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/51.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So to describe the ideas that are discussed in this distributed file system lesson, I have to introduce you to a lot of background technologies. The first background technology that I'm going to introduce you to is what is called RAID storage. RAID stands for redundant array of inexpensive disks. The idea is a given disk may have a certain I/O bandwidth available. Now, if I can string together a number of disks in parallel, then cumulatively I can get much more I/O bandwidth coming out of all of these disks. That's the idea of the RAID technology, which is redundant array of inexpensive disks.</li> 
  <li>Since we have an array of disks, we are also increasing the probability of failures. And that is why in the RAID technology they also use an error correcting technology associated with RAID. And the basic idea is that when you write a file, you're going to do the following: You take a file, let's say, the file has four parts to it. What I'm going to do, is when I write this file, I'm going to write part one to this disk 1, part two to this disk 2, three to this disk 3 and four to this disk 4. Because my data is on multiple disks, I'm also increasing the chance that there may be failures that can hurt me. And therefore, what we do is we compute a checksum for this particular data that I've stored on these disks and store that in a fifth disk. And this is what is called Error Correcting Code. So an Error Correcting Code allows errors to be detected when I read things from the disk and I say, "oh, something is wrong I can correct it using this extra information that I'm writing on the fifth disk here."</li> 
  <li>So that's the big picture of how striping a file to multiple disks works. Basically, what we do is we take a file and decide that if it is going to be striped over four disks, we stripe it on the four disks and we also write an error correcting code for the data that we have striped across these disks. So that if in fact there is an error that manifests itself in the future, with anyone of these disks that error can be corrected using this error correcting data that we have written to augment the original data. So failure protection is being achieved through this error correction code. That's the idea of striping a file in the RAID system.</li> 
  <li>The drawback in the RAID technology is first of all the cost. The fact that we have to have multiple hardware drives in order to store a single file. And the second problem is what is called a small write problem and that is if my file is really really small, then I'm saying that a part of this file is going to be written on each one of these disks, and that's inefficient in terms of how you store data. And the reason why it is inefficient of course is the fact if it is a small file and I've striped it across multiple disks. In order to read this small file, I have to get data from all of these disks, and that's inefficient. And that's the thing that is detrimental about the hardware RAID in terms of handling a normal file system that may have a population of small files and large files and so on. But the idea of having an array of disks to serve the file system is a good one, because it increases the overall I/O bandwidth that's available in the server. So, how can we solve the small write problem?</li> 
</ul>


<h2>6. Preliminaries (Log Structured File System)</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/52.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>That brings me to another background technology that I have to explain to you and which is called log structured file system. The idea here is that when I make a change to file y meaning I either append to the file or make some modifications to it. What I'm going to do is rather than writing the file as is, I'm going to write the change that I made to the file as a log record. So, I have a log record that says, what are the changes I made to this file x. Similarly, I have a log record of all the changes I made to this file y. And this is being done in a data structure which I'll call log segment. And I'll keep this log segment data structure in memory, of course, to make it fast in terms of the file system operation.</li> 
  <li>With this log segment data structure, what I can do is, buffer the changes to multiple files in one contiguous log segment data structure. So this log segment data structure, I can write it out as a file, and when I write it out, I'm not writing a single file, but I'm actually writing a log segment which contains all the changes made to multiple files. And because the log segment is contiguous, I can write it sequentially on the disk and sequential writes are good in the disk subsystem. And what we want to do is, we want to gather these changes to files that are happening in my system in the log segment in memory, and every once in a while, flush the log segment to disk, once the log segment fills up to a certain extent or periodically. And the reason, of course, is the fact that if it is in memory, you have to worry about reliability of your file system, if, in fact, the node crashes. And therefore, what we want to do is, we want to either write out these log segments periodically or when a lot of file activity is happening and the log segment fills up very rapidly. After it passes of threshold, then you write it out to the desk. So in other words, we use a space metric or a time metric to figure out when to flush the changes from the log segment into the disk. And this solves the small write problem because if y happens to be a small file. No problem. Because we are not writing y as-is on to the disk. But what we are writing is this log segment that contains changes that have been made to y in addition to changes that have been made to a number of other files. And therefore, this log segment is going to be a big file. And therefore, we can use the RAID technology to stripe the log segments across multiple disks and give the benefit of the parallel I/O that's possible with the RAID technology. So this log structured file system solves the small write problem.</li> 
  <li>In log structured file system, there are only logs. No data files. You'll never write any data files. All the things that you're writing are these append only logs to the disk. And when you have a read of a file, the read of a file, if it has to go to the disk and fetch that file, then the file system has to reconstruct the file from the logs that it has stored on the disk. Of course, once it comes into the memory of the server, then in the file cache the file is going to remain as a file. But, if at any point, the server has to fetch the file from the disk, it's actually fetching the log segments. And then reconstructing the file from the log segments. That's important. Which means that, in a log structured file system, there could be latency associated with reading a file for the first time from the disk. Of course, once it is read from the disk and reconstructed, it is in memory. In the file cache of the server, then everything is fine. But the first time, you have to read it from the disk, it's going to take some time because you have to read all these log segments and reconstruct it and that's where parallel RAID technology can be very helpful, because you're aggregating all the bandwidth that's available for reading the log segments from multiple disks at the same time.</li> 
  <li>The other thing that you have to worry about, when you have a log structured file system, is that these logs represent changes that have been made to the files. So, for instance, I may have written a particular block of y and that may be the change sitting here. Next time, what I'm doing is perhaps I'm writing the same block of the file. In which case, the first strike that I did, that is invalid. I have got a new write of that same block. So, you see that over time, the logs are going to have lots of holes created by overwriting the same block of a particular file. So in a log structured file system, one of the things that has to happen is that the logs have to be cleaned periodically to ensure that the disk is not cluttered with wasted logs that have empty holes in them because of old writes to parts of a file that are no longer relevant. Because those parts of the file have been rewritten, overwritten by subsequent writes to the same file.</li> 
  <li>So logs, as I've introduced you, is similar to the disks that you've seen in the DSM system with the multiple writer protocol that we talked about in a previous lecture. You may have also heard the term, journalling file system, there is a difference between log structured file system, and journalling file system. Journalling file systems has both log files as well as data files, and what a journalling file system does, is it applies the log files to the data files and discards the log files. The goal is similar in a journaling file system, and the goal is to solve the small write problem, but in a journaling file system, the logs are there only for a short duration of time before the logs are committed to the data files themselves. Whereas in a log structured file system, you don't have data files at all, all that you have are log files and reads have to deconstruct the data from the log files.
</li> 
</ul>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/oh1.JPG?raw=true" alt="drawing" width="800"/>
</p>


<h2>7. Preliminaries Software (RAID)</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/53.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>The next background technology that I'm telling about is software RAID. I mentioned that hardware RAID has two problems. The first problem being small writes and we said we can get rid of the small write problem by using log structure file systems. But the hardware RAID also has another problem and that is it is employing multiple hardware drives. And hardware RAID, generally speaking, is very expensive proposition.</li> 
  <li>On the other hand, in a local area network, we have lots of compute power distributed over the LAN and every node on the LAN has associated with it are disks. So could we not use the disks that are available on local data network for doing exactly the same thing that we did with hardware RAID? And that is, stripe a file across the disks of all the nodes that are in the local area network. So that's the idea behind the Zebra file system that was built at UC Berkeley, which was the first one to experiment with this software RAID technology. It combines both log structure file system and the RAID technology, log structure file system in order to get rid of the small write problem, and the RAID technology to get the parallelism you want in a file server to be able to read from the disks in parallel, so that you can reduce the latency for serving client requests.</li> 
  <li>The idea here is that you're going to use commodity hardware available as nodes connected to disks on a local area network. And since LFS log structured file system is good for getting rid of the small write problem, we are going to employ LFS as the technology for the file server. So in this case, what we have are not data files but we have log segments that have to be written out to the disk in an LFS. We're going to stripe the log segment on multiple nodes of the disks in software and that is the RAID technology. So if this is a log segment that represents the changes made to several different files on a particular client node, then the software RAID, what it will do, is then take this log segment and stripe it. Part one of the log segment on this node, part two on this, part three on this, part four on this and the ECC that corresponds to these four parts of the log segments into a fifth drive. That's the idea in software RAID. Exactly similar to the hardware RAID except software is doing this striping of the log segment on multiple nodes available in a local area network.
</li> 
</ul>


<h2>8. Putting Them All Together Plus More</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/54.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Now it's time to put together the background technology that I introduced to you, plus some more, and describe to you a particular distributor file system called xFS which is also built at UC Berkeley. xFS builds on the shoulders of prior technologies. The first one, the log based striping that I just mentioned to you from the Zebra file system. And another technology called co-operative caching which is also a prior UC Berkley project. And in addition to these two technologies, xFS also has introduced several new nuances in order to make the distributor file system truly scalable and get towards what is called serverlessness, or in other words, no reliance on a central server. And those techniques include dynamic management of data and metadata. Subsetting of the storage servers, and we'll talk about all of these techniques in much more detail in the rest of this lecture.</li> 
</ul>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/oh2.JPG?raw=true" alt="drawing" width="600"/>
</p>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/oh3.JPG?raw=true" alt="drawing" width="600"/>
</p>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/oh4.JPG?raw=true" alt="drawing" width="600"/>
</p>
<h2>9. Dynamic Management</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/55.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>To motivate the need for dynamic management of data and metadata, it's useful to look at the structure of a traditional NFS server which is centralized. In a traditional centralized NFS server what you have is the data blocks are residing on the disks. So in the memory of the server, the contents include the metadata for the files like the i-node structures. And file cache which is files that have been brought in from the disk are stored in the memory in what is called the file cache. So that future requests for the same files can be served from the memory of the server rather than going to the disk. And the server also keeps the client caching directory. That is, who on the local area network are currently accessing files that is the propriety of this particular server. And in a Unix file system the server is unconcerned about the semantics of file sharing. In other words, the assumption is that the server is caching for each client completely independently. And therefore if clients happened to share a file that is completely the problem of the clients, and the server is not concerned about that. </li> 
  <li>All of these contents that I just described to you, metadata, file cache, client caching directory. All of these are in the memory of a particular server. And if the server happens to be housing hot files used by a lot of users that is being served by this particular server. Then that's bad news for the server in terms of scalability, because it has to worry about the requests simultaneously coming from lots of clients for these hot files. And so it is constrained by the bandwidth that's available to access the files from the disk. It is constrained by the amount of memory space it's got, for caching files, and the metadata of the files, and so on. At the same time, there could be another server that also has adequate bandwidth to the storage, and memory space. But unfortunately it may be housing cold files. And therefore, there are not many clients for this server. So you can immediately see that the sort of centralization of the traditional file system results in hot spots. And that's the thing that we're trying to avoid in a distributed file system, and that's where dynamic management comes into play.</li> 
  <li>In xFS, it provides the same functionality as a centralized NFS Server, but it is distributed and the metadata management is dynamic. And that is, in a centralized file server, the mapping between the manager node for a file and the location of the file is the same. Or in other words, if the file happens to reside on the disk of this server, then this server is the guy that is going to handle the metadata management for this file as well. On the other hand, in xFS, metadata management is dynamically distributed. So, let's say that you have F1, F2, and F3 are the hot files. In that case, metadata management for files F2 and F3 can be done by some other node, say S3. And this server may have the cache for the file. So, in other words, all the data structures that we've talked about that has to reside in the memory of a particular server like metadata, file cache, and the caching information about who's having the files and so on. All of that can be distributed with dynamic management of data and metadata, which is the idea in xFS. And I'll shortly explain how exactly this dynamic management is facilitated by the implementation of xFS. So, in any systems research, there is always first the idea and then there's implementation. So the idea in xFS is that we want to manage the data and metadata management dynamically, and we'll see how that is done. And also, what we want to do is we don't want the cache for the files to be only at the server. What we would like to be able to do is, if a file is accessed by several different nodes, then they're living in the client caches of the different nodes. If a file is residing in the cache of a peer node, then it makes sense that if a new request comes from the same file, then getting that file from a peer cache may be much more efficient than getting it from the disk. And that way we can also conserve the total amount of memory that's available on the servers and use it more frugally by exploiting the memories that are available in the clients, so that the caching of the files can be done cooperatively among the clients. And that's the other nugget in the technical contribution of xFS is the cooperative client caching.
</li>  
</ul>


<h2>10. Log Based Striping and Stripe Groups</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/56.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>I mention that xFS uses log based striping in software. So let's understand that technology in a little bit more detail. You have clients on the local area network that are writing to files. When a client makes a change to a file. The changes that are made by this client to files are written to an append only log. And this append only log is a data structure. This log segment is a data structure in the memory of the client in the distributed file system and this append only data structure contains the changes made to files on this client node. And for instance, this could be a change for a particular file X, this could be a change for another file Z and so on. And this is an append only data structure that is residing at this client. Similarly, there is a log segment that is available at this client. For the changes made to files on this node, and when this log segment fills up beyond a certain capacity, you decide on a certain threshold, and once that threshold number of fragments have been written in this log segment, then you decide that it's time now to write to the disk. So you take these log fragments, and compute the parity, which is the check sum or ECC, whatever you may want to call it, and this becomes the log segment that I want to write to the disk. And you take this and stripe it across storage servers. So you take this, particular log fragment, along with its ECC and stripe it on storage servers so that it is now available on the storage system and as I mentioned, you want to do this periodically in order to avoid the chance of data loss due to failures of a particular node, and that's what you're doing every periodically. Same thing is happening on this so if you look at the storage server, you see that the storage server has the log segments that have been written by this client, and the log segment that has been written by the log segment that has been written by this client. All of this, gets into the storage servers, and the other thing that you want to do is, you don't want every log segment to be written on all the disks available on the local area network, you don't want to do that. This again is concerned with solving the small rate problem, and therefore, what we want to do is subset the storage servers, and say if, I have 100 storage servers available on the local area network. I might decide that every log segment is going to write its log over a small fraction of that maybe 10 servers. And this client may similarly choose a subset of storage servers to write it on. The subset of storage servers that is used for striping a given log segment is called a stripe group for that particular log segment.
</li> 
</ul>

<h2>11. Stripe Group</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/57.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>So, using the stripe group for a log segment avoids first of all the small write pitfall. So for instance here, we might decide for certain log segments X, Y, and Z the stripe group is this. And for another set of log segments, say P, Q, and E the stripe group is this. And for another set of log segments, L, M, and N, the stripe group is this. So we're subsetting the servers into these stripe groups, and what that allows is parallel client activities. If the log segments X,Y, and Z, belong to a particular client. And if the log segments P, Q, and R belong to a different client, and L, M, and N, belong to a different client then you can see that the client activity corresponding to this particular stripe group can go on and parallel with this. And similarly this activity can go on and parallel with other stripe groups that exist in the system and so on. And so it increases the availability of the server in saying that not all the servers has to be working on the same client request. Different subset of servers are working on different client requests and that results in higher throughput for overall client processing.</li> 
  <li>When it comes to cleaning the logs, remember that I mentioned earlier that every once in a while, you have to clean the logs because the logs may have been overwritten by new writes to files on a particular client machine, in which case there are logs that Have to be recycled. And if you don't recycle them, you're filling up the disks with junk. And therefore, every once in a while, we have to go and clean up the log. And so efficient log cleaning is again facilitated by the fact that you have different stripe groups, so you can assign different cleaning service for different stripe groups that increases the parallelism in the management of all the things that need to be done in a distributed file system. An increased availability also means that you can survive multiple server failures. So let's say that these two disks fail for some reason. You can still serve the clients who are being served by this particular strip group. That's the idea of sub-setting the server group for striping so that you increase the availability and allow incremental satisfaction of the user community in spite of failures that may be happening in the system as a whole.</li> 
</ul>


<h2>12. Cooperative Caching</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/58.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>Next lets talk about how xFS uses the memory's available in the clients for cooperatively caching the files and reducing the stress on the management of data files. In xFS as supposed to traditional Unix file system, they also worry about the coherence of files. I mentioned earlier that in Unix file system, the file server assumes that it is serving each client independently, and therefore it doesn't worry about sharing a file, if a particular file happens to be access by multiple users at the same time, the server doesn't worry about the coherence of that file. But on the other hand in xFS the file system worries about cache coherence and I have already introduced the idea of cache coherence in the context of multiprocesses and distributed shared memory. So you are familiar with the terminology single writer multiple reader meaning that a particular file at any point of time we have only a single writer. There cannot be multiple writers to the same file, but it can have multiple readers at the same time. And the unit of cache coherence that xFS maintains is at the level of file blocks, not an entire file. But at the level of individual file blocks. So if you look at a manager for a file, the guy that is responsible for the metadata management for the file. It has information about the files for which it is the manager. Let's say it has a file f1 for which it has a manager. Then the metadata in the memory of this manager will have information about the current state of that file. For instance, this particular entry says that a file f1 managed by this manager is being read concurrently by two different clients, c1 and c2. So there are two different clients, c1 and c2, and they have copies of this file that they have retrieved from this manager at some point of time, which means that the client caches, c1 and c2, contain the contents of this file. Now for simplicity, I'm showing this as a file, but in fact the granularity at which the coherence and information about files is kept is at a file block level. So at a block level the manager says a particular block of a file is in the cache of client c1 and in the cache of client c2. And yet I'm using the word cache to mean that it is in the memory of these clients. So the semantics that is observed for cache coherence, a single writer multiple readers. So if client c3 makes a request to the manager for writing to this file f1, and again, I have to mentioned that, the request is going to be at the granularity of a file block, but for simplicity, I'm showing it as a write request for this file f1. But you understand that the granularity at which this request is being made is for writing a particular block of that file. So the manager gets this write request. It looks up the metadata for that particular file. And it sees that this file is now currently read-shared by two different clients, c1 and c2. This guy wants to write to the file. That results in a conflict, a read/write conflict, and what the manager is going to do is basically say, well, if somebody wants to write to that file I have to tell the guys that currently have the file. They cannot have it anymore. So just as in the case of cache coherence in a multiprocessor. This manage is going to send an invalidation message for the file F1 to c1 and to c2, and they are going to acknowledge to the manager saying "yes, we have invalidated our local copies of the files." and once the manager gets that indication back from the clients, at that point the manager can tell the client c3 that Okay, now you have got dibs on writing to this file. That's the protocol that is being observed in xFS.</li> 
  <li>To keep the copies of the files consistent so at the end of this exchange c3 will have the right to write to this particular file. Now how long does it have that privilege? Well the write request when it is granted the client gets the token And the manager, at any point of time, can revoke the token that was given to c3. And this in particular will happen when a future read for the same file comes to the manager. At that point, the manager will go to v and say, "I'm revoking the token from you. You cannot write to that file anymore. You can read it because the request that I got is only a read request and therefore you can keep the file, but you cannot write to it anymore. If you want to write it again, then you have to make a request again." This is the protocol that is observed. And of course, if a particular client is writing to a file, and another client also wants to write to the same file, at that point the manager once again is going to invoke the token, invalidate the file at this client, pull back the contents of the file, and then distribute it to a future requester who wants to write to the same file. That's how cache coherence works. And using the fact that copies of the file is existing in multiple clients excepts as exploits that fact to do cooperative caching. What that means is that if a client is currently having a copy of the file, lets say, after this interchange, c3 has a copy of the file that is also writing to. A future read request comes. When it comes, that read request can be satisfied by getting the contents of the file from the cache of c3. And that is what cooperative caching is all about, where instead of going to the disk to retrieve the file, we can actual get the file content from the cache of one of the clients that happens to have a copy of the file.
</li> 
</ul>


<h2>13. Log Cleaning</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/59.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>I mentioned log cleaning that has to be done. Now as client activities go on, log segments evolve on the disk. So for instance, if on a particular client node some blocks were written to the log segment, a log segment may fill up like this. And now it is sitting on the disk. So the blocks that are containing this log segment corresponds to write to file blocks 1, 2, and 5. And these file blocks may belong to different files, but it is okay. So, so far as the file system is concerned, segment1 is a contiguous file. It is a log segment, but it is a file. And that's the one that is residing on the disk.</li> 
  <li>On the client node, this particular block1 may get overwritten due to activity on the client. So now we have a new content for that same file block1, one double prime. And once this new content has been created, this is no longer valid, and so block1 is overwritten, which means we have to kill the old block that was in this segment. So there's a new segment in which we wrote the contents of the new file block, one double prime, and we have to go back to this old segment and kill this particular copy of that block, which is a stale copy of that same block. So we don't need that anymore. So you can that see as client activities progress we are going to create holes in the segments. Remember that these segments are persistent data structures on the disk. This segment was valid at some point of time. But once that particular block one was overwritten on the client node, this segment contains the latest and the current copy of that same file block. And therefore we nuke this particular file block and so we create hole in this log segment. And subsequently, let's say that the client writes to other file blocks 3 and 4. So the log segment2 contains one double prime, three prime, and four prime are the blocks that it contains. And segment1 contains two prime and five prime. One prime is not relevant anymore because it has been overwritten by one double prime. Activities continue on the client box and a third segment is created. And that third segment contains two double prime. That is, this block number two is overwritten by new contents, two double prime. And when this happens the file system has to create another hole by nuking this two prime to indicate that this block is not relevant anymore, because there is a more recent version of the block in segment three. So the block two is overwritten, killing the old block.</li> 
  <li>Now you see that as client activities progress in the entire distributed system, we are creating a number of these log segments on the disk, and these log segments may progressively have holes in them because they have been overwritten by new contents, by activities on the client machine. And this is what log cleaning is all about. It has to do with cleaning up the disk and getting rid of all of this junk so that we don't fill up the disk with unnecessary junk.</li> 
  <li>So for example, what we want to do is recognize that we have three segments here, and the segments have holes in them. And what we are going to do is, we're going to aggregate all the live blocks from all of these segments in to a new segment. So we've got five from this segment that is still alive, and from this segment we've got one double prime, three prime, and four prime that are still alive. And from this segment we've got two double prime that is still alive. So we have coalesced all of the live blocks from the existing segments into one new segment. Now once we have aggregated this into one new segment, all of the old log segments can be garbage collected and that's what log cleaning is all about. And this is very similar if you think about it to the way we described cleaning up the diff files that are created in the DSM system, Treadmarks. And the same thing is happening except that these data structures are on the disk we are conserving the space on the disk by getting rid of all the old log segments and garbage collecting them and saving this space once we've aggregated all the live blocks in these segments into a new segment. So this is what log cleaning is all about and in a distributed file system, there is a lot of garbage that is being created all across the storage service. And we don't want this to be done by a single manager. We would ideally like it to be done in a distributed manner. This is another step towards a true distributed file system by making this log cleaning activity also a distributed activity. So log cleaner's responsibilities include the following. It has to find the utilization status of the old log segments. Then it has to pick some set of log segments to clean, and once it picks a certain number of log segments to clean, it has to read all the live blocks that it finds in these log segments that it has chosen for cleaning, write it into a new log segment. And once it has done that, it can garbage collect all of those log segments. So this is the cleaning activity that an LFS cleaner has to do and in xFS they distribute this log cleaning activity as well.</li> 
  <li>Now remember that this log cleaning activity is happening concurrently with writing to files on the nodes in the distributed system. So there are lots of subtle issues involved in managing this log cleaning in parallel with new activity that may be creating new log segments, or writing to existing log segments. I encourage you to read the paper that I've assigned to you, the XSF paper, to get a good feel for all the subtleties that are involved in managing log cleaning concurrently with writing to the files. So in xFS they may make the clients also responsible for log cleaning. There is no separation between client and server. Any node can be a client or a server depending on what it is doing and each client, meaning a node that is generating a log segments, it is responsible for the segment utilization information for the files that they are writing. After all the activity is happening at the client end in terms of creating new files, and new file writes are manifesting as creating blocks and log segments in xFS. And so the clients are responsible for knowing the utilization of the segments that are resident at that client node. And since we have divvied up the entire space of servers into stripe groups, each stripe group is responsible for cleaning activity that is in that set of servers. And every stripe group has a leader, and the leader in that stripe group is responsible for assigning cleaning services to the members of that stripe group. Recall that the manager is responsible for the integrity of the files because it is doing metadata management. And, requests for reading and writing files are going to come to the manager. On the other hand, the log cleaning responsibility is going to the leader of a stripe group, and the manager is the one that is responsible for resolving conflicts that may arise between client updates that want to change some log segments and cleaner functions that want to garbage collect some log segments. Those conflicts are resolved by the manager. These again are subtle details which I want you to read carefully in the paper.
</li> 
</ul>


<h2>14. Unix File System</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/60.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Next let's talk a little bit about the implementation details of xFS. First of all, in any Unix file system there are these i-node data structures which give you a mapping between the file name and the data blocks on the disk. So given a file name and the offset that you want to get into that file, the file system has a way of looking up a data section called i-node and deciding where exactly on the disk of the data blocks that you're looking for. This is happening in any Unix file system.</li> 
</ul>


<br>
<p><b>-------------------------------------------------------------------------------</b></p>
<p><b>Note: about inode. From CS6200</b></p>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/note3.JPG?raw=true" alt="drawing" width="500"/>
</p>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/note4.JPG?raw=true" alt="drawing" width="500"/>
</p>

<h2>15. xFS Data Structures</h2>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/61.JPG?raw=true" alt="drawing" width="500"/>
</p>


<ul>
  <li>xFS data structures for implementing a truly distributed file system are much more involved. Let's talk little bit about that, first of all, I mentioned that the metadata management is not static. Even though, the file that you are looking for may be resident in particular node, the manager for that file may not be at that same node. The client action when it is looking for a file, it starts with a file name and this is a data structure that is a replicated data structure at every node in the entire distributed system. So any client node, when it starts with a file name. It consults this manager map data structure to know who's the metadata manager for this particular file name. And the manger node action is fairly involved. So, the client comes to the manager with a file name. And when you come to the manager with a file name, the manager looks up the first data structure, called the file directory. And that file directly has the i-node number. And that i-node number is the starting point for looking up the contents of that file. Now lets talk about all the data structures that are used by the manager node. On the manager node, when the client presents the file name, the manager node uses a data structure called a file directory. To map that file name to an i-number. And from the i-number, it uses another data structure called i-map data structure to get the i-note address for this particular file name. The i-node address is the i-node address for the log segment associated with this file name. And using this strip group map, which is telling. How this particular file is striped. It can locate the storage server that contains the log segment ID, that is associated with this file name. I mentioned earlier, that every log segment is actually striped on a whole bunch of disks, a stripe group. What of the stripe group associated with this particular log segment? That's the information that it gets from the stripe group map. Once it has the set of storage servers that contain this log segment, it can go to the set of storage servers to get the data blocks associated with this particular file name. That's the entire road map of what the manager will have to do. To go from the file name to actual data blocks that corresponds to that file name. Now this sounds like a lot of work being done, fortunately caching helps in making sure that this long path is not taken for every file access. We will see that, in terms of how reads and writes happen. In the xFS file system. Just to recap the data structures, file name to i-number mapping is contained in this data structure, FileDir. The mapping between the i-number and the i-node address for the log segment ID, that is contained in this i-map. Given the i-node address, I can consult the stripe group map to know which storage server actually has the i-node for this file name that is a large segment that corresponds to this file name. I can get that. And once I get that, then I can find out the stripe group that is associated with this log segment, and that'll say what are all the storage servers that I have to contact in order to get the contents of that log segment. Then I can go to those storage servers and get all the data blocks that correspond to a particular log segment. So that's the road map.
</li> 
</ul>
<br>
<p><b>-------------------------------------------------------------------------------</b></p>
<p><b>OH Note: Mmap data structure contains i-number to manager node mapping is determined statically. We hope it can be determined dynamically but it is not</b></p>

<h2>16. Client Reading a File Own Cache</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/62.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>As I said, fortunately, every file read is not going to result in going through so many hoops to get the data blocks. This is where caches come into play. So when you start with a filename and an offset on a client node, you look up the directory. And from that, you get an index and an offset. So, this is a data structure which is in the client memory, and once you get the index in the offset, and if this file has been accessed before, it's most likely in its own unique cache. This is a file cache of the file system, and if it is in your cache, then you get the data block. In other words, going from the file name to the data block that is associated with the file is all happening through the client memory because of local caching. And there's a fastest path for file access and hopefully there's a common case.</li> 

</ul>
<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/63.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li>Now it is possible that a file is shared. Or the same file is being read by different clients at different points in time, but in either case, there is a possibility that a particular file has been accessed by a client and therefore in the cache of that client, and so the next possibility is that you start with a directory You don't find it in you local cache. And if you don't find it in your local cache, then you have to go to the manager node in order to get a copy of the file. So this is where the manager map data structure, which is a replicated data structure, is available in the memory of every client and so given the index number and the offset. I can go and look up the manager map data structure and that tells me who's the manager that I have to contact to get this particular file. So this might involve a network cop because the manager node is different from the local node. Then I have to go to the manager node across a network. And when I get to the manager node, the manager node may say, oh you know what, this particular file, has been accessed by a different client. I know that because my metadata says that some of the client has got this, in their cache. So, the manager will tell the client that currently is holding a copy of the file in its cache. To please send the data over to the first guy that requested it. Now, the data that I requested is coming not from my local cache, but it is coming from the cache of a peer. And that's much better than going to the disk and putting it out of the disk, because network speeds are much faster than accessing the disk. So this is the second best path for file access. Sure, there is network communication involved here because, the first thing I that have to do is I have to hop over the network to get to the manager node if in fact I am not the manager node myself for this particular file that I'm looking for. That is a network hop. And there could be another network hop if the manager says, oh, this particular file is cached in a different node. In that case, there is another network hop to go to the client that currently containing that particular file. And once we get to that, there may be another network hop in order to send the data over to the requester. So potentially there could be three network hops in order to get The file that I'm looking for, but it could be less than that depending on the core location of the manager and the node that is requesting it or the manager and the node that contains a copy of that file itself. So this is the second best path for file access. </li> 
</ul>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/64.JPG?raw=true" alt="drawing" width="500"/>
</p>
<ul>
  <li> But there is also the pathologically 'real long way' of accessing a particular file, and that path is shown here, using all of the data structures that I mentioned earlier that is available at the manager. You start with a file name, look up the directory, get the index number and the offset. It's not in your cache. You go to the manager by looking up the manager map data structure that's in my local memory. I go the manager and the manager then looks up its metadata for this file, finds that nobody has it in the cache So it has to pull it from the desk. If it has to pull it from the desk, then it has to look up its imap data structure, the imap data structure, and the stripe group map data structure in order to find out the location of the I node that corresponds to the log segment that I am looking for. That look up happens through the imap data structure, stripe group map data structure, then I can to the storage server and get the index node of the log segment ID for the requested data block of this client. Once I have that, then the manager has to look up the stripe group map to see what storage servers. Have this log segment striped, and out of that which storage server should I contact for the particular portion of the file that you're looking for? And that storage server will be contacted and that storage server is going to give the data block that is requested by the client. So you can see that in this long path, there is network hop as well as accessing the storage servers to pull the data blocks. It is possible that the index node for the log segment ID associated with this file has been previously accessed by this manager. In which case, it doesn't have to go to the storage server to get the Index node for the large segment, because it'll be present in the memory of the manager as part of its caching strategy. And therefore it can bypass these two network hops because directly from this strip group map it can figure out what the long segment ID is locally cached so that it can then go to this stripe group map data structure and figure out. Where on the disk the data blocks for that log segment is actually secured, so we might be able to get rid of at least two of these network hops if you're lucky and this particular log segment has been accessed before by this manager, it'll be in the memory of the manager. And therefore we can avoid these two network hubs. But the worst case scenario, if this particular log segment is never being accessed before then the long way to get to the data block that you're requesting is going through the data structures in the manager, network hub, storage look up, and get the data and give it to the client.
</li> 
</ul>


<h2>17. Client Writing a File</h2>

<p align="center">
   <img src="https://github.com/audrey617/CS6210-Advanced-Operating-Systems-Notes/blob/main/img/l7/65.JPG?raw=true" alt="drawing" width="500"/>
</p>

<ul>
  <li>Writing to a file by a client is fairly straightforward. What the client is doing is actually aggregating all the writes that is going on into this Log Segment Data Structure, which is in its memory. And at some point, it decides that it wants to flush this Log Segment. And put it on the disk. And when it decides to do that, it knows the Stripe Group, this particular Log Segment it belongs to, and so it is going to take this Log Segment and stripe it on the Storage Servers that are part of the Stripe Group. So once it does this write to its Stripe Group. The client will notify the manager on the lock segments that are being flushed to the disk so that the manager has up-to-date information about the status of the files it manages. So xFS is a research prototype of a Distributed File System. There have been other instances of Distributed File Systems such as. The Android File System and the Coder File System that were built at CMU and in fact the Android File System served a community of Users in the CMU campus in a little lesson we will return to discussing Distributed File Systems again. Specifically we'll look at Andrew File System and discuss the issue of security and privacy for User Files in a Distributed System. But I want to leave you with some closing thoughts on the Exophus File System. First of all, Log based striping, and particularly sub-setting the Storage Servers over which you'll stripe the Log. That's a Technical Innovation. The second Technical Innovation is combining Cooperative Caching with Dynamic Management of Data and Metadata. And the last technical nugget is the distributive Log cleaning, making sure that the responsibility for cleaning up the Logs on the Disk is not left to one Node, but it is actually distributed and especially taking advantage of the fact that the clients, who are the mutators for the file system, meaning they are the guys that are writing to the file system, they can keep a count of the changes that they are making to the Log Segments and use that information in the Log Cleaning effectively.
</li> 
</ul>


<h2>18. Distributed File Systems Conclusion</h2>
<ul>
  <li>Today, network file systems are an important component of any computing environment, be it a corporate setting or university setting. There are companies that have sprung up such as NetApp solely to peddle scalable NFS products. In this lesson, going beyond NFS we learned a lot of concepts pertaining to the design and implementation of distributed file systems, in particular how to make the implementation scalable by removing centralization and utilizing memory that's available in the nodes of a local area network intelligently. Such techniques for identifying and removing bottlenecks are the reusable nuggets that we can take and apply to the design and implementation of other distributed subsystems in additions to file systems themselves. Overall, in the set of papers that we studied in this lesson module spanning GSM, DSM and DFS, we discussed the design and implementation of subsystems that found creative ways to fully utilize memory that's available in the nodes of a local area network.
</li> 
</ul>
